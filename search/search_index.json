{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"LabeL Maker Streamlit Interface : streamlit user interface workflows. Categorize : documentation of the categorize handler for single and multipage classification tasks. Evaluate : documentation of the evaluate handler for users with ground truth labels who want to evaluate classification performance. Categorize : Documentation for categorization step Categorizer Zero-shot Few-shot Many-shot Evaluate : Documentation for evaluation step Evaluator Confidence Intervals data loader Utils : Documentation for supporting utility files Category Class Balance File Manager Normalize Text Page Renderer If you found this helpful in your work, please cite:","title":"Home"},{"location":"index.html#label-maker","text":"Streamlit Interface : streamlit user interface workflows. Categorize : documentation of the categorize handler for single and multipage classification tasks. Evaluate : documentation of the evaluate handler for users with ground truth labels who want to evaluate classification performance. Categorize : Documentation for categorization step Categorizer Zero-shot Few-shot Many-shot Evaluate : Documentation for evaluation step Evaluator Confidence Intervals data loader Utils : Documentation for supporting utility files Category Class Balance File Manager Normalize Text Page Renderer If you found this helpful in your work, please cite:","title":"LabeL Maker"},{"location":"LabeLMaker/categorize_handler.html","text":"Categorizer Handlers This module defines methods to perform content categorization using different techniques. It provides a base handler (BaseCategorizeHandler) for core logic and specialized handlers for Streamlit and FastAPI integration. BaseCategorizeHandler Abstract base class that implements categorization logic. In evaluation mode, it accepts a list of evaluation techniques. In production mode it inspects the provided ground truth examples and then: - If there are enough training examples for a given category, uses Many Shot. - Else if there are a few examples available, uses Few Shot. - Otherwise, falls back to Zero Shot. UI/transport-specific request parsing should be done in child classes. Source code in LabeLMaker/categorize_handler.py 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 class BaseCategorizeHandler : \"\"\" Abstract base class that implements categorization logic. In evaluation mode, it accepts a list of evaluation techniques. In production mode it inspects the provided ground truth examples and then: - If there are enough training examples for a given category, uses Many Shot. - Else if there are a few examples available, uses Few Shot. - Otherwise, falls back to Zero Shot. UI/transport-specific request parsing should be done in child classes. \"\"\" def __init__ ( self , azure_key : Optional [ str ] = None ) -> None : self . config = Config # Expose configuration constants. self . azure_key = azure_key def _prepare_ground_truth_examples ( self , df : pd . DataFrame , id_col : str , text_col : str , gt_col : str , few_shot_count : int = Config . MIN_SAMPLES_FEW_SHOT , many_shot_train_ratio : float = Config . MANY_SHOT_TRAIN_RATIO , ) -> Tuple [ List [ Example ], Set [ str ], List [ Example ], Set [ str ]]: few_shot_examples : List [ Example ] = [] few_shot_ids : Set [ str ] = set () many_shot_examples : List [ Example ] = [] many_shot_test_ids : Set [ str ] = set () df_gt = df [[ id_col , text_col , gt_col ]] . copy () df_gt [ gt_col ] = df_gt [ gt_col ] . astype ( str ) . str . lower () # Group by the ground truth label. grouped = df_gt . groupby ( gt_col ) for _ , group in grouped : records = group . to_dict ( orient = \"records\" ) # Select a few-shot sample up to the provided count. count = min ( few_shot_count , len ( records )) if count > 0 : sampled = random . sample ( records , count ) for rec in sampled : few_shot_examples . append ( Example ( text_with_label = str ( rec [ text_col ]), label = str ( rec [ gt_col ])) ) few_shot_ids . add ( str ( rec [ id_col ])) # Prepare many-shot data if there are multiple records. if len ( records ) > 1 : shuffled = records . copy () random . shuffle ( shuffled ) train_size = max ( 1 , int ( many_shot_train_ratio * len ( records ))) train_examples = shuffled [: train_size ] test_examples = shuffled [ train_size :] for rec in train_examples : many_shot_examples . append ( Example ( text_with_label = str ( rec [ text_col ]), label = str ( rec [ gt_col ])) ) for rec in test_examples : many_shot_test_ids . add ( str ( rec [ id_col ])) return few_shot_examples , few_shot_ids , many_shot_examples , many_shot_test_ids def categorize_data ( self , df : pd . DataFrame , mode : str , index_column : Optional [ str ], text_column : str , ground_truth_column : str , examples_column : str , categories_dict : Dict [ str , Any ], zs_prompty : Path , fs_prompty : Path , evaluation_techniques : Optional [ List [ str ]] = None , few_shot_count : int = Config . FEW_SHOT_COUNT , many_shot_train_ratio : float = Config . MANY_SHOT_TRAIN_RATIO , ) -> pd . DataFrame : \"\"\" The heart of the abstract categorization logic. If mode is \"evaluation\", it prepares ground truth examples and then applies the chosen evaluation techniques. Otherwise (production mode), it selects among zero, few, or many shot modes depending on whether a ground truth column is provided and on the number of examples available. \"\"\" # If no dedicated index is passed in, add one. if not index_column : df [ \"index\" ] = df . index . astype ( str ) index_column = \"index\" # Prepare a list of texts and corresponding unique ids. text_to_label = df [ text_column ] . astype ( str ) . tolist () unique_ids = df [ index_column ] . astype ( str ) . tolist () if mode . lower () == \"evaluation\" and evaluation_techniques : # Evaluation mode: run all requested techniques (e.g., Zero Shot, Few Shot, Many Shot) categorization_request = CategoryManager . create_request ( unique_ids , text_to_label , categories_dict ) predictions = {} ( few_shot_examples , few_shot_ids , many_shot_examples , many_shot_test_ids , ) = self . _prepare_ground_truth_examples ( df , index_column , text_column , ground_truth_column , few_shot_count , many_shot_train_ratio , ) for tech in evaluation_techniques : if tech == \"Zero Shot\" : zs_categorizer = ZeroShotCategorizer ( prompty_path = zs_prompty , category_request = categorization_request ) results = zs_categorizer . process () predictions [ \"Zero Shot\" ] = results elif tech == \"Few Shot\" : fs_request = CategoryManager . create_request ( unique_ids , text_to_label , categories_dict , few_shot_examples ) fs_categorizer = FewShotCategorizer ( prompty_path = fs_prompty , category_request = fs_request ) results = fs_categorizer . process () # Remove any examples already in the few-shot gold set. predictions [ \"Few Shot\" ] = [ r for r in results if str ( r [ 0 ]) not in few_shot_ids ] elif tech == \"Many Shot\" : ms_request = CategoryManager . create_request ( unique_ids , text_to_label , categories_dict , many_shot_examples ) ms_categorizer = ManyshotClassifier ( categorization_request = ms_request , min_class_count = self . config . MIN_SAMPLES_MANY_SHOT , ) results = ms_categorizer . process () predictions [ \"Many Shot\" ] = [ r for r in results if str ( r [ 0 ]) in many_shot_test_ids ] else : raise ValueError ( f \"Unsupported technique ' { tech } ' in evaluation mode.\" ) # Merge the results into the dataframe: one additional set of columns per technique. merged_df = df . copy () for technique , results in predictions . items (): tech_pred_df = pd . DataFrame ( [( row [ 0 ], row [ 2 ], row [ 3 ]) for row in results ], columns = [ index_column , f \"Predicted Category ( { technique } )\" , f \"Rationale ( { technique } )\" ], ) merged_df [ index_column ] = merged_df [ index_column ] . astype ( str ) tech_pred_df [ index_column ] = tech_pred_df [ index_column ] . astype ( str ) merged_df = pd . merge ( merged_df , tech_pred_df , on = index_column , how = \"left\" ) return merged_df else : print ( \"gt col - \" , examples_column ) print ( \"df cols - \" , df . columns ) # Production mode: choose the most appropriate technique based on the provided examples. if examples_column and examples_column in df . columns : ( few_shot_examples , few_shot_ids , many_shot_examples , many_shot_test_ids , ) = self . _prepare_ground_truth_examples ( df , index_column , text_column , examples_column , few_shot_count , many_shot_train_ratio , ) print ( \"FS Examples - \" , few_shot_examples ) print ( \"MS Examples - \" , few_shot_examples ) # Prefer Many Shot if we have enough training examples. if len ( many_shot_examples ) >= self . config . MIN_SAMPLES_MANY_SHOT : ms_request = CategoryManager . create_request ( unique_ids , text_to_label , categories_dict , many_shot_examples ) ms_categorizer = ManyshotClassifier ( categorization_request = ms_request , min_class_count = self . config . MIN_SAMPLES_MANY_SHOT , ) predictions = ms_categorizer . process () # In production, we assume predictions are for test examples only. results = [ r for r in predictions if str ( r [ 0 ]) in many_shot_test_ids ] # Else, try Few Shot if any examples exist. elif len ( few_shot_examples ) > 0 : fs_request = CategoryManager . create_request ( unique_ids , text_to_label , categories_dict , few_shot_examples ) fs_categorizer = FewShotCategorizer ( prompty_path = fs_prompty , category_request = fs_request ) predictions = fs_categorizer . process () results = predictions else : # Fallback to Zero Shot if no ground truth examples are available. zs_request = CategoryManager . create_request ( unique_ids , text_to_label , categories_dict ) zs_categorizer = ZeroShotCategorizer ( prompty_path = zs_prompty , category_request = zs_request ) results = zs_categorizer . process () else : # No ground truth column provided: use Zero Shot as default. zs_request = CategoryManager . create_request ( unique_ids , text_to_label , categories_dict ) zs_categorizer = ZeroShotCategorizer ( prompty_path = zs_prompty , category_request = zs_request ) results = zs_categorizer . process () # In production we assume a single set of predicted results. results_df = pd . DataFrame ( [( row [ 0 ], row [ 2 ], row [ 3 ]) for row in results ], columns = [ index_column , \"Category\" , \"Rationale\" ], ) # Ensure columns are strings for a proper merge. df [ index_column ] = df [ index_column ] . astype ( str ) results_df [ index_column ] = results_df [ index_column ] . astype ( str ) merged_df = pd . merge ( df , results_df , on = index_column , how = \"left\" ) final_columns = list ( df . columns ) + [ \"Category\" , \"Rationale\" ] return merged_df [ final_columns ] categorize_data ( df , mode , index_column , text_column , ground_truth_column , examples_column , categories_dict , zs_prompty , fs_prompty , evaluation_techniques = None , few_shot_count = Config . FEW_SHOT_COUNT , many_shot_train_ratio = Config . MANY_SHOT_TRAIN_RATIO ) The heart of the abstract categorization logic. If mode is \"evaluation\", it prepares ground truth examples and then applies the chosen evaluation techniques. Otherwise (production mode), it selects among zero, few, or many shot modes depending on whether a ground truth column is provided and on the number of examples available. Source code in LabeLMaker/categorize_handler.py 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 def categorize_data ( self , df : pd . DataFrame , mode : str , index_column : Optional [ str ], text_column : str , ground_truth_column : str , examples_column : str , categories_dict : Dict [ str , Any ], zs_prompty : Path , fs_prompty : Path , evaluation_techniques : Optional [ List [ str ]] = None , few_shot_count : int = Config . FEW_SHOT_COUNT , many_shot_train_ratio : float = Config . MANY_SHOT_TRAIN_RATIO , ) -> pd . DataFrame : \"\"\" The heart of the abstract categorization logic. If mode is \"evaluation\", it prepares ground truth examples and then applies the chosen evaluation techniques. Otherwise (production mode), it selects among zero, few, or many shot modes depending on whether a ground truth column is provided and on the number of examples available. \"\"\" # If no dedicated index is passed in, add one. if not index_column : df [ \"index\" ] = df . index . astype ( str ) index_column = \"index\" # Prepare a list of texts and corresponding unique ids. text_to_label = df [ text_column ] . astype ( str ) . tolist () unique_ids = df [ index_column ] . astype ( str ) . tolist () if mode . lower () == \"evaluation\" and evaluation_techniques : # Evaluation mode: run all requested techniques (e.g., Zero Shot, Few Shot, Many Shot) categorization_request = CategoryManager . create_request ( unique_ids , text_to_label , categories_dict ) predictions = {} ( few_shot_examples , few_shot_ids , many_shot_examples , many_shot_test_ids , ) = self . _prepare_ground_truth_examples ( df , index_column , text_column , ground_truth_column , few_shot_count , many_shot_train_ratio , ) for tech in evaluation_techniques : if tech == \"Zero Shot\" : zs_categorizer = ZeroShotCategorizer ( prompty_path = zs_prompty , category_request = categorization_request ) results = zs_categorizer . process () predictions [ \"Zero Shot\" ] = results elif tech == \"Few Shot\" : fs_request = CategoryManager . create_request ( unique_ids , text_to_label , categories_dict , few_shot_examples ) fs_categorizer = FewShotCategorizer ( prompty_path = fs_prompty , category_request = fs_request ) results = fs_categorizer . process () # Remove any examples already in the few-shot gold set. predictions [ \"Few Shot\" ] = [ r for r in results if str ( r [ 0 ]) not in few_shot_ids ] elif tech == \"Many Shot\" : ms_request = CategoryManager . create_request ( unique_ids , text_to_label , categories_dict , many_shot_examples ) ms_categorizer = ManyshotClassifier ( categorization_request = ms_request , min_class_count = self . config . MIN_SAMPLES_MANY_SHOT , ) results = ms_categorizer . process () predictions [ \"Many Shot\" ] = [ r for r in results if str ( r [ 0 ]) in many_shot_test_ids ] else : raise ValueError ( f \"Unsupported technique ' { tech } ' in evaluation mode.\" ) # Merge the results into the dataframe: one additional set of columns per technique. merged_df = df . copy () for technique , results in predictions . items (): tech_pred_df = pd . DataFrame ( [( row [ 0 ], row [ 2 ], row [ 3 ]) for row in results ], columns = [ index_column , f \"Predicted Category ( { technique } )\" , f \"Rationale ( { technique } )\" ], ) merged_df [ index_column ] = merged_df [ index_column ] . astype ( str ) tech_pred_df [ index_column ] = tech_pred_df [ index_column ] . astype ( str ) merged_df = pd . merge ( merged_df , tech_pred_df , on = index_column , how = \"left\" ) return merged_df else : print ( \"gt col - \" , examples_column ) print ( \"df cols - \" , df . columns ) # Production mode: choose the most appropriate technique based on the provided examples. if examples_column and examples_column in df . columns : ( few_shot_examples , few_shot_ids , many_shot_examples , many_shot_test_ids , ) = self . _prepare_ground_truth_examples ( df , index_column , text_column , examples_column , few_shot_count , many_shot_train_ratio , ) print ( \"FS Examples - \" , few_shot_examples ) print ( \"MS Examples - \" , few_shot_examples ) # Prefer Many Shot if we have enough training examples. if len ( many_shot_examples ) >= self . config . MIN_SAMPLES_MANY_SHOT : ms_request = CategoryManager . create_request ( unique_ids , text_to_label , categories_dict , many_shot_examples ) ms_categorizer = ManyshotClassifier ( categorization_request = ms_request , min_class_count = self . config . MIN_SAMPLES_MANY_SHOT , ) predictions = ms_categorizer . process () # In production, we assume predictions are for test examples only. results = [ r for r in predictions if str ( r [ 0 ]) in many_shot_test_ids ] # Else, try Few Shot if any examples exist. elif len ( few_shot_examples ) > 0 : fs_request = CategoryManager . create_request ( unique_ids , text_to_label , categories_dict , few_shot_examples ) fs_categorizer = FewShotCategorizer ( prompty_path = fs_prompty , category_request = fs_request ) predictions = fs_categorizer . process () results = predictions else : # Fallback to Zero Shot if no ground truth examples are available. zs_request = CategoryManager . create_request ( unique_ids , text_to_label , categories_dict ) zs_categorizer = ZeroShotCategorizer ( prompty_path = zs_prompty , category_request = zs_request ) results = zs_categorizer . process () else : # No ground truth column provided: use Zero Shot as default. zs_request = CategoryManager . create_request ( unique_ids , text_to_label , categories_dict ) zs_categorizer = ZeroShotCategorizer ( prompty_path = zs_prompty , category_request = zs_request ) results = zs_categorizer . process () # In production we assume a single set of predicted results. results_df = pd . DataFrame ( [( row [ 0 ], row [ 2 ], row [ 3 ]) for row in results ], columns = [ index_column , \"Category\" , \"Rationale\" ], ) # Ensure columns are strings for a proper merge. df [ index_column ] = df [ index_column ] . astype ( str ) results_df [ index_column ] = results_df [ index_column ] . astype ( str ) merged_df = pd . merge ( df , results_df , on = index_column , how = \"left\" ) final_columns = list ( df . columns ) + [ \"Category\" , \"Rationale\" ] return merged_df [ final_columns ] FastAPICategorizeHandler Bases: BaseCategorizeHandler Provides categorization functionality for FastAPI endpoints. Source code in LabeLMaker/categorize_handler.py 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 class FastAPICategorizeHandler ( BaseCategorizeHandler ): \"\"\" Provides categorization functionality for FastAPI endpoints. \"\"\" def __init__ ( self , azure_key : Optional [ str ] = None ) -> None : super () . __init__ ( azure_key = azure_key ) def fastapi_categorize ( self , data : pd . DataFrame , request : Any , zs_prompty : Path , fs_prompty : Path ) -> pd . DataFrame : \"\"\" Extract values from the FastAPI request and pass them to BaseCategorizeHandler. (Here we assume the request object carries attributes like index_column, text_column, etc.) \"\"\" index_column = request . index_column text_column = request . text_column # ex_label_column may be empty or None. gt_column = request . ex_label_column if getattr ( request , \"ex_label_column\" , None ) else \"\" examples_column = gt_column categories_dict = { cat . name : cat . description for cat in request . categories } return self . categorize_data ( df = data , mode = request . mode , index_column = index_column , text_column = text_column , ground_truth_column = gt_column , examples_column = examples_column , categories_dict = categories_dict , zs_prompty = zs_prompty , fs_prompty = fs_prompty , evaluation_techniques = getattr ( request , \"model\" , None ), # Could be a list of techniques. few_shot_count = int ( request . few_shot_count ), many_shot_train_ratio = float ( request . many_shot_train_ratio ), ) fastapi_categorize ( data , request , zs_prompty , fs_prompty ) Extract values from the FastAPI request and pass them to BaseCategorizeHandler. (Here we assume the request object carries attributes like index_column, text_column, etc.) Source code in LabeLMaker/categorize_handler.py 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 def fastapi_categorize ( self , data : pd . DataFrame , request : Any , zs_prompty : Path , fs_prompty : Path ) -> pd . DataFrame : \"\"\" Extract values from the FastAPI request and pass them to BaseCategorizeHandler. (Here we assume the request object carries attributes like index_column, text_column, etc.) \"\"\" index_column = request . index_column text_column = request . text_column # ex_label_column may be empty or None. gt_column = request . ex_label_column if getattr ( request , \"ex_label_column\" , None ) else \"\" examples_column = gt_column categories_dict = { cat . name : cat . description for cat in request . categories } return self . categorize_data ( df = data , mode = request . mode , index_column = index_column , text_column = text_column , ground_truth_column = gt_column , examples_column = examples_column , categories_dict = categories_dict , zs_prompty = zs_prompty , fs_prompty = fs_prompty , evaluation_techniques = getattr ( request , \"model\" , None ), # Could be a list of techniques. few_shot_count = int ( request . few_shot_count ), many_shot_train_ratio = float ( request . many_shot_train_ratio ), ) StreamlitCategorizeHandler Bases: BaseCategorizeHandler Thin wrapper for Streamlit usage. Expects that UI parameters (collected via the UI) are passed in a dictionary. Source code in LabeLMaker/categorize_handler.py 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 class StreamlitCategorizeHandler ( BaseCategorizeHandler ): \"\"\" Thin wrapper for Streamlit usage. Expects that UI parameters (collected via the UI) are passed in a dictionary. \"\"\" def __init__ ( self , azure_key : Optional [ str ] = None ) -> None : super () . __init__ ( azure_key = azure_key ) def streamlit_categorize ( self , df : pd . DataFrame , ui_params : Dict [ str , Any ], zs_prompty : Path , fs_prompty : Path , ) -> pd . DataFrame : \"\"\" Extract values from the Streamlit UI dictionary and pass them to BaseCategorizeHandler. \"\"\" return self . categorize_data ( df = df , mode = ui_params . get ( \"mode\" , \"production\" ), index_column = ui_params . get ( \"index_column\" ), text_column = ui_params . get ( \"categorizing_column\" ), ground_truth_column = ui_params . get ( \"ground_truth_column\" , \"\" ), examples_column = ui_params . get ( \"examples_column\" , \"\" ), categories_dict = ui_params . get ( \"categories_dict\" , {}), zs_prompty = zs_prompty , fs_prompty = fs_prompty , evaluation_techniques = ui_params . get ( \"evaluation_techniques\" ), few_shot_count = ui_params . get ( \"few_shot_count\" , Config . FEW_SHOT_COUNT ), many_shot_train_ratio = ui_params . get ( \"many_shot_train_ratio\" , Config . MANY_SHOT_TRAIN_RATIO ), ) streamlit_categorize ( df , ui_params , zs_prompty , fs_prompty ) Extract values from the Streamlit UI dictionary and pass them to BaseCategorizeHandler. Source code in LabeLMaker/categorize_handler.py 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 def streamlit_categorize ( self , df : pd . DataFrame , ui_params : Dict [ str , Any ], zs_prompty : Path , fs_prompty : Path , ) -> pd . DataFrame : \"\"\" Extract values from the Streamlit UI dictionary and pass them to BaseCategorizeHandler. \"\"\" return self . categorize_data ( df = df , mode = ui_params . get ( \"mode\" , \"production\" ), index_column = ui_params . get ( \"index_column\" ), text_column = ui_params . get ( \"categorizing_column\" ), ground_truth_column = ui_params . get ( \"ground_truth_column\" , \"\" ), examples_column = ui_params . get ( \"examples_column\" , \"\" ), categories_dict = ui_params . get ( \"categories_dict\" , {}), zs_prompty = zs_prompty , fs_prompty = fs_prompty , evaluation_techniques = ui_params . get ( \"evaluation_techniques\" ), few_shot_count = ui_params . get ( \"few_shot_count\" , Config . FEW_SHOT_COUNT ), many_shot_train_ratio = ui_params . get ( \"many_shot_train_ratio\" , Config . MANY_SHOT_TRAIN_RATIO ), )","title":"Categorize"},{"location":"LabeLMaker/categorize_handler.html#LabeLMaker.categorize_handler.BaseCategorizeHandler","text":"Abstract base class that implements categorization logic. In evaluation mode, it accepts a list of evaluation techniques. In production mode it inspects the provided ground truth examples and then: - If there are enough training examples for a given category, uses Many Shot. - Else if there are a few examples available, uses Few Shot. - Otherwise, falls back to Zero Shot. UI/transport-specific request parsing should be done in child classes. Source code in LabeLMaker/categorize_handler.py 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 class BaseCategorizeHandler : \"\"\" Abstract base class that implements categorization logic. In evaluation mode, it accepts a list of evaluation techniques. In production mode it inspects the provided ground truth examples and then: - If there are enough training examples for a given category, uses Many Shot. - Else if there are a few examples available, uses Few Shot. - Otherwise, falls back to Zero Shot. UI/transport-specific request parsing should be done in child classes. \"\"\" def __init__ ( self , azure_key : Optional [ str ] = None ) -> None : self . config = Config # Expose configuration constants. self . azure_key = azure_key def _prepare_ground_truth_examples ( self , df : pd . DataFrame , id_col : str , text_col : str , gt_col : str , few_shot_count : int = Config . MIN_SAMPLES_FEW_SHOT , many_shot_train_ratio : float = Config . MANY_SHOT_TRAIN_RATIO , ) -> Tuple [ List [ Example ], Set [ str ], List [ Example ], Set [ str ]]: few_shot_examples : List [ Example ] = [] few_shot_ids : Set [ str ] = set () many_shot_examples : List [ Example ] = [] many_shot_test_ids : Set [ str ] = set () df_gt = df [[ id_col , text_col , gt_col ]] . copy () df_gt [ gt_col ] = df_gt [ gt_col ] . astype ( str ) . str . lower () # Group by the ground truth label. grouped = df_gt . groupby ( gt_col ) for _ , group in grouped : records = group . to_dict ( orient = \"records\" ) # Select a few-shot sample up to the provided count. count = min ( few_shot_count , len ( records )) if count > 0 : sampled = random . sample ( records , count ) for rec in sampled : few_shot_examples . append ( Example ( text_with_label = str ( rec [ text_col ]), label = str ( rec [ gt_col ])) ) few_shot_ids . add ( str ( rec [ id_col ])) # Prepare many-shot data if there are multiple records. if len ( records ) > 1 : shuffled = records . copy () random . shuffle ( shuffled ) train_size = max ( 1 , int ( many_shot_train_ratio * len ( records ))) train_examples = shuffled [: train_size ] test_examples = shuffled [ train_size :] for rec in train_examples : many_shot_examples . append ( Example ( text_with_label = str ( rec [ text_col ]), label = str ( rec [ gt_col ])) ) for rec in test_examples : many_shot_test_ids . add ( str ( rec [ id_col ])) return few_shot_examples , few_shot_ids , many_shot_examples , many_shot_test_ids def categorize_data ( self , df : pd . DataFrame , mode : str , index_column : Optional [ str ], text_column : str , ground_truth_column : str , examples_column : str , categories_dict : Dict [ str , Any ], zs_prompty : Path , fs_prompty : Path , evaluation_techniques : Optional [ List [ str ]] = None , few_shot_count : int = Config . FEW_SHOT_COUNT , many_shot_train_ratio : float = Config . MANY_SHOT_TRAIN_RATIO , ) -> pd . DataFrame : \"\"\" The heart of the abstract categorization logic. If mode is \"evaluation\", it prepares ground truth examples and then applies the chosen evaluation techniques. Otherwise (production mode), it selects among zero, few, or many shot modes depending on whether a ground truth column is provided and on the number of examples available. \"\"\" # If no dedicated index is passed in, add one. if not index_column : df [ \"index\" ] = df . index . astype ( str ) index_column = \"index\" # Prepare a list of texts and corresponding unique ids. text_to_label = df [ text_column ] . astype ( str ) . tolist () unique_ids = df [ index_column ] . astype ( str ) . tolist () if mode . lower () == \"evaluation\" and evaluation_techniques : # Evaluation mode: run all requested techniques (e.g., Zero Shot, Few Shot, Many Shot) categorization_request = CategoryManager . create_request ( unique_ids , text_to_label , categories_dict ) predictions = {} ( few_shot_examples , few_shot_ids , many_shot_examples , many_shot_test_ids , ) = self . _prepare_ground_truth_examples ( df , index_column , text_column , ground_truth_column , few_shot_count , many_shot_train_ratio , ) for tech in evaluation_techniques : if tech == \"Zero Shot\" : zs_categorizer = ZeroShotCategorizer ( prompty_path = zs_prompty , category_request = categorization_request ) results = zs_categorizer . process () predictions [ \"Zero Shot\" ] = results elif tech == \"Few Shot\" : fs_request = CategoryManager . create_request ( unique_ids , text_to_label , categories_dict , few_shot_examples ) fs_categorizer = FewShotCategorizer ( prompty_path = fs_prompty , category_request = fs_request ) results = fs_categorizer . process () # Remove any examples already in the few-shot gold set. predictions [ \"Few Shot\" ] = [ r for r in results if str ( r [ 0 ]) not in few_shot_ids ] elif tech == \"Many Shot\" : ms_request = CategoryManager . create_request ( unique_ids , text_to_label , categories_dict , many_shot_examples ) ms_categorizer = ManyshotClassifier ( categorization_request = ms_request , min_class_count = self . config . MIN_SAMPLES_MANY_SHOT , ) results = ms_categorizer . process () predictions [ \"Many Shot\" ] = [ r for r in results if str ( r [ 0 ]) in many_shot_test_ids ] else : raise ValueError ( f \"Unsupported technique ' { tech } ' in evaluation mode.\" ) # Merge the results into the dataframe: one additional set of columns per technique. merged_df = df . copy () for technique , results in predictions . items (): tech_pred_df = pd . DataFrame ( [( row [ 0 ], row [ 2 ], row [ 3 ]) for row in results ], columns = [ index_column , f \"Predicted Category ( { technique } )\" , f \"Rationale ( { technique } )\" ], ) merged_df [ index_column ] = merged_df [ index_column ] . astype ( str ) tech_pred_df [ index_column ] = tech_pred_df [ index_column ] . astype ( str ) merged_df = pd . merge ( merged_df , tech_pred_df , on = index_column , how = \"left\" ) return merged_df else : print ( \"gt col - \" , examples_column ) print ( \"df cols - \" , df . columns ) # Production mode: choose the most appropriate technique based on the provided examples. if examples_column and examples_column in df . columns : ( few_shot_examples , few_shot_ids , many_shot_examples , many_shot_test_ids , ) = self . _prepare_ground_truth_examples ( df , index_column , text_column , examples_column , few_shot_count , many_shot_train_ratio , ) print ( \"FS Examples - \" , few_shot_examples ) print ( \"MS Examples - \" , few_shot_examples ) # Prefer Many Shot if we have enough training examples. if len ( many_shot_examples ) >= self . config . MIN_SAMPLES_MANY_SHOT : ms_request = CategoryManager . create_request ( unique_ids , text_to_label , categories_dict , many_shot_examples ) ms_categorizer = ManyshotClassifier ( categorization_request = ms_request , min_class_count = self . config . MIN_SAMPLES_MANY_SHOT , ) predictions = ms_categorizer . process () # In production, we assume predictions are for test examples only. results = [ r for r in predictions if str ( r [ 0 ]) in many_shot_test_ids ] # Else, try Few Shot if any examples exist. elif len ( few_shot_examples ) > 0 : fs_request = CategoryManager . create_request ( unique_ids , text_to_label , categories_dict , few_shot_examples ) fs_categorizer = FewShotCategorizer ( prompty_path = fs_prompty , category_request = fs_request ) predictions = fs_categorizer . process () results = predictions else : # Fallback to Zero Shot if no ground truth examples are available. zs_request = CategoryManager . create_request ( unique_ids , text_to_label , categories_dict ) zs_categorizer = ZeroShotCategorizer ( prompty_path = zs_prompty , category_request = zs_request ) results = zs_categorizer . process () else : # No ground truth column provided: use Zero Shot as default. zs_request = CategoryManager . create_request ( unique_ids , text_to_label , categories_dict ) zs_categorizer = ZeroShotCategorizer ( prompty_path = zs_prompty , category_request = zs_request ) results = zs_categorizer . process () # In production we assume a single set of predicted results. results_df = pd . DataFrame ( [( row [ 0 ], row [ 2 ], row [ 3 ]) for row in results ], columns = [ index_column , \"Category\" , \"Rationale\" ], ) # Ensure columns are strings for a proper merge. df [ index_column ] = df [ index_column ] . astype ( str ) results_df [ index_column ] = results_df [ index_column ] . astype ( str ) merged_df = pd . merge ( df , results_df , on = index_column , how = \"left\" ) final_columns = list ( df . columns ) + [ \"Category\" , \"Rationale\" ] return merged_df [ final_columns ]","title":"BaseCategorizeHandler"},{"location":"LabeLMaker/categorize_handler.html#LabeLMaker.categorize_handler.BaseCategorizeHandler.categorize_data","text":"The heart of the abstract categorization logic. If mode is \"evaluation\", it prepares ground truth examples and then applies the chosen evaluation techniques. Otherwise (production mode), it selects among zero, few, or many shot modes depending on whether a ground truth column is provided and on the number of examples available. Source code in LabeLMaker/categorize_handler.py 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 def categorize_data ( self , df : pd . DataFrame , mode : str , index_column : Optional [ str ], text_column : str , ground_truth_column : str , examples_column : str , categories_dict : Dict [ str , Any ], zs_prompty : Path , fs_prompty : Path , evaluation_techniques : Optional [ List [ str ]] = None , few_shot_count : int = Config . FEW_SHOT_COUNT , many_shot_train_ratio : float = Config . MANY_SHOT_TRAIN_RATIO , ) -> pd . DataFrame : \"\"\" The heart of the abstract categorization logic. If mode is \"evaluation\", it prepares ground truth examples and then applies the chosen evaluation techniques. Otherwise (production mode), it selects among zero, few, or many shot modes depending on whether a ground truth column is provided and on the number of examples available. \"\"\" # If no dedicated index is passed in, add one. if not index_column : df [ \"index\" ] = df . index . astype ( str ) index_column = \"index\" # Prepare a list of texts and corresponding unique ids. text_to_label = df [ text_column ] . astype ( str ) . tolist () unique_ids = df [ index_column ] . astype ( str ) . tolist () if mode . lower () == \"evaluation\" and evaluation_techniques : # Evaluation mode: run all requested techniques (e.g., Zero Shot, Few Shot, Many Shot) categorization_request = CategoryManager . create_request ( unique_ids , text_to_label , categories_dict ) predictions = {} ( few_shot_examples , few_shot_ids , many_shot_examples , many_shot_test_ids , ) = self . _prepare_ground_truth_examples ( df , index_column , text_column , ground_truth_column , few_shot_count , many_shot_train_ratio , ) for tech in evaluation_techniques : if tech == \"Zero Shot\" : zs_categorizer = ZeroShotCategorizer ( prompty_path = zs_prompty , category_request = categorization_request ) results = zs_categorizer . process () predictions [ \"Zero Shot\" ] = results elif tech == \"Few Shot\" : fs_request = CategoryManager . create_request ( unique_ids , text_to_label , categories_dict , few_shot_examples ) fs_categorizer = FewShotCategorizer ( prompty_path = fs_prompty , category_request = fs_request ) results = fs_categorizer . process () # Remove any examples already in the few-shot gold set. predictions [ \"Few Shot\" ] = [ r for r in results if str ( r [ 0 ]) not in few_shot_ids ] elif tech == \"Many Shot\" : ms_request = CategoryManager . create_request ( unique_ids , text_to_label , categories_dict , many_shot_examples ) ms_categorizer = ManyshotClassifier ( categorization_request = ms_request , min_class_count = self . config . MIN_SAMPLES_MANY_SHOT , ) results = ms_categorizer . process () predictions [ \"Many Shot\" ] = [ r for r in results if str ( r [ 0 ]) in many_shot_test_ids ] else : raise ValueError ( f \"Unsupported technique ' { tech } ' in evaluation mode.\" ) # Merge the results into the dataframe: one additional set of columns per technique. merged_df = df . copy () for technique , results in predictions . items (): tech_pred_df = pd . DataFrame ( [( row [ 0 ], row [ 2 ], row [ 3 ]) for row in results ], columns = [ index_column , f \"Predicted Category ( { technique } )\" , f \"Rationale ( { technique } )\" ], ) merged_df [ index_column ] = merged_df [ index_column ] . astype ( str ) tech_pred_df [ index_column ] = tech_pred_df [ index_column ] . astype ( str ) merged_df = pd . merge ( merged_df , tech_pred_df , on = index_column , how = \"left\" ) return merged_df else : print ( \"gt col - \" , examples_column ) print ( \"df cols - \" , df . columns ) # Production mode: choose the most appropriate technique based on the provided examples. if examples_column and examples_column in df . columns : ( few_shot_examples , few_shot_ids , many_shot_examples , many_shot_test_ids , ) = self . _prepare_ground_truth_examples ( df , index_column , text_column , examples_column , few_shot_count , many_shot_train_ratio , ) print ( \"FS Examples - \" , few_shot_examples ) print ( \"MS Examples - \" , few_shot_examples ) # Prefer Many Shot if we have enough training examples. if len ( many_shot_examples ) >= self . config . MIN_SAMPLES_MANY_SHOT : ms_request = CategoryManager . create_request ( unique_ids , text_to_label , categories_dict , many_shot_examples ) ms_categorizer = ManyshotClassifier ( categorization_request = ms_request , min_class_count = self . config . MIN_SAMPLES_MANY_SHOT , ) predictions = ms_categorizer . process () # In production, we assume predictions are for test examples only. results = [ r for r in predictions if str ( r [ 0 ]) in many_shot_test_ids ] # Else, try Few Shot if any examples exist. elif len ( few_shot_examples ) > 0 : fs_request = CategoryManager . create_request ( unique_ids , text_to_label , categories_dict , few_shot_examples ) fs_categorizer = FewShotCategorizer ( prompty_path = fs_prompty , category_request = fs_request ) predictions = fs_categorizer . process () results = predictions else : # Fallback to Zero Shot if no ground truth examples are available. zs_request = CategoryManager . create_request ( unique_ids , text_to_label , categories_dict ) zs_categorizer = ZeroShotCategorizer ( prompty_path = zs_prompty , category_request = zs_request ) results = zs_categorizer . process () else : # No ground truth column provided: use Zero Shot as default. zs_request = CategoryManager . create_request ( unique_ids , text_to_label , categories_dict ) zs_categorizer = ZeroShotCategorizer ( prompty_path = zs_prompty , category_request = zs_request ) results = zs_categorizer . process () # In production we assume a single set of predicted results. results_df = pd . DataFrame ( [( row [ 0 ], row [ 2 ], row [ 3 ]) for row in results ], columns = [ index_column , \"Category\" , \"Rationale\" ], ) # Ensure columns are strings for a proper merge. df [ index_column ] = df [ index_column ] . astype ( str ) results_df [ index_column ] = results_df [ index_column ] . astype ( str ) merged_df = pd . merge ( df , results_df , on = index_column , how = \"left\" ) final_columns = list ( df . columns ) + [ \"Category\" , \"Rationale\" ] return merged_df [ final_columns ]","title":"categorize_data"},{"location":"LabeLMaker/categorize_handler.html#LabeLMaker.categorize_handler.FastAPICategorizeHandler","text":"Bases: BaseCategorizeHandler Provides categorization functionality for FastAPI endpoints. Source code in LabeLMaker/categorize_handler.py 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 class FastAPICategorizeHandler ( BaseCategorizeHandler ): \"\"\" Provides categorization functionality for FastAPI endpoints. \"\"\" def __init__ ( self , azure_key : Optional [ str ] = None ) -> None : super () . __init__ ( azure_key = azure_key ) def fastapi_categorize ( self , data : pd . DataFrame , request : Any , zs_prompty : Path , fs_prompty : Path ) -> pd . DataFrame : \"\"\" Extract values from the FastAPI request and pass them to BaseCategorizeHandler. (Here we assume the request object carries attributes like index_column, text_column, etc.) \"\"\" index_column = request . index_column text_column = request . text_column # ex_label_column may be empty or None. gt_column = request . ex_label_column if getattr ( request , \"ex_label_column\" , None ) else \"\" examples_column = gt_column categories_dict = { cat . name : cat . description for cat in request . categories } return self . categorize_data ( df = data , mode = request . mode , index_column = index_column , text_column = text_column , ground_truth_column = gt_column , examples_column = examples_column , categories_dict = categories_dict , zs_prompty = zs_prompty , fs_prompty = fs_prompty , evaluation_techniques = getattr ( request , \"model\" , None ), # Could be a list of techniques. few_shot_count = int ( request . few_shot_count ), many_shot_train_ratio = float ( request . many_shot_train_ratio ), )","title":"FastAPICategorizeHandler"},{"location":"LabeLMaker/categorize_handler.html#LabeLMaker.categorize_handler.FastAPICategorizeHandler.fastapi_categorize","text":"Extract values from the FastAPI request and pass them to BaseCategorizeHandler. (Here we assume the request object carries attributes like index_column, text_column, etc.) Source code in LabeLMaker/categorize_handler.py 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 def fastapi_categorize ( self , data : pd . DataFrame , request : Any , zs_prompty : Path , fs_prompty : Path ) -> pd . DataFrame : \"\"\" Extract values from the FastAPI request and pass them to BaseCategorizeHandler. (Here we assume the request object carries attributes like index_column, text_column, etc.) \"\"\" index_column = request . index_column text_column = request . text_column # ex_label_column may be empty or None. gt_column = request . ex_label_column if getattr ( request , \"ex_label_column\" , None ) else \"\" examples_column = gt_column categories_dict = { cat . name : cat . description for cat in request . categories } return self . categorize_data ( df = data , mode = request . mode , index_column = index_column , text_column = text_column , ground_truth_column = gt_column , examples_column = examples_column , categories_dict = categories_dict , zs_prompty = zs_prompty , fs_prompty = fs_prompty , evaluation_techniques = getattr ( request , \"model\" , None ), # Could be a list of techniques. few_shot_count = int ( request . few_shot_count ), many_shot_train_ratio = float ( request . many_shot_train_ratio ), )","title":"fastapi_categorize"},{"location":"LabeLMaker/categorize_handler.html#LabeLMaker.categorize_handler.StreamlitCategorizeHandler","text":"Bases: BaseCategorizeHandler Thin wrapper for Streamlit usage. Expects that UI parameters (collected via the UI) are passed in a dictionary. Source code in LabeLMaker/categorize_handler.py 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 class StreamlitCategorizeHandler ( BaseCategorizeHandler ): \"\"\" Thin wrapper for Streamlit usage. Expects that UI parameters (collected via the UI) are passed in a dictionary. \"\"\" def __init__ ( self , azure_key : Optional [ str ] = None ) -> None : super () . __init__ ( azure_key = azure_key ) def streamlit_categorize ( self , df : pd . DataFrame , ui_params : Dict [ str , Any ], zs_prompty : Path , fs_prompty : Path , ) -> pd . DataFrame : \"\"\" Extract values from the Streamlit UI dictionary and pass them to BaseCategorizeHandler. \"\"\" return self . categorize_data ( df = df , mode = ui_params . get ( \"mode\" , \"production\" ), index_column = ui_params . get ( \"index_column\" ), text_column = ui_params . get ( \"categorizing_column\" ), ground_truth_column = ui_params . get ( \"ground_truth_column\" , \"\" ), examples_column = ui_params . get ( \"examples_column\" , \"\" ), categories_dict = ui_params . get ( \"categories_dict\" , {}), zs_prompty = zs_prompty , fs_prompty = fs_prompty , evaluation_techniques = ui_params . get ( \"evaluation_techniques\" ), few_shot_count = ui_params . get ( \"few_shot_count\" , Config . FEW_SHOT_COUNT ), many_shot_train_ratio = ui_params . get ( \"many_shot_train_ratio\" , Config . MANY_SHOT_TRAIN_RATIO ), )","title":"StreamlitCategorizeHandler"},{"location":"LabeLMaker/categorize_handler.html#LabeLMaker.categorize_handler.StreamlitCategorizeHandler.streamlit_categorize","text":"Extract values from the Streamlit UI dictionary and pass them to BaseCategorizeHandler. Source code in LabeLMaker/categorize_handler.py 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 def streamlit_categorize ( self , df : pd . DataFrame , ui_params : Dict [ str , Any ], zs_prompty : Path , fs_prompty : Path , ) -> pd . DataFrame : \"\"\" Extract values from the Streamlit UI dictionary and pass them to BaseCategorizeHandler. \"\"\" return self . categorize_data ( df = df , mode = ui_params . get ( \"mode\" , \"production\" ), index_column = ui_params . get ( \"index_column\" ), text_column = ui_params . get ( \"categorizing_column\" ), ground_truth_column = ui_params . get ( \"ground_truth_column\" , \"\" ), examples_column = ui_params . get ( \"examples_column\" , \"\" ), categories_dict = ui_params . get ( \"categories_dict\" , {}), zs_prompty = zs_prompty , fs_prompty = fs_prompty , evaluation_techniques = ui_params . get ( \"evaluation_techniques\" ), few_shot_count = ui_params . get ( \"few_shot_count\" , Config . FEW_SHOT_COUNT ), many_shot_train_ratio = ui_params . get ( \"many_shot_train_ratio\" , Config . MANY_SHOT_TRAIN_RATIO ), )","title":"streamlit_categorize"},{"location":"LabeLMaker/evaluate_handler.html","text":"Evaluation Handlers This module provides three classes \u2022 BaseEvaluateHandler: Contains core evaluation methods (data loading, model evaluation, metric calculation, bootstrap confidence interval computation, confusion matrix plotting). \u2022 StreamlitEvaluateHandler: Inherits from BaseEvaluateHandler and integrates with a Streamlit UI. \u2022 FastAPIEvaluateHandler: Inherits from BaseEvaluateHandler and exposes a FastAPI\u2013friendly method. Note This module assumes that the underlying evaluation components (DataLoader, Evaluator, compute_bootstrap_confidence_intervals, etc.) are available. BaseEvaluateHandler Provides core evaluation functionality. Source code in LabeLMaker/evaluate_handler.py 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 class BaseEvaluateHandler : \"\"\" Provides core evaluation functionality. \"\"\" def __init__ ( self , azure_key : str = None ) -> None : self . azure_key = azure_key self . config = Config def _load_data ( self , uploaded_file : Any ) -> pd . DataFrame : try : if hasattr ( uploaded_file , \"seek\" ): uploaded_file . seek ( 0 ) # Reset file pointer before reading df = pd . read_csv ( uploaded_file ) if df . empty : raise Exception ( \"File appears to be empty or has no valid columns.\" ) return df except Exception as e : raise Exception ( f \"Error processing CSV file: { e } \" ) def evaluate_model ( self , df : pd . DataFrame , pred_col : str , ground_truth_col : str , n_bootstraps : int = 1000 , alpha : float = 0.05 , ) -> Tuple [ pd . DataFrame , pd . DataFrame , pd . DataFrame , Any ]: \"\"\" Evaluate model predictions and compute associated metrics. Returns a tuple: (metrics_df, classification_report_df, bootstrap_df, confusion_matrix_fig) \"\"\" if pred_col not in df . columns or ground_truth_col not in df . columns : raise ValueError ( \"Prediction or ground truth column not found in DataFrame.\" ) valid = df [ pred_col ] . notna () y_true = df . loc [ valid , ground_truth_col ] . astype ( str ) . tolist () y_pred = df . loc [ valid , pred_col ] . astype ( str ) . tolist () evaluator = Evaluator ( y_true , y_pred ) evaluator . calculate_metrics () metrics_df = evaluator . display_metrics () report_df = pd . DataFrame ( evaluator . metrics [ \"Classification Report\" ]) . transpose () bs_results = compute_bootstrap_confidence_intervals ( y_true , y_pred , n_bootstraps = n_bootstraps , alpha = alpha ) bs_display = [] for metric , values in bs_results . items (): if values [ \"Value\" ] is not None : bs_display . append ( { \"Metric\" : metric , \"Value\" : f \" { values [ 'Value' ] : .4f } \" , \"Bootstrap Mean\" : f \" { values [ 'Bootstrap Mean' ] : .4f } \" , \"95% CI\" : f \"( { values [ '95% CI' ][ 0 ] : .4f } , { values [ '95% CI' ][ 1 ] : .4f } )\" , } ) else : bs_display . append ( { \"Metric\" : metric , \"Value\" : \"Undefined\" , \"Bootstrap Mean\" : \"Undefined\" , \"95% CI\" : \"Undefined\" , } ) bs_df = pd . DataFrame ( bs_display ) cm_fig = evaluator . plot_confusion_matrix () return metrics_df , report_df , bs_df , cm_fig def compare_methods ( self , df : pd . DataFrame , ground_truth_col : str , selected_methods : list , ) -> Tuple [ pd . DataFrame , Dict [ str , Any ], Dict [ str , Any ]]: \"\"\" Compare prediction methods (e.g. Zero Shot, Few Shot, Many Shot) by evaluating predictions in multiple columns. Returns (common_df, results, confusion_matrices) \"\"\" # Map the method to the corresponding DataFrame column name. method_columns = { method : f \"Predicted Category ( { method } )\" for method in selected_methods } valid_methods = { m : col for m , col in method_columns . items () if col in df . columns } if not valid_methods : raise ValueError ( \"No selected method prediction columns exist in DataFrame.\" ) common_df = df . dropna ( subset = list ( valid_methods . values ())) results = {} confusion_matrices = {} for method , col in valid_methods . items (): y_true = common_df [ ground_truth_col ] . astype ( str ) . tolist () y_pred = common_df [ col ] . astype ( str ) . tolist () evaluator = Evaluator ( y_true , y_pred ) evaluator . calculate_metrics () results [ method ] = evaluator . metrics confusion_matrices [ method ] = evaluator . plot_confusion_matrix () return common_df , results , confusion_matrices compare_methods ( df , ground_truth_col , selected_methods ) Compare prediction methods (e.g. Zero Shot, Few Shot, Many Shot) by evaluating predictions in multiple columns. Returns (common_df, results, confusion_matrices) Source code in LabeLMaker/evaluate_handler.py 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 def compare_methods ( self , df : pd . DataFrame , ground_truth_col : str , selected_methods : list , ) -> Tuple [ pd . DataFrame , Dict [ str , Any ], Dict [ str , Any ]]: \"\"\" Compare prediction methods (e.g. Zero Shot, Few Shot, Many Shot) by evaluating predictions in multiple columns. Returns (common_df, results, confusion_matrices) \"\"\" # Map the method to the corresponding DataFrame column name. method_columns = { method : f \"Predicted Category ( { method } )\" for method in selected_methods } valid_methods = { m : col for m , col in method_columns . items () if col in df . columns } if not valid_methods : raise ValueError ( \"No selected method prediction columns exist in DataFrame.\" ) common_df = df . dropna ( subset = list ( valid_methods . values ())) results = {} confusion_matrices = {} for method , col in valid_methods . items (): y_true = common_df [ ground_truth_col ] . astype ( str ) . tolist () y_pred = common_df [ col ] . astype ( str ) . tolist () evaluator = Evaluator ( y_true , y_pred ) evaluator . calculate_metrics () results [ method ] = evaluator . metrics confusion_matrices [ method ] = evaluator . plot_confusion_matrix () return common_df , results , confusion_matrices evaluate_model ( df , pred_col , ground_truth_col , n_bootstraps = 1000 , alpha = 0.05 ) Evaluate model predictions and compute associated metrics. Returns a tuple: (metrics_df, classification_report_df, bootstrap_df, confusion_matrix_fig) Source code in LabeLMaker/evaluate_handler.py 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 def evaluate_model ( self , df : pd . DataFrame , pred_col : str , ground_truth_col : str , n_bootstraps : int = 1000 , alpha : float = 0.05 , ) -> Tuple [ pd . DataFrame , pd . DataFrame , pd . DataFrame , Any ]: \"\"\" Evaluate model predictions and compute associated metrics. Returns a tuple: (metrics_df, classification_report_df, bootstrap_df, confusion_matrix_fig) \"\"\" if pred_col not in df . columns or ground_truth_col not in df . columns : raise ValueError ( \"Prediction or ground truth column not found in DataFrame.\" ) valid = df [ pred_col ] . notna () y_true = df . loc [ valid , ground_truth_col ] . astype ( str ) . tolist () y_pred = df . loc [ valid , pred_col ] . astype ( str ) . tolist () evaluator = Evaluator ( y_true , y_pred ) evaluator . calculate_metrics () metrics_df = evaluator . display_metrics () report_df = pd . DataFrame ( evaluator . metrics [ \"Classification Report\" ]) . transpose () bs_results = compute_bootstrap_confidence_intervals ( y_true , y_pred , n_bootstraps = n_bootstraps , alpha = alpha ) bs_display = [] for metric , values in bs_results . items (): if values [ \"Value\" ] is not None : bs_display . append ( { \"Metric\" : metric , \"Value\" : f \" { values [ 'Value' ] : .4f } \" , \"Bootstrap Mean\" : f \" { values [ 'Bootstrap Mean' ] : .4f } \" , \"95% CI\" : f \"( { values [ '95% CI' ][ 0 ] : .4f } , { values [ '95% CI' ][ 1 ] : .4f } )\" , } ) else : bs_display . append ( { \"Metric\" : metric , \"Value\" : \"Undefined\" , \"Bootstrap Mean\" : \"Undefined\" , \"95% CI\" : \"Undefined\" , } ) bs_df = pd . DataFrame ( bs_display ) cm_fig = evaluator . plot_confusion_matrix () return metrics_df , report_df , bs_df , cm_fig FastAPIEvaluateHandler Bases: BaseEvaluateHandler Provides a FastAPI\u2013friendly evaluation method. Source code in LabeLMaker/evaluate_handler.py 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 class FastAPIEvaluateHandler ( BaseEvaluateHandler ): \"\"\" Provides a FastAPI\u2013friendly evaluation method. \"\"\" def __init__ ( self , azure_key : str = None ) -> None : super () . __init__ ( azure_key = azure_key ) def fastapi_evaluate ( self , data : pd . DataFrame , request : Any ) -> Dict [ str , Any ]: \"\"\" Execute evaluation and return results as a JSON\u2013serializable dictionary. Expects that request defines: \u2022 ground_truth_column \u2022 pred_column \u2022 Optional: n_bootstraps, alpha \"\"\" ground_truth_col = request . ground_truth_column pred_col = request . pred_column n_bootstraps = getattr ( request , \"n_bootstraps\" , 1000 ) alpha = getattr ( request , \"alpha\" , 0.05 ) try : metrics_df , report_df , bs_df , cm_fig = self . evaluate_model ( data , pred_col , ground_truth_col , n_bootstraps = n_bootstraps , alpha = alpha ) response = { \"metrics\" : metrics_df . to_dict (), \"classification_report\" : report_df . to_dict (), \"bootstrap_confidence_intervals\" : bs_df . to_dict (), \"confusion_matrix\" : str ( cm_fig ), # Customize serialization as needed. } return response except Exception as e : return { \"error\" : str ( e )} fastapi_evaluate ( data , request ) Execute evaluation and return results as a JSON\u2013serializable dictionary. Expects that request defines: \u2022 ground_truth_column \u2022 pred_column \u2022 Optional: n_bootstraps, alpha Source code in LabeLMaker/evaluate_handler.py 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 def fastapi_evaluate ( self , data : pd . DataFrame , request : Any ) -> Dict [ str , Any ]: \"\"\" Execute evaluation and return results as a JSON\u2013serializable dictionary. Expects that request defines: \u2022 ground_truth_column \u2022 pred_column \u2022 Optional: n_bootstraps, alpha \"\"\" ground_truth_col = request . ground_truth_column pred_col = request . pred_column n_bootstraps = getattr ( request , \"n_bootstraps\" , 1000 ) alpha = getattr ( request , \"alpha\" , 0.05 ) try : metrics_df , report_df , bs_df , cm_fig = self . evaluate_model ( data , pred_col , ground_truth_col , n_bootstraps = n_bootstraps , alpha = alpha ) response = { \"metrics\" : metrics_df . to_dict (), \"classification_report\" : report_df . to_dict (), \"bootstrap_confidence_intervals\" : bs_df . to_dict (), \"confusion_matrix\" : str ( cm_fig ), # Customize serialization as needed. } return response except Exception as e : return { \"error\" : str ( e )} StreamlitEvaluateHandler Bases: BaseEvaluateHandler Integrates the evaluation workflow with a Streamlit UI. Source code in LabeLMaker/evaluate_handler.py 131 132 133 134 135 136 137 138 class StreamlitEvaluateHandler ( BaseEvaluateHandler ): \"\"\" Integrates the evaluation workflow with a Streamlit UI. \"\"\" def __init__ ( self , ui_helper : Any , azure_key : str = None ) -> None : super () . __init__ ( azure_key = azure_key ) self . ui = ui_helper","title":"Evaluate"},{"location":"LabeLMaker/evaluate_handler.html#LabeLMaker.evaluate_handler.BaseEvaluateHandler","text":"Provides core evaluation functionality. Source code in LabeLMaker/evaluate_handler.py 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 class BaseEvaluateHandler : \"\"\" Provides core evaluation functionality. \"\"\" def __init__ ( self , azure_key : str = None ) -> None : self . azure_key = azure_key self . config = Config def _load_data ( self , uploaded_file : Any ) -> pd . DataFrame : try : if hasattr ( uploaded_file , \"seek\" ): uploaded_file . seek ( 0 ) # Reset file pointer before reading df = pd . read_csv ( uploaded_file ) if df . empty : raise Exception ( \"File appears to be empty or has no valid columns.\" ) return df except Exception as e : raise Exception ( f \"Error processing CSV file: { e } \" ) def evaluate_model ( self , df : pd . DataFrame , pred_col : str , ground_truth_col : str , n_bootstraps : int = 1000 , alpha : float = 0.05 , ) -> Tuple [ pd . DataFrame , pd . DataFrame , pd . DataFrame , Any ]: \"\"\" Evaluate model predictions and compute associated metrics. Returns a tuple: (metrics_df, classification_report_df, bootstrap_df, confusion_matrix_fig) \"\"\" if pred_col not in df . columns or ground_truth_col not in df . columns : raise ValueError ( \"Prediction or ground truth column not found in DataFrame.\" ) valid = df [ pred_col ] . notna () y_true = df . loc [ valid , ground_truth_col ] . astype ( str ) . tolist () y_pred = df . loc [ valid , pred_col ] . astype ( str ) . tolist () evaluator = Evaluator ( y_true , y_pred ) evaluator . calculate_metrics () metrics_df = evaluator . display_metrics () report_df = pd . DataFrame ( evaluator . metrics [ \"Classification Report\" ]) . transpose () bs_results = compute_bootstrap_confidence_intervals ( y_true , y_pred , n_bootstraps = n_bootstraps , alpha = alpha ) bs_display = [] for metric , values in bs_results . items (): if values [ \"Value\" ] is not None : bs_display . append ( { \"Metric\" : metric , \"Value\" : f \" { values [ 'Value' ] : .4f } \" , \"Bootstrap Mean\" : f \" { values [ 'Bootstrap Mean' ] : .4f } \" , \"95% CI\" : f \"( { values [ '95% CI' ][ 0 ] : .4f } , { values [ '95% CI' ][ 1 ] : .4f } )\" , } ) else : bs_display . append ( { \"Metric\" : metric , \"Value\" : \"Undefined\" , \"Bootstrap Mean\" : \"Undefined\" , \"95% CI\" : \"Undefined\" , } ) bs_df = pd . DataFrame ( bs_display ) cm_fig = evaluator . plot_confusion_matrix () return metrics_df , report_df , bs_df , cm_fig def compare_methods ( self , df : pd . DataFrame , ground_truth_col : str , selected_methods : list , ) -> Tuple [ pd . DataFrame , Dict [ str , Any ], Dict [ str , Any ]]: \"\"\" Compare prediction methods (e.g. Zero Shot, Few Shot, Many Shot) by evaluating predictions in multiple columns. Returns (common_df, results, confusion_matrices) \"\"\" # Map the method to the corresponding DataFrame column name. method_columns = { method : f \"Predicted Category ( { method } )\" for method in selected_methods } valid_methods = { m : col for m , col in method_columns . items () if col in df . columns } if not valid_methods : raise ValueError ( \"No selected method prediction columns exist in DataFrame.\" ) common_df = df . dropna ( subset = list ( valid_methods . values ())) results = {} confusion_matrices = {} for method , col in valid_methods . items (): y_true = common_df [ ground_truth_col ] . astype ( str ) . tolist () y_pred = common_df [ col ] . astype ( str ) . tolist () evaluator = Evaluator ( y_true , y_pred ) evaluator . calculate_metrics () results [ method ] = evaluator . metrics confusion_matrices [ method ] = evaluator . plot_confusion_matrix () return common_df , results , confusion_matrices","title":"BaseEvaluateHandler"},{"location":"LabeLMaker/evaluate_handler.html#LabeLMaker.evaluate_handler.BaseEvaluateHandler.compare_methods","text":"Compare prediction methods (e.g. Zero Shot, Few Shot, Many Shot) by evaluating predictions in multiple columns. Returns (common_df, results, confusion_matrices) Source code in LabeLMaker/evaluate_handler.py 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 def compare_methods ( self , df : pd . DataFrame , ground_truth_col : str , selected_methods : list , ) -> Tuple [ pd . DataFrame , Dict [ str , Any ], Dict [ str , Any ]]: \"\"\" Compare prediction methods (e.g. Zero Shot, Few Shot, Many Shot) by evaluating predictions in multiple columns. Returns (common_df, results, confusion_matrices) \"\"\" # Map the method to the corresponding DataFrame column name. method_columns = { method : f \"Predicted Category ( { method } )\" for method in selected_methods } valid_methods = { m : col for m , col in method_columns . items () if col in df . columns } if not valid_methods : raise ValueError ( \"No selected method prediction columns exist in DataFrame.\" ) common_df = df . dropna ( subset = list ( valid_methods . values ())) results = {} confusion_matrices = {} for method , col in valid_methods . items (): y_true = common_df [ ground_truth_col ] . astype ( str ) . tolist () y_pred = common_df [ col ] . astype ( str ) . tolist () evaluator = Evaluator ( y_true , y_pred ) evaluator . calculate_metrics () results [ method ] = evaluator . metrics confusion_matrices [ method ] = evaluator . plot_confusion_matrix () return common_df , results , confusion_matrices","title":"compare_methods"},{"location":"LabeLMaker/evaluate_handler.html#LabeLMaker.evaluate_handler.BaseEvaluateHandler.evaluate_model","text":"Evaluate model predictions and compute associated metrics. Returns a tuple: (metrics_df, classification_report_df, bootstrap_df, confusion_matrix_fig) Source code in LabeLMaker/evaluate_handler.py 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 def evaluate_model ( self , df : pd . DataFrame , pred_col : str , ground_truth_col : str , n_bootstraps : int = 1000 , alpha : float = 0.05 , ) -> Tuple [ pd . DataFrame , pd . DataFrame , pd . DataFrame , Any ]: \"\"\" Evaluate model predictions and compute associated metrics. Returns a tuple: (metrics_df, classification_report_df, bootstrap_df, confusion_matrix_fig) \"\"\" if pred_col not in df . columns or ground_truth_col not in df . columns : raise ValueError ( \"Prediction or ground truth column not found in DataFrame.\" ) valid = df [ pred_col ] . notna () y_true = df . loc [ valid , ground_truth_col ] . astype ( str ) . tolist () y_pred = df . loc [ valid , pred_col ] . astype ( str ) . tolist () evaluator = Evaluator ( y_true , y_pred ) evaluator . calculate_metrics () metrics_df = evaluator . display_metrics () report_df = pd . DataFrame ( evaluator . metrics [ \"Classification Report\" ]) . transpose () bs_results = compute_bootstrap_confidence_intervals ( y_true , y_pred , n_bootstraps = n_bootstraps , alpha = alpha ) bs_display = [] for metric , values in bs_results . items (): if values [ \"Value\" ] is not None : bs_display . append ( { \"Metric\" : metric , \"Value\" : f \" { values [ 'Value' ] : .4f } \" , \"Bootstrap Mean\" : f \" { values [ 'Bootstrap Mean' ] : .4f } \" , \"95% CI\" : f \"( { values [ '95% CI' ][ 0 ] : .4f } , { values [ '95% CI' ][ 1 ] : .4f } )\" , } ) else : bs_display . append ( { \"Metric\" : metric , \"Value\" : \"Undefined\" , \"Bootstrap Mean\" : \"Undefined\" , \"95% CI\" : \"Undefined\" , } ) bs_df = pd . DataFrame ( bs_display ) cm_fig = evaluator . plot_confusion_matrix () return metrics_df , report_df , bs_df , cm_fig","title":"evaluate_model"},{"location":"LabeLMaker/evaluate_handler.html#LabeLMaker.evaluate_handler.FastAPIEvaluateHandler","text":"Bases: BaseEvaluateHandler Provides a FastAPI\u2013friendly evaluation method. Source code in LabeLMaker/evaluate_handler.py 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 class FastAPIEvaluateHandler ( BaseEvaluateHandler ): \"\"\" Provides a FastAPI\u2013friendly evaluation method. \"\"\" def __init__ ( self , azure_key : str = None ) -> None : super () . __init__ ( azure_key = azure_key ) def fastapi_evaluate ( self , data : pd . DataFrame , request : Any ) -> Dict [ str , Any ]: \"\"\" Execute evaluation and return results as a JSON\u2013serializable dictionary. Expects that request defines: \u2022 ground_truth_column \u2022 pred_column \u2022 Optional: n_bootstraps, alpha \"\"\" ground_truth_col = request . ground_truth_column pred_col = request . pred_column n_bootstraps = getattr ( request , \"n_bootstraps\" , 1000 ) alpha = getattr ( request , \"alpha\" , 0.05 ) try : metrics_df , report_df , bs_df , cm_fig = self . evaluate_model ( data , pred_col , ground_truth_col , n_bootstraps = n_bootstraps , alpha = alpha ) response = { \"metrics\" : metrics_df . to_dict (), \"classification_report\" : report_df . to_dict (), \"bootstrap_confidence_intervals\" : bs_df . to_dict (), \"confusion_matrix\" : str ( cm_fig ), # Customize serialization as needed. } return response except Exception as e : return { \"error\" : str ( e )}","title":"FastAPIEvaluateHandler"},{"location":"LabeLMaker/evaluate_handler.html#LabeLMaker.evaluate_handler.FastAPIEvaluateHandler.fastapi_evaluate","text":"Execute evaluation and return results as a JSON\u2013serializable dictionary. Expects that request defines: \u2022 ground_truth_column \u2022 pred_column \u2022 Optional: n_bootstraps, alpha Source code in LabeLMaker/evaluate_handler.py 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 def fastapi_evaluate ( self , data : pd . DataFrame , request : Any ) -> Dict [ str , Any ]: \"\"\" Execute evaluation and return results as a JSON\u2013serializable dictionary. Expects that request defines: \u2022 ground_truth_column \u2022 pred_column \u2022 Optional: n_bootstraps, alpha \"\"\" ground_truth_col = request . ground_truth_column pred_col = request . pred_column n_bootstraps = getattr ( request , \"n_bootstraps\" , 1000 ) alpha = getattr ( request , \"alpha\" , 0.05 ) try : metrics_df , report_df , bs_df , cm_fig = self . evaluate_model ( data , pred_col , ground_truth_col , n_bootstraps = n_bootstraps , alpha = alpha ) response = { \"metrics\" : metrics_df . to_dict (), \"classification_report\" : report_df . to_dict (), \"bootstrap_confidence_intervals\" : bs_df . to_dict (), \"confusion_matrix\" : str ( cm_fig ), # Customize serialization as needed. } return response except Exception as e : return { \"error\" : str ( e )}","title":"fastapi_evaluate"},{"location":"LabeLMaker/evaluate_handler.html#LabeLMaker.evaluate_handler.StreamlitEvaluateHandler","text":"Bases: BaseEvaluateHandler Integrates the evaluation workflow with a Streamlit UI. Source code in LabeLMaker/evaluate_handler.py 131 132 133 134 135 136 137 138 class StreamlitEvaluateHandler ( BaseEvaluateHandler ): \"\"\" Integrates the evaluation workflow with a Streamlit UI. \"\"\" def __init__ ( self , ui_helper : Any , azure_key : str = None ) -> None : super () . __init__ ( azure_key = azure_key ) self . ui = ui_helper","title":"StreamlitEvaluateHandler"},{"location":"LabeLMaker/streamlit_interface.html","text":"Streamlit Interface This module connects the Streamlit UI components with the underlying evaluation and categorization workflows. It defines a BaseHandler with common UI helper functions, as well as an Evaluate class and a Categorize class that wrap the corresponding logic from evaluate_handler and categorizer_handler. BaseHandler Provides common helper functionality for both evaluation and categorization workflows. Source code in LabeLMaker/streamlit_interface.py 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 class BaseHandler : \"\"\" Provides common helper functionality for both evaluation and categorization workflows. \"\"\" def __init__ ( self , ui_helper : Any , azure_key : str = None ) -> None : \"\"\" Initialize with a UI helper instance and an optional azure_key. Args: ui_helper: An object providing Streamlit helper methods. azure_key: Optional Azure key for creating document analysis client. \"\"\" self . ui = ui_helper self . file_manager = FileManager ( azure_key ) def _ensure_file ( self , file : Any , upload_message : str , file_types : list , key : str , info_message : str , accept_multiple_files : bool = False , ) -> Any : \"\"\" Ensure file(s) are uploaded. If not, prompt the user. \"\"\" if file is None : file = self . ui . file_uploader ( label = upload_message , type = file_types , accept_multiple_files = accept_multiple_files , key = key , ) if not file : self . ui . info ( info_message ) return file def _load_data ( self , uploaded_file : Any ) -> pd . DataFrame : \"\"\" Process an uploaded file into a DataFrame using the FileManager. This function will work for the Streamlit and FastAPI contexts because FileManager delegates to the correct underlying UploadManager. \"\"\" try : # For Streamlit use process_file_upload, which uses StreamlitUploadManager. df , _ = self . file_manager . process_file_upload ( uploaded_file ) return df except Exception as e : raise Exception ( f \"Error processing CSV file: { e } \" ) def generate_docx_report_download ( self , doc : Any ) -> bytes : \"\"\" Convert a DOCX document into bytes for download. \"\"\" import io with io . BytesIO () as temp_stream : doc . save ( temp_stream ) temp_stream . seek ( 0 ) return temp_stream . read () __init__ ( ui_helper , azure_key = None ) Initialize with a UI helper instance and an optional azure_key. Parameters: ui_helper ( Any ) \u2013 An object providing Streamlit helper methods. azure_key ( str , default: None ) \u2013 Optional Azure key for creating document analysis client. Source code in LabeLMaker/streamlit_interface.py 32 33 34 35 36 37 38 39 40 41 def __init__ ( self , ui_helper : Any , azure_key : str = None ) -> None : \"\"\" Initialize with a UI helper instance and an optional azure_key. Args: ui_helper: An object providing Streamlit helper methods. azure_key: Optional Azure key for creating document analysis client. \"\"\" self . ui = ui_helper self . file_manager = FileManager ( azure_key ) generate_docx_report_download ( doc ) Convert a DOCX document into bytes for download. Source code in LabeLMaker/streamlit_interface.py 79 80 81 82 83 84 85 86 87 def generate_docx_report_download ( self , doc : Any ) -> bytes : \"\"\" Convert a DOCX document into bytes for download. \"\"\" import io with io . BytesIO () as temp_stream : doc . save ( temp_stream ) temp_stream . seek ( 0 ) return temp_stream . read () Categorize Bases: BaseHandler Wraps the categorization workflow. Source code in LabeLMaker/streamlit_interface.py 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 class Categorize ( BaseHandler ): \"\"\" Wraps the categorization workflow. \"\"\" def __init__ ( self , ui_helper : Any , azure_key : str = None ) -> None : \"\"\" Initialize the categorization handler. Args: ui_helper: An object providing Streamlit UI methods. azure_key: Optional Azure key if needed. \"\"\" super () . __init__ ( ui_helper ) from LabeLMaker.utils.file_manager import FileManager self . fm = FileManager ( azure_key ) # Used for any file operations self . config = Config self . cat_handler = StreamlitCategorizeHandler ( azure_key = azure_key ) def setup_workflow ( self , df : pd . DataFrame ) -> dict : \"\"\" Gather workflow parameters from the UI. Includes unique ID setup, mode selection, text column selection, etc. Args: df: The input DataFrame. Returns: A dictionary of workflow parameters. \"\"\" params = {} # Unique Identifier Setup if not self . ui . session_state . get ( \"uniqueIdSetup_done\" ): self . ui . markdown ( \"### Unique Identifier Setup\" ) id_choice = self . ui . radio ( \"How would you like to specify a unique identifier?\" , options = [ \"Create new ID column\" , \"Use an existing column\" ], index = 0 , key = \"id_choice\" , ) if id_choice == \"Create new ID column\" : new_id_col = self . ui . text_input ( \"Enter name for the new ID column\" , value = \"id\" , key = \"new_id_column\" ) if self . ui . button ( \"Create ID Column\" , key = \"create_id_column\" ): if new_id_col not in df . columns : df [ new_id_col ] = df . index . astype ( str ) self . ui . success ( f \"New ID column ' { new_id_col } ' created.\" ) else : self . ui . info ( f \"Column ' { new_id_col } ' already exists; using it.\" ) self . ui . session_state [ \"single_file_df\" ] = df self . ui . session_state [ \"selected_id_column\" ] = new_id_col self . ui . session_state [ \"uniqueIdSetup_done\" ] = True else : selected_existing = self . ui . selectbox ( \"Select the column to use as the unique identifier\" , options = df . columns . tolist (), key = \"existing_id_column\" , ) if self . ui . button ( \"Confirm ID Column\" , key = \"confirm_id_column\" ): self . ui . session_state [ \"selected_id_column\" ] = selected_existing self . ui . session_state [ \"uniqueIdSetup_done\" ] = True sel_id = self . ui . session_state . get ( \"selected_id_column\" ) if sel_id and sel_id not in df . columns : df [ sel_id ] = df . index . astype ( str ) self . ui . session_state [ \"single_file_df\" ] = df params [ \"index_column\" ] = sel_id self . ui . write ( f \"Using ' { params [ 'index_column' ] } ' as the unique identifier column.\" ) advanced_mode = self . ui . checkbox ( \"Advanced Mode\" , help = \"For users with ground truth labels. Check to run evaluation pipeline.\" , key = \"advanced_mode\" , ) params [ \"mode\" ] = \"Evaluation\" if advanced_mode else \"Production\" self . ui . info ( \"Advanced Mode activated.\" if advanced_mode else \"Software will automatically select labelling method\" ) df_columns = df . columns . tolist () params [ \"categorizing_column\" ] = self . ui . selectbox ( \"Select the column with text data you want to label\" , options = df_columns , key = \"categorizing_column\" ) if params [ \"mode\" ] == \"Evaluation\" : gt_col = self . ui . selectbox ( \"Select the column with ground truth labels\" , options = df_columns , key = \"ground_truth_column\" ) params [ \"ground_truth_column\" ] = gt_col eval_techniques = self . ui . multiselect ( \"Select the evaluation approaches to run:\" , options = [ \"Zero Shot\" , \"Few Shot\" , \"Many Shot\" ], default = [ \"Zero Shot\" ], key = \"evaluation_techniques\" , ) params [ \"evaluation_techniques\" ] = eval_techniques if \"Few Shot\" in eval_techniques : few_shot_count = self . ui . number_input ( \"Enter maximum examples per category (Few Shot)\" , min_value = 1 , value = 2 , key = \"few_shot_count\" ) params [ \"few_shot_count\" ] = few_shot_count if \"Many Shot\" in eval_techniques : many_shot_train_ratio = self . ui . number_input ( \"Enter train proportion for Many Shot (0 to 1)\" , min_value = 0.0 , max_value = 1.0 , value = 0.8 , key = \"many_shot_train_ratio\" ) params [ \"many_shot_train_ratio\" ] = many_shot_train_ratio else : ex_options = [ \"None\" ] + df_columns ex_col = self . ui . selectbox ( \"Select the column containing your examples (if any)\" , options = ex_options , key = \"examples_column\" ) params [ \"examples_column\" ] = None if ex_col == \"None\" else ex_col # Category definition using CategoryManager. from LabeLMaker.utils.category import CategoryManager default_col = params . get ( \"ground_truth_column\" ) if params [ \"mode\" ] == \"Evaluation\" else ( params . get ( \"examples_column\" ) or \"\" ) default_categories = \"\" if default_col and default_col in df . columns : unique_values = df [ default_col ] . dropna () . unique () if len ( unique_values ) <= Config . MAX_RECOMMENDED_GROUPS : default_categories = \",\" . join ([ str ( val ) for val in unique_values ]) else : self . ui . warning ( \"There are more than 10 unique values in the column. Auto-population of categories may not be practical.\" ) categories_dict , categories_with_descriptions = CategoryManager . define_categories ( self . ui , \"tab1\" , unique_values_str = default_categories ) params [ \"categories_dict\" ] = categories_dict params [ \"categories_with_descriptions\" ] = categories_with_descriptions return params def categorize_data ( self , df : pd . DataFrame , params : dict , zs_prompty : Path , fs_prompty : Path ) -> pd . DataFrame : \"\"\" Delegate categorization to the underlying StreamlitCategorizeHandler. Args: df: The input DataFrame. params: Dictionary of workflow parameters. zs_prompty: Path to the Zero Shot prompty file. fs_prompty: Path to the Few Shot prompty file. Returns: DataFrame with categorization results. \"\"\" return self . cat_handler . streamlit_categorize ( df , params , zs_prompty , fs_prompty ) def process_multiple_files ( self , uploaded_files : Any ) -> Tuple [ list , list ]: \"\"\" Process multiple file uploads using document analysis. Args: uploaded_files: List of uploaded file objects. Returns: A tuple of (filenames, texts) extracted from the files. \"\"\" filenames = [] texts = [] document_analysis_client = Config . DOCUMENT_ANALYSIS_CLIENT if hasattr ( Config , \"AZURE_DOCAI_KEY\" ) else None for file in uploaded_files : upload_manager = StreamlitUploadManager ( file , accept_multiple_files = True , document_analysis_client = document_analysis_client ) self . ui . spinner ( 'Reading in Files...' ) file_data , _ = upload_manager . process_upload () if file_data is not None : filenames . append ( file . name ) texts . append ( file_data ) return filenames , texts def display_results ( self , results : Any ) -> None : \"\"\" Display categorization results. Args: results: The categorization results to display. \"\"\" self . ui . write ( \"Categorization Results:\" ) self . ui . write ( results ) def handle_single_upload ( self , zs_prompty : Path = Path ( Config . ZS_PROMPTY ), fs_prompty : Path = Path ( Config . FS_PROMPTY )) -> None : \"\"\" Handle a single file upload for categorization. The document is processed only once and cached in session state to prevent repeated reads on refresh. Args: zs_prompty: Path to the Zero Shot prompty file. fs_prompty: Path to the Few Shot prompty file. \"\"\" file = self . _ensure_file ( file = None , upload_message = \"Upload your CSV/XLSX file\" , file_types = [ \"csv\" , \"xlsx\" ], key = \"single_file_uploader\" , info_message = \"Please upload a CSV or XLSX file to proceed.\" , ) if file is None : return # Process and cache the file only if not already cached. if \"single_file_df\" not in self . ui . session_state : try : document_analysis_client = Config . DOCUMENT_ANALYSIS_CLIENT if hasattr ( Config , \"AZURE_DOCAI_KEY\" ) else None upload_manager = StreamlitUploadManager ( file , accept_multiple_files = False , document_analysis_client = document_analysis_client ) file_data , _ = upload_manager . process_upload () self . ui . session_state [ \"single_file_df\" ] = file_data self . ui . session_state [ \"uploaded_file_single\" ] = file except Exception as e : self . ui . error ( f \"Error processing the uploaded file: { e } \" ) return df = self . ui . session_state [ \"single_file_df\" ] self . ui . write ( f \"Uploaded file: { file . name } \" ) ui_params = self . setup_workflow ( df ) if self . ui . button ( \"Categorize\" , key = \"tab1_submit\" ): merged_df = self . categorize_data ( df , ui_params , zs_prompty , fs_prompty ) csv_data = merged_df . to_csv ( index = False ) . encode ( \"utf-8\" ) self . ui . download_button ( label = \"Download Results\" , data = csv_data , file_name = \"AI_Generated_Categorization.csv\" , mime = \"text/csv\" , ) def handle_multiple_upload ( self , zs_prompty : Path = Path ( Config . ZS_PROMPTY ), fs_prompty : Path = Path ( Config . FS_PROMPTY )) -> None : \"\"\" Handle multiple file uploads for categorization. The processed file data is cached in session state so the files are not re-read on every refresh. Args: zs_prompty: Path to the Zero Shot prompty file. fs_prompty: Path to the Few Shot prompty file. \"\"\" if self . ui . button ( \"Clear All\" , key = \"multi_clear_all\" ): for key in ( \"uploaded_files_multiple\" , \"processed_files_multiple\" ): if key in self . ui . session_state : del self . ui . session_state [ key ] self . ui . rerun () files = self . _ensure_file ( file = None , upload_message = \"Upload your DOCX or PDF files\" , file_types = [ \"docx\" , \"pdf\" ], key = \"multiple_file_uploader\" , info_message = \"Please upload DOCX or PDF files to proceed.\" , accept_multiple_files = True , ) if files : # Cache processed files only once. if \"processed_files_multiple\" not in self . ui . session_state : self . ui . session_state [ \"uploaded_files_multiple\" ] = files file_names = [ file . name for file in files ] if isinstance ( files , list ) else [ files . name ] self . ui . write ( f \"Uploaded files: { file_names } \" ) filenames , texts = self . process_multiple_files ( files ) self . ui . session_state [ \"processed_files_multiple\" ] = ( filenames , texts ) if \"processed_files_multiple\" in self . ui . session_state : filenames , texts = self . ui . session_state [ \"processed_files_multiple\" ] from LabeLMaker.utils.category import CategoryManager categories_dict , examples = CategoryManager . define_categories ( self . ui , \"tab2\" , get_file_examples = True ) self . ui . write ( \"Texts (excerpts) to label:\" ) for text in texts [: 5 ]: self . ui . write ( text [: 250 ]) if self . ui . button ( \"Categorize Multiple Files\" , key = \"tab2_submit\" ): if examples : categorization_request = CategoryManager . create_request ( filenames , texts , categories_dict , examples ) from LabeLMaker.Categorize.fewshot import FewShotCategorizer few_shot_categorizer = FewShotCategorizer ( prompty_path = fs_prompty , category_request = categorization_request ) categorized_results = few_shot_categorizer . process () else : categorization_request = CategoryManager . create_request ( filenames , texts , categories_dict ) from LabeLMaker.Categorize.zeroshot import ZeroShotCategorizer zero_shot_categorizer = ZeroShotCategorizer ( prompty_path = zs_prompty , category_request = categorization_request ) categorized_results = zero_shot_categorizer . process () #self.display_results(categorized_results) #TODO make the next two lines their own function json_data = json . dumps ( categorized_results ) st . download_button ( label = \"Download Results\" , data = json_data , file_name = \"AI_Generated_Categorization_multi.json\" , mime = \"application/json\" ) __init__ ( ui_helper , azure_key = None ) Initialize the categorization handler. Parameters: ui_helper ( Any ) \u2013 An object providing Streamlit UI methods. azure_key ( str , default: None ) \u2013 Optional Azure key if needed. Source code in LabeLMaker/streamlit_interface.py 168 169 170 171 172 173 174 175 176 177 178 179 180 def __init__ ( self , ui_helper : Any , azure_key : str = None ) -> None : \"\"\" Initialize the categorization handler. Args: ui_helper: An object providing Streamlit UI methods. azure_key: Optional Azure key if needed. \"\"\" super () . __init__ ( ui_helper ) from LabeLMaker.utils.file_manager import FileManager self . fm = FileManager ( azure_key ) # Used for any file operations self . config = Config self . cat_handler = StreamlitCategorizeHandler ( azure_key = azure_key ) categorize_data ( df , params , zs_prompty , fs_prompty ) Delegate categorization to the underlying StreamlitCategorizeHandler. Parameters: df ( DataFrame ) \u2013 The input DataFrame. params ( dict ) \u2013 Dictionary of workflow parameters. zs_prompty ( Path ) \u2013 Path to the Zero Shot prompty file. fs_prompty ( Path ) \u2013 Path to the Few Shot prompty file. Returns: DataFrame \u2013 DataFrame with categorization results. Source code in LabeLMaker/streamlit_interface.py 293 294 295 296 297 298 299 300 301 302 303 304 305 306 def categorize_data ( self , df : pd . DataFrame , params : dict , zs_prompty : Path , fs_prompty : Path ) -> pd . DataFrame : \"\"\" Delegate categorization to the underlying StreamlitCategorizeHandler. Args: df: The input DataFrame. params: Dictionary of workflow parameters. zs_prompty: Path to the Zero Shot prompty file. fs_prompty: Path to the Few Shot prompty file. Returns: DataFrame with categorization results. \"\"\" return self . cat_handler . streamlit_categorize ( df , params , zs_prompty , fs_prompty ) display_results ( results ) Display categorization results. Parameters: results ( Any ) \u2013 The categorization results to display. Source code in LabeLMaker/streamlit_interface.py 330 331 332 333 334 335 336 337 338 def display_results ( self , results : Any ) -> None : \"\"\" Display categorization results. Args: results: The categorization results to display. \"\"\" self . ui . write ( \"Categorization Results:\" ) self . ui . write ( results ) handle_multiple_upload ( zs_prompty = Path ( Config . ZS_PROMPTY ), fs_prompty = Path ( Config . FS_PROMPTY )) Handle multiple file uploads for categorization. The processed file data is cached in session state so the files are not re-read on every refresh. Parameters: zs_prompty ( Path , default: Path ( ZS_PROMPTY ) ) \u2013 Path to the Zero Shot prompty file. fs_prompty ( Path , default: Path ( FS_PROMPTY ) ) \u2013 Path to the Few Shot prompty file. Source code in LabeLMaker/streamlit_interface.py 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 def handle_multiple_upload ( self , zs_prompty : Path = Path ( Config . ZS_PROMPTY ), fs_prompty : Path = Path ( Config . FS_PROMPTY )) -> None : \"\"\" Handle multiple file uploads for categorization. The processed file data is cached in session state so the files are not re-read on every refresh. Args: zs_prompty: Path to the Zero Shot prompty file. fs_prompty: Path to the Few Shot prompty file. \"\"\" if self . ui . button ( \"Clear All\" , key = \"multi_clear_all\" ): for key in ( \"uploaded_files_multiple\" , \"processed_files_multiple\" ): if key in self . ui . session_state : del self . ui . session_state [ key ] self . ui . rerun () files = self . _ensure_file ( file = None , upload_message = \"Upload your DOCX or PDF files\" , file_types = [ \"docx\" , \"pdf\" ], key = \"multiple_file_uploader\" , info_message = \"Please upload DOCX or PDF files to proceed.\" , accept_multiple_files = True , ) if files : # Cache processed files only once. if \"processed_files_multiple\" not in self . ui . session_state : self . ui . session_state [ \"uploaded_files_multiple\" ] = files file_names = [ file . name for file in files ] if isinstance ( files , list ) else [ files . name ] self . ui . write ( f \"Uploaded files: { file_names } \" ) filenames , texts = self . process_multiple_files ( files ) self . ui . session_state [ \"processed_files_multiple\" ] = ( filenames , texts ) if \"processed_files_multiple\" in self . ui . session_state : filenames , texts = self . ui . session_state [ \"processed_files_multiple\" ] from LabeLMaker.utils.category import CategoryManager categories_dict , examples = CategoryManager . define_categories ( self . ui , \"tab2\" , get_file_examples = True ) self . ui . write ( \"Texts (excerpts) to label:\" ) for text in texts [: 5 ]: self . ui . write ( text [: 250 ]) if self . ui . button ( \"Categorize Multiple Files\" , key = \"tab2_submit\" ): if examples : categorization_request = CategoryManager . create_request ( filenames , texts , categories_dict , examples ) from LabeLMaker.Categorize.fewshot import FewShotCategorizer few_shot_categorizer = FewShotCategorizer ( prompty_path = fs_prompty , category_request = categorization_request ) categorized_results = few_shot_categorizer . process () else : categorization_request = CategoryManager . create_request ( filenames , texts , categories_dict ) from LabeLMaker.Categorize.zeroshot import ZeroShotCategorizer zero_shot_categorizer = ZeroShotCategorizer ( prompty_path = zs_prompty , category_request = categorization_request ) categorized_results = zero_shot_categorizer . process () #self.display_results(categorized_results) #TODO make the next two lines their own function json_data = json . dumps ( categorized_results ) st . download_button ( label = \"Download Results\" , data = json_data , file_name = \"AI_Generated_Categorization_multi.json\" , mime = \"application/json\" ) handle_single_upload ( zs_prompty = Path ( Config . ZS_PROMPTY ), fs_prompty = Path ( Config . FS_PROMPTY )) Handle a single file upload for categorization. The document is processed only once and cached in session state to prevent repeated reads on refresh. Parameters: zs_prompty ( Path , default: Path ( ZS_PROMPTY ) ) \u2013 Path to the Zero Shot prompty file. fs_prompty ( Path , default: Path ( FS_PROMPTY ) ) \u2013 Path to the Few Shot prompty file. Source code in LabeLMaker/streamlit_interface.py 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 def handle_single_upload ( self , zs_prompty : Path = Path ( Config . ZS_PROMPTY ), fs_prompty : Path = Path ( Config . FS_PROMPTY )) -> None : \"\"\" Handle a single file upload for categorization. The document is processed only once and cached in session state to prevent repeated reads on refresh. Args: zs_prompty: Path to the Zero Shot prompty file. fs_prompty: Path to the Few Shot prompty file. \"\"\" file = self . _ensure_file ( file = None , upload_message = \"Upload your CSV/XLSX file\" , file_types = [ \"csv\" , \"xlsx\" ], key = \"single_file_uploader\" , info_message = \"Please upload a CSV or XLSX file to proceed.\" , ) if file is None : return # Process and cache the file only if not already cached. if \"single_file_df\" not in self . ui . session_state : try : document_analysis_client = Config . DOCUMENT_ANALYSIS_CLIENT if hasattr ( Config , \"AZURE_DOCAI_KEY\" ) else None upload_manager = StreamlitUploadManager ( file , accept_multiple_files = False , document_analysis_client = document_analysis_client ) file_data , _ = upload_manager . process_upload () self . ui . session_state [ \"single_file_df\" ] = file_data self . ui . session_state [ \"uploaded_file_single\" ] = file except Exception as e : self . ui . error ( f \"Error processing the uploaded file: { e } \" ) return df = self . ui . session_state [ \"single_file_df\" ] self . ui . write ( f \"Uploaded file: { file . name } \" ) ui_params = self . setup_workflow ( df ) if self . ui . button ( \"Categorize\" , key = \"tab1_submit\" ): merged_df = self . categorize_data ( df , ui_params , zs_prompty , fs_prompty ) csv_data = merged_df . to_csv ( index = False ) . encode ( \"utf-8\" ) self . ui . download_button ( label = \"Download Results\" , data = csv_data , file_name = \"AI_Generated_Categorization.csv\" , mime = \"text/csv\" , ) process_multiple_files ( uploaded_files ) Process multiple file uploads using document analysis. Parameters: uploaded_files ( Any ) \u2013 List of uploaded file objects. Returns: Tuple [ list , list ] \u2013 A tuple of (filenames, texts) extracted from the files. Source code in LabeLMaker/streamlit_interface.py 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 def process_multiple_files ( self , uploaded_files : Any ) -> Tuple [ list , list ]: \"\"\" Process multiple file uploads using document analysis. Args: uploaded_files: List of uploaded file objects. Returns: A tuple of (filenames, texts) extracted from the files. \"\"\" filenames = [] texts = [] document_analysis_client = Config . DOCUMENT_ANALYSIS_CLIENT if hasattr ( Config , \"AZURE_DOCAI_KEY\" ) else None for file in uploaded_files : upload_manager = StreamlitUploadManager ( file , accept_multiple_files = True , document_analysis_client = document_analysis_client ) self . ui . spinner ( 'Reading in Files...' ) file_data , _ = upload_manager . process_upload () if file_data is not None : filenames . append ( file . name ) texts . append ( file_data ) return filenames , texts setup_workflow ( df ) Gather workflow parameters from the UI. Includes unique ID setup, mode selection, text column selection, etc. Parameters: df ( DataFrame ) \u2013 The input DataFrame. Returns: dict \u2013 A dictionary of workflow parameters. Source code in LabeLMaker/streamlit_interface.py 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 def setup_workflow ( self , df : pd . DataFrame ) -> dict : \"\"\" Gather workflow parameters from the UI. Includes unique ID setup, mode selection, text column selection, etc. Args: df: The input DataFrame. Returns: A dictionary of workflow parameters. \"\"\" params = {} # Unique Identifier Setup if not self . ui . session_state . get ( \"uniqueIdSetup_done\" ): self . ui . markdown ( \"### Unique Identifier Setup\" ) id_choice = self . ui . radio ( \"How would you like to specify a unique identifier?\" , options = [ \"Create new ID column\" , \"Use an existing column\" ], index = 0 , key = \"id_choice\" , ) if id_choice == \"Create new ID column\" : new_id_col = self . ui . text_input ( \"Enter name for the new ID column\" , value = \"id\" , key = \"new_id_column\" ) if self . ui . button ( \"Create ID Column\" , key = \"create_id_column\" ): if new_id_col not in df . columns : df [ new_id_col ] = df . index . astype ( str ) self . ui . success ( f \"New ID column ' { new_id_col } ' created.\" ) else : self . ui . info ( f \"Column ' { new_id_col } ' already exists; using it.\" ) self . ui . session_state [ \"single_file_df\" ] = df self . ui . session_state [ \"selected_id_column\" ] = new_id_col self . ui . session_state [ \"uniqueIdSetup_done\" ] = True else : selected_existing = self . ui . selectbox ( \"Select the column to use as the unique identifier\" , options = df . columns . tolist (), key = \"existing_id_column\" , ) if self . ui . button ( \"Confirm ID Column\" , key = \"confirm_id_column\" ): self . ui . session_state [ \"selected_id_column\" ] = selected_existing self . ui . session_state [ \"uniqueIdSetup_done\" ] = True sel_id = self . ui . session_state . get ( \"selected_id_column\" ) if sel_id and sel_id not in df . columns : df [ sel_id ] = df . index . astype ( str ) self . ui . session_state [ \"single_file_df\" ] = df params [ \"index_column\" ] = sel_id self . ui . write ( f \"Using ' { params [ 'index_column' ] } ' as the unique identifier column.\" ) advanced_mode = self . ui . checkbox ( \"Advanced Mode\" , help = \"For users with ground truth labels. Check to run evaluation pipeline.\" , key = \"advanced_mode\" , ) params [ \"mode\" ] = \"Evaluation\" if advanced_mode else \"Production\" self . ui . info ( \"Advanced Mode activated.\" if advanced_mode else \"Software will automatically select labelling method\" ) df_columns = df . columns . tolist () params [ \"categorizing_column\" ] = self . ui . selectbox ( \"Select the column with text data you want to label\" , options = df_columns , key = \"categorizing_column\" ) if params [ \"mode\" ] == \"Evaluation\" : gt_col = self . ui . selectbox ( \"Select the column with ground truth labels\" , options = df_columns , key = \"ground_truth_column\" ) params [ \"ground_truth_column\" ] = gt_col eval_techniques = self . ui . multiselect ( \"Select the evaluation approaches to run:\" , options = [ \"Zero Shot\" , \"Few Shot\" , \"Many Shot\" ], default = [ \"Zero Shot\" ], key = \"evaluation_techniques\" , ) params [ \"evaluation_techniques\" ] = eval_techniques if \"Few Shot\" in eval_techniques : few_shot_count = self . ui . number_input ( \"Enter maximum examples per category (Few Shot)\" , min_value = 1 , value = 2 , key = \"few_shot_count\" ) params [ \"few_shot_count\" ] = few_shot_count if \"Many Shot\" in eval_techniques : many_shot_train_ratio = self . ui . number_input ( \"Enter train proportion for Many Shot (0 to 1)\" , min_value = 0.0 , max_value = 1.0 , value = 0.8 , key = \"many_shot_train_ratio\" ) params [ \"many_shot_train_ratio\" ] = many_shot_train_ratio else : ex_options = [ \"None\" ] + df_columns ex_col = self . ui . selectbox ( \"Select the column containing your examples (if any)\" , options = ex_options , key = \"examples_column\" ) params [ \"examples_column\" ] = None if ex_col == \"None\" else ex_col # Category definition using CategoryManager. from LabeLMaker.utils.category import CategoryManager default_col = params . get ( \"ground_truth_column\" ) if params [ \"mode\" ] == \"Evaluation\" else ( params . get ( \"examples_column\" ) or \"\" ) default_categories = \"\" if default_col and default_col in df . columns : unique_values = df [ default_col ] . dropna () . unique () if len ( unique_values ) <= Config . MAX_RECOMMENDED_GROUPS : default_categories = \",\" . join ([ str ( val ) for val in unique_values ]) else : self . ui . warning ( \"There are more than 10 unique values in the column. Auto-population of categories may not be practical.\" ) categories_dict , categories_with_descriptions = CategoryManager . define_categories ( self . ui , \"tab1\" , unique_values_str = default_categories ) params [ \"categories_dict\" ] = categories_dict params [ \"categories_with_descriptions\" ] = categories_with_descriptions return params Evaluate Bases: BaseHandler Wraps the evaluation workflow. Source code in LabeLMaker/streamlit_interface.py 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 class Evaluate ( BaseHandler ): \"\"\" Wraps the evaluation workflow. \"\"\" def __init__ ( self , ui_helper : Any , azure_key : str = None ) -> None : super () . __init__ ( ui_helper , azure_key = azure_key ) self . eval_handler = StreamlitEvaluateHandler ( ui_helper , azure_key = azure_key ) def handle_evaluation ( self ) -> None : \"\"\" Execute the evaluation workflow. \"\"\" file = self . _ensure_file ( file = None , upload_message = \"Upload a CSV file for Evaluation\" , file_types = [ \"csv\" ], key = \"eval_file_uploader_handler\" , info_message = \"Please upload a CSV file to proceed.\" , ) if file is None : return try : df = self . _load_data ( file ) except Exception as error : self . ui . error ( f \"Error reading file: { error } \" ) return self . ui . subheader ( \"CSV Preview:\" ) self . ui . write ( df . head ()) self . ui . write ( f \"Total rows in dataframe: { len ( df ) } \" ) # Let the user select the ground truth column and evaluation methods. ground_truth_col = self . ui . selectbox ( \"Select Ground Truth Column\" , df . columns . tolist (), key = \"eval_gt_column\" ) selected_methods = self . ui . multiselect ( \"Select evaluation methods\" , [ \"Zero Shot\" , \"Few Shot\" , \"Many Shot\" ], default = [ \"Zero Shot\" , \"Few Shot\" , \"Many Shot\" ], key = \"eval_methods\" , ) # Move the button to the UI layer. if self . ui . button ( \"Calculate Results\" , key = \"calc_results_button\" ): # Use a spinner if desired. with self . ui . spinner ( \"Evaluating\" ): try : # Call your evaluation function (i.e. a pure function) common_df , results , confusion_matrices = self . eval_handler . compare_methods ( df , ground_truth_col , selected_methods ) # Now display the results. for method , metrics in results . items (): self . ui . subheader ( method ) self . ui . write ( metrics ) # Create and display a DOCX download button. docx_maker = StreamlitDocxCreator ( results , confusion_matrices ) doc = docx_maker . create_docx_report () docx_content = self . generate_docx_report_download ( doc ) self . ui . download_button ( label = \"Download DOCX Report\" , data = docx_content , file_name = \"evaluation_report.docx\" , mime = \"application/vnd.openxmlformats-officedocument.wordprocessingml.document\" , ) except Exception as e : self . ui . error ( f \"Error during evaluation: { e } \" ) handle_evaluation () Execute the evaluation workflow. Source code in LabeLMaker/streamlit_interface.py 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 def handle_evaluation ( self ) -> None : \"\"\" Execute the evaluation workflow. \"\"\" file = self . _ensure_file ( file = None , upload_message = \"Upload a CSV file for Evaluation\" , file_types = [ \"csv\" ], key = \"eval_file_uploader_handler\" , info_message = \"Please upload a CSV file to proceed.\" , ) if file is None : return try : df = self . _load_data ( file ) except Exception as error : self . ui . error ( f \"Error reading file: { error } \" ) return self . ui . subheader ( \"CSV Preview:\" ) self . ui . write ( df . head ()) self . ui . write ( f \"Total rows in dataframe: { len ( df ) } \" ) # Let the user select the ground truth column and evaluation methods. ground_truth_col = self . ui . selectbox ( \"Select Ground Truth Column\" , df . columns . tolist (), key = \"eval_gt_column\" ) selected_methods = self . ui . multiselect ( \"Select evaluation methods\" , [ \"Zero Shot\" , \"Few Shot\" , \"Many Shot\" ], default = [ \"Zero Shot\" , \"Few Shot\" , \"Many Shot\" ], key = \"eval_methods\" , ) # Move the button to the UI layer. if self . ui . button ( \"Calculate Results\" , key = \"calc_results_button\" ): # Use a spinner if desired. with self . ui . spinner ( \"Evaluating\" ): try : # Call your evaluation function (i.e. a pure function) common_df , results , confusion_matrices = self . eval_handler . compare_methods ( df , ground_truth_col , selected_methods ) # Now display the results. for method , metrics in results . items (): self . ui . subheader ( method ) self . ui . write ( metrics ) # Create and display a DOCX download button. docx_maker = StreamlitDocxCreator ( results , confusion_matrices ) doc = docx_maker . create_docx_report () docx_content = self . generate_docx_report_download ( doc ) self . ui . download_button ( label = \"Download DOCX Report\" , data = docx_content , file_name = \"evaluation_report.docx\" , mime = \"application/vnd.openxmlformats-officedocument.wordprocessingml.document\" , ) except Exception as e : self . ui . error ( f \"Error during evaluation: { e } \" )","title":"Streamlit"},{"location":"LabeLMaker/streamlit_interface.html#LabeLMaker.streamlit_interface.BaseHandler","text":"Provides common helper functionality for both evaluation and categorization workflows. Source code in LabeLMaker/streamlit_interface.py 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 class BaseHandler : \"\"\" Provides common helper functionality for both evaluation and categorization workflows. \"\"\" def __init__ ( self , ui_helper : Any , azure_key : str = None ) -> None : \"\"\" Initialize with a UI helper instance and an optional azure_key. Args: ui_helper: An object providing Streamlit helper methods. azure_key: Optional Azure key for creating document analysis client. \"\"\" self . ui = ui_helper self . file_manager = FileManager ( azure_key ) def _ensure_file ( self , file : Any , upload_message : str , file_types : list , key : str , info_message : str , accept_multiple_files : bool = False , ) -> Any : \"\"\" Ensure file(s) are uploaded. If not, prompt the user. \"\"\" if file is None : file = self . ui . file_uploader ( label = upload_message , type = file_types , accept_multiple_files = accept_multiple_files , key = key , ) if not file : self . ui . info ( info_message ) return file def _load_data ( self , uploaded_file : Any ) -> pd . DataFrame : \"\"\" Process an uploaded file into a DataFrame using the FileManager. This function will work for the Streamlit and FastAPI contexts because FileManager delegates to the correct underlying UploadManager. \"\"\" try : # For Streamlit use process_file_upload, which uses StreamlitUploadManager. df , _ = self . file_manager . process_file_upload ( uploaded_file ) return df except Exception as e : raise Exception ( f \"Error processing CSV file: { e } \" ) def generate_docx_report_download ( self , doc : Any ) -> bytes : \"\"\" Convert a DOCX document into bytes for download. \"\"\" import io with io . BytesIO () as temp_stream : doc . save ( temp_stream ) temp_stream . seek ( 0 ) return temp_stream . read ()","title":"BaseHandler"},{"location":"LabeLMaker/streamlit_interface.html#LabeLMaker.streamlit_interface.BaseHandler.__init__","text":"Initialize with a UI helper instance and an optional azure_key. Parameters: ui_helper ( Any ) \u2013 An object providing Streamlit helper methods. azure_key ( str , default: None ) \u2013 Optional Azure key for creating document analysis client. Source code in LabeLMaker/streamlit_interface.py 32 33 34 35 36 37 38 39 40 41 def __init__ ( self , ui_helper : Any , azure_key : str = None ) -> None : \"\"\" Initialize with a UI helper instance and an optional azure_key. Args: ui_helper: An object providing Streamlit helper methods. azure_key: Optional Azure key for creating document analysis client. \"\"\" self . ui = ui_helper self . file_manager = FileManager ( azure_key )","title":"__init__"},{"location":"LabeLMaker/streamlit_interface.html#LabeLMaker.streamlit_interface.BaseHandler.generate_docx_report_download","text":"Convert a DOCX document into bytes for download. Source code in LabeLMaker/streamlit_interface.py 79 80 81 82 83 84 85 86 87 def generate_docx_report_download ( self , doc : Any ) -> bytes : \"\"\" Convert a DOCX document into bytes for download. \"\"\" import io with io . BytesIO () as temp_stream : doc . save ( temp_stream ) temp_stream . seek ( 0 ) return temp_stream . read ()","title":"generate_docx_report_download"},{"location":"LabeLMaker/streamlit_interface.html#LabeLMaker.streamlit_interface.Categorize","text":"Bases: BaseHandler Wraps the categorization workflow. Source code in LabeLMaker/streamlit_interface.py 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 class Categorize ( BaseHandler ): \"\"\" Wraps the categorization workflow. \"\"\" def __init__ ( self , ui_helper : Any , azure_key : str = None ) -> None : \"\"\" Initialize the categorization handler. Args: ui_helper: An object providing Streamlit UI methods. azure_key: Optional Azure key if needed. \"\"\" super () . __init__ ( ui_helper ) from LabeLMaker.utils.file_manager import FileManager self . fm = FileManager ( azure_key ) # Used for any file operations self . config = Config self . cat_handler = StreamlitCategorizeHandler ( azure_key = azure_key ) def setup_workflow ( self , df : pd . DataFrame ) -> dict : \"\"\" Gather workflow parameters from the UI. Includes unique ID setup, mode selection, text column selection, etc. Args: df: The input DataFrame. Returns: A dictionary of workflow parameters. \"\"\" params = {} # Unique Identifier Setup if not self . ui . session_state . get ( \"uniqueIdSetup_done\" ): self . ui . markdown ( \"### Unique Identifier Setup\" ) id_choice = self . ui . radio ( \"How would you like to specify a unique identifier?\" , options = [ \"Create new ID column\" , \"Use an existing column\" ], index = 0 , key = \"id_choice\" , ) if id_choice == \"Create new ID column\" : new_id_col = self . ui . text_input ( \"Enter name for the new ID column\" , value = \"id\" , key = \"new_id_column\" ) if self . ui . button ( \"Create ID Column\" , key = \"create_id_column\" ): if new_id_col not in df . columns : df [ new_id_col ] = df . index . astype ( str ) self . ui . success ( f \"New ID column ' { new_id_col } ' created.\" ) else : self . ui . info ( f \"Column ' { new_id_col } ' already exists; using it.\" ) self . ui . session_state [ \"single_file_df\" ] = df self . ui . session_state [ \"selected_id_column\" ] = new_id_col self . ui . session_state [ \"uniqueIdSetup_done\" ] = True else : selected_existing = self . ui . selectbox ( \"Select the column to use as the unique identifier\" , options = df . columns . tolist (), key = \"existing_id_column\" , ) if self . ui . button ( \"Confirm ID Column\" , key = \"confirm_id_column\" ): self . ui . session_state [ \"selected_id_column\" ] = selected_existing self . ui . session_state [ \"uniqueIdSetup_done\" ] = True sel_id = self . ui . session_state . get ( \"selected_id_column\" ) if sel_id and sel_id not in df . columns : df [ sel_id ] = df . index . astype ( str ) self . ui . session_state [ \"single_file_df\" ] = df params [ \"index_column\" ] = sel_id self . ui . write ( f \"Using ' { params [ 'index_column' ] } ' as the unique identifier column.\" ) advanced_mode = self . ui . checkbox ( \"Advanced Mode\" , help = \"For users with ground truth labels. Check to run evaluation pipeline.\" , key = \"advanced_mode\" , ) params [ \"mode\" ] = \"Evaluation\" if advanced_mode else \"Production\" self . ui . info ( \"Advanced Mode activated.\" if advanced_mode else \"Software will automatically select labelling method\" ) df_columns = df . columns . tolist () params [ \"categorizing_column\" ] = self . ui . selectbox ( \"Select the column with text data you want to label\" , options = df_columns , key = \"categorizing_column\" ) if params [ \"mode\" ] == \"Evaluation\" : gt_col = self . ui . selectbox ( \"Select the column with ground truth labels\" , options = df_columns , key = \"ground_truth_column\" ) params [ \"ground_truth_column\" ] = gt_col eval_techniques = self . ui . multiselect ( \"Select the evaluation approaches to run:\" , options = [ \"Zero Shot\" , \"Few Shot\" , \"Many Shot\" ], default = [ \"Zero Shot\" ], key = \"evaluation_techniques\" , ) params [ \"evaluation_techniques\" ] = eval_techniques if \"Few Shot\" in eval_techniques : few_shot_count = self . ui . number_input ( \"Enter maximum examples per category (Few Shot)\" , min_value = 1 , value = 2 , key = \"few_shot_count\" ) params [ \"few_shot_count\" ] = few_shot_count if \"Many Shot\" in eval_techniques : many_shot_train_ratio = self . ui . number_input ( \"Enter train proportion for Many Shot (0 to 1)\" , min_value = 0.0 , max_value = 1.0 , value = 0.8 , key = \"many_shot_train_ratio\" ) params [ \"many_shot_train_ratio\" ] = many_shot_train_ratio else : ex_options = [ \"None\" ] + df_columns ex_col = self . ui . selectbox ( \"Select the column containing your examples (if any)\" , options = ex_options , key = \"examples_column\" ) params [ \"examples_column\" ] = None if ex_col == \"None\" else ex_col # Category definition using CategoryManager. from LabeLMaker.utils.category import CategoryManager default_col = params . get ( \"ground_truth_column\" ) if params [ \"mode\" ] == \"Evaluation\" else ( params . get ( \"examples_column\" ) or \"\" ) default_categories = \"\" if default_col and default_col in df . columns : unique_values = df [ default_col ] . dropna () . unique () if len ( unique_values ) <= Config . MAX_RECOMMENDED_GROUPS : default_categories = \",\" . join ([ str ( val ) for val in unique_values ]) else : self . ui . warning ( \"There are more than 10 unique values in the column. Auto-population of categories may not be practical.\" ) categories_dict , categories_with_descriptions = CategoryManager . define_categories ( self . ui , \"tab1\" , unique_values_str = default_categories ) params [ \"categories_dict\" ] = categories_dict params [ \"categories_with_descriptions\" ] = categories_with_descriptions return params def categorize_data ( self , df : pd . DataFrame , params : dict , zs_prompty : Path , fs_prompty : Path ) -> pd . DataFrame : \"\"\" Delegate categorization to the underlying StreamlitCategorizeHandler. Args: df: The input DataFrame. params: Dictionary of workflow parameters. zs_prompty: Path to the Zero Shot prompty file. fs_prompty: Path to the Few Shot prompty file. Returns: DataFrame with categorization results. \"\"\" return self . cat_handler . streamlit_categorize ( df , params , zs_prompty , fs_prompty ) def process_multiple_files ( self , uploaded_files : Any ) -> Tuple [ list , list ]: \"\"\" Process multiple file uploads using document analysis. Args: uploaded_files: List of uploaded file objects. Returns: A tuple of (filenames, texts) extracted from the files. \"\"\" filenames = [] texts = [] document_analysis_client = Config . DOCUMENT_ANALYSIS_CLIENT if hasattr ( Config , \"AZURE_DOCAI_KEY\" ) else None for file in uploaded_files : upload_manager = StreamlitUploadManager ( file , accept_multiple_files = True , document_analysis_client = document_analysis_client ) self . ui . spinner ( 'Reading in Files...' ) file_data , _ = upload_manager . process_upload () if file_data is not None : filenames . append ( file . name ) texts . append ( file_data ) return filenames , texts def display_results ( self , results : Any ) -> None : \"\"\" Display categorization results. Args: results: The categorization results to display. \"\"\" self . ui . write ( \"Categorization Results:\" ) self . ui . write ( results ) def handle_single_upload ( self , zs_prompty : Path = Path ( Config . ZS_PROMPTY ), fs_prompty : Path = Path ( Config . FS_PROMPTY )) -> None : \"\"\" Handle a single file upload for categorization. The document is processed only once and cached in session state to prevent repeated reads on refresh. Args: zs_prompty: Path to the Zero Shot prompty file. fs_prompty: Path to the Few Shot prompty file. \"\"\" file = self . _ensure_file ( file = None , upload_message = \"Upload your CSV/XLSX file\" , file_types = [ \"csv\" , \"xlsx\" ], key = \"single_file_uploader\" , info_message = \"Please upload a CSV or XLSX file to proceed.\" , ) if file is None : return # Process and cache the file only if not already cached. if \"single_file_df\" not in self . ui . session_state : try : document_analysis_client = Config . DOCUMENT_ANALYSIS_CLIENT if hasattr ( Config , \"AZURE_DOCAI_KEY\" ) else None upload_manager = StreamlitUploadManager ( file , accept_multiple_files = False , document_analysis_client = document_analysis_client ) file_data , _ = upload_manager . process_upload () self . ui . session_state [ \"single_file_df\" ] = file_data self . ui . session_state [ \"uploaded_file_single\" ] = file except Exception as e : self . ui . error ( f \"Error processing the uploaded file: { e } \" ) return df = self . ui . session_state [ \"single_file_df\" ] self . ui . write ( f \"Uploaded file: { file . name } \" ) ui_params = self . setup_workflow ( df ) if self . ui . button ( \"Categorize\" , key = \"tab1_submit\" ): merged_df = self . categorize_data ( df , ui_params , zs_prompty , fs_prompty ) csv_data = merged_df . to_csv ( index = False ) . encode ( \"utf-8\" ) self . ui . download_button ( label = \"Download Results\" , data = csv_data , file_name = \"AI_Generated_Categorization.csv\" , mime = \"text/csv\" , ) def handle_multiple_upload ( self , zs_prompty : Path = Path ( Config . ZS_PROMPTY ), fs_prompty : Path = Path ( Config . FS_PROMPTY )) -> None : \"\"\" Handle multiple file uploads for categorization. The processed file data is cached in session state so the files are not re-read on every refresh. Args: zs_prompty: Path to the Zero Shot prompty file. fs_prompty: Path to the Few Shot prompty file. \"\"\" if self . ui . button ( \"Clear All\" , key = \"multi_clear_all\" ): for key in ( \"uploaded_files_multiple\" , \"processed_files_multiple\" ): if key in self . ui . session_state : del self . ui . session_state [ key ] self . ui . rerun () files = self . _ensure_file ( file = None , upload_message = \"Upload your DOCX or PDF files\" , file_types = [ \"docx\" , \"pdf\" ], key = \"multiple_file_uploader\" , info_message = \"Please upload DOCX or PDF files to proceed.\" , accept_multiple_files = True , ) if files : # Cache processed files only once. if \"processed_files_multiple\" not in self . ui . session_state : self . ui . session_state [ \"uploaded_files_multiple\" ] = files file_names = [ file . name for file in files ] if isinstance ( files , list ) else [ files . name ] self . ui . write ( f \"Uploaded files: { file_names } \" ) filenames , texts = self . process_multiple_files ( files ) self . ui . session_state [ \"processed_files_multiple\" ] = ( filenames , texts ) if \"processed_files_multiple\" in self . ui . session_state : filenames , texts = self . ui . session_state [ \"processed_files_multiple\" ] from LabeLMaker.utils.category import CategoryManager categories_dict , examples = CategoryManager . define_categories ( self . ui , \"tab2\" , get_file_examples = True ) self . ui . write ( \"Texts (excerpts) to label:\" ) for text in texts [: 5 ]: self . ui . write ( text [: 250 ]) if self . ui . button ( \"Categorize Multiple Files\" , key = \"tab2_submit\" ): if examples : categorization_request = CategoryManager . create_request ( filenames , texts , categories_dict , examples ) from LabeLMaker.Categorize.fewshot import FewShotCategorizer few_shot_categorizer = FewShotCategorizer ( prompty_path = fs_prompty , category_request = categorization_request ) categorized_results = few_shot_categorizer . process () else : categorization_request = CategoryManager . create_request ( filenames , texts , categories_dict ) from LabeLMaker.Categorize.zeroshot import ZeroShotCategorizer zero_shot_categorizer = ZeroShotCategorizer ( prompty_path = zs_prompty , category_request = categorization_request ) categorized_results = zero_shot_categorizer . process () #self.display_results(categorized_results) #TODO make the next two lines their own function json_data = json . dumps ( categorized_results ) st . download_button ( label = \"Download Results\" , data = json_data , file_name = \"AI_Generated_Categorization_multi.json\" , mime = \"application/json\" )","title":"Categorize"},{"location":"LabeLMaker/streamlit_interface.html#LabeLMaker.streamlit_interface.Categorize.__init__","text":"Initialize the categorization handler. Parameters: ui_helper ( Any ) \u2013 An object providing Streamlit UI methods. azure_key ( str , default: None ) \u2013 Optional Azure key if needed. Source code in LabeLMaker/streamlit_interface.py 168 169 170 171 172 173 174 175 176 177 178 179 180 def __init__ ( self , ui_helper : Any , azure_key : str = None ) -> None : \"\"\" Initialize the categorization handler. Args: ui_helper: An object providing Streamlit UI methods. azure_key: Optional Azure key if needed. \"\"\" super () . __init__ ( ui_helper ) from LabeLMaker.utils.file_manager import FileManager self . fm = FileManager ( azure_key ) # Used for any file operations self . config = Config self . cat_handler = StreamlitCategorizeHandler ( azure_key = azure_key )","title":"__init__"},{"location":"LabeLMaker/streamlit_interface.html#LabeLMaker.streamlit_interface.Categorize.categorize_data","text":"Delegate categorization to the underlying StreamlitCategorizeHandler. Parameters: df ( DataFrame ) \u2013 The input DataFrame. params ( dict ) \u2013 Dictionary of workflow parameters. zs_prompty ( Path ) \u2013 Path to the Zero Shot prompty file. fs_prompty ( Path ) \u2013 Path to the Few Shot prompty file. Returns: DataFrame \u2013 DataFrame with categorization results. Source code in LabeLMaker/streamlit_interface.py 293 294 295 296 297 298 299 300 301 302 303 304 305 306 def categorize_data ( self , df : pd . DataFrame , params : dict , zs_prompty : Path , fs_prompty : Path ) -> pd . DataFrame : \"\"\" Delegate categorization to the underlying StreamlitCategorizeHandler. Args: df: The input DataFrame. params: Dictionary of workflow parameters. zs_prompty: Path to the Zero Shot prompty file. fs_prompty: Path to the Few Shot prompty file. Returns: DataFrame with categorization results. \"\"\" return self . cat_handler . streamlit_categorize ( df , params , zs_prompty , fs_prompty )","title":"categorize_data"},{"location":"LabeLMaker/streamlit_interface.html#LabeLMaker.streamlit_interface.Categorize.display_results","text":"Display categorization results. Parameters: results ( Any ) \u2013 The categorization results to display. Source code in LabeLMaker/streamlit_interface.py 330 331 332 333 334 335 336 337 338 def display_results ( self , results : Any ) -> None : \"\"\" Display categorization results. Args: results: The categorization results to display. \"\"\" self . ui . write ( \"Categorization Results:\" ) self . ui . write ( results )","title":"display_results"},{"location":"LabeLMaker/streamlit_interface.html#LabeLMaker.streamlit_interface.Categorize.handle_multiple_upload","text":"Handle multiple file uploads for categorization. The processed file data is cached in session state so the files are not re-read on every refresh. Parameters: zs_prompty ( Path , default: Path ( ZS_PROMPTY ) ) \u2013 Path to the Zero Shot prompty file. fs_prompty ( Path , default: Path ( FS_PROMPTY ) ) \u2013 Path to the Few Shot prompty file. Source code in LabeLMaker/streamlit_interface.py 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 def handle_multiple_upload ( self , zs_prompty : Path = Path ( Config . ZS_PROMPTY ), fs_prompty : Path = Path ( Config . FS_PROMPTY )) -> None : \"\"\" Handle multiple file uploads for categorization. The processed file data is cached in session state so the files are not re-read on every refresh. Args: zs_prompty: Path to the Zero Shot prompty file. fs_prompty: Path to the Few Shot prompty file. \"\"\" if self . ui . button ( \"Clear All\" , key = \"multi_clear_all\" ): for key in ( \"uploaded_files_multiple\" , \"processed_files_multiple\" ): if key in self . ui . session_state : del self . ui . session_state [ key ] self . ui . rerun () files = self . _ensure_file ( file = None , upload_message = \"Upload your DOCX or PDF files\" , file_types = [ \"docx\" , \"pdf\" ], key = \"multiple_file_uploader\" , info_message = \"Please upload DOCX or PDF files to proceed.\" , accept_multiple_files = True , ) if files : # Cache processed files only once. if \"processed_files_multiple\" not in self . ui . session_state : self . ui . session_state [ \"uploaded_files_multiple\" ] = files file_names = [ file . name for file in files ] if isinstance ( files , list ) else [ files . name ] self . ui . write ( f \"Uploaded files: { file_names } \" ) filenames , texts = self . process_multiple_files ( files ) self . ui . session_state [ \"processed_files_multiple\" ] = ( filenames , texts ) if \"processed_files_multiple\" in self . ui . session_state : filenames , texts = self . ui . session_state [ \"processed_files_multiple\" ] from LabeLMaker.utils.category import CategoryManager categories_dict , examples = CategoryManager . define_categories ( self . ui , \"tab2\" , get_file_examples = True ) self . ui . write ( \"Texts (excerpts) to label:\" ) for text in texts [: 5 ]: self . ui . write ( text [: 250 ]) if self . ui . button ( \"Categorize Multiple Files\" , key = \"tab2_submit\" ): if examples : categorization_request = CategoryManager . create_request ( filenames , texts , categories_dict , examples ) from LabeLMaker.Categorize.fewshot import FewShotCategorizer few_shot_categorizer = FewShotCategorizer ( prompty_path = fs_prompty , category_request = categorization_request ) categorized_results = few_shot_categorizer . process () else : categorization_request = CategoryManager . create_request ( filenames , texts , categories_dict ) from LabeLMaker.Categorize.zeroshot import ZeroShotCategorizer zero_shot_categorizer = ZeroShotCategorizer ( prompty_path = zs_prompty , category_request = categorization_request ) categorized_results = zero_shot_categorizer . process () #self.display_results(categorized_results) #TODO make the next two lines their own function json_data = json . dumps ( categorized_results ) st . download_button ( label = \"Download Results\" , data = json_data , file_name = \"AI_Generated_Categorization_multi.json\" , mime = \"application/json\" )","title":"handle_multiple_upload"},{"location":"LabeLMaker/streamlit_interface.html#LabeLMaker.streamlit_interface.Categorize.handle_single_upload","text":"Handle a single file upload for categorization. The document is processed only once and cached in session state to prevent repeated reads on refresh. Parameters: zs_prompty ( Path , default: Path ( ZS_PROMPTY ) ) \u2013 Path to the Zero Shot prompty file. fs_prompty ( Path , default: Path ( FS_PROMPTY ) ) \u2013 Path to the Few Shot prompty file. Source code in LabeLMaker/streamlit_interface.py 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 def handle_single_upload ( self , zs_prompty : Path = Path ( Config . ZS_PROMPTY ), fs_prompty : Path = Path ( Config . FS_PROMPTY )) -> None : \"\"\" Handle a single file upload for categorization. The document is processed only once and cached in session state to prevent repeated reads on refresh. Args: zs_prompty: Path to the Zero Shot prompty file. fs_prompty: Path to the Few Shot prompty file. \"\"\" file = self . _ensure_file ( file = None , upload_message = \"Upload your CSV/XLSX file\" , file_types = [ \"csv\" , \"xlsx\" ], key = \"single_file_uploader\" , info_message = \"Please upload a CSV or XLSX file to proceed.\" , ) if file is None : return # Process and cache the file only if not already cached. if \"single_file_df\" not in self . ui . session_state : try : document_analysis_client = Config . DOCUMENT_ANALYSIS_CLIENT if hasattr ( Config , \"AZURE_DOCAI_KEY\" ) else None upload_manager = StreamlitUploadManager ( file , accept_multiple_files = False , document_analysis_client = document_analysis_client ) file_data , _ = upload_manager . process_upload () self . ui . session_state [ \"single_file_df\" ] = file_data self . ui . session_state [ \"uploaded_file_single\" ] = file except Exception as e : self . ui . error ( f \"Error processing the uploaded file: { e } \" ) return df = self . ui . session_state [ \"single_file_df\" ] self . ui . write ( f \"Uploaded file: { file . name } \" ) ui_params = self . setup_workflow ( df ) if self . ui . button ( \"Categorize\" , key = \"tab1_submit\" ): merged_df = self . categorize_data ( df , ui_params , zs_prompty , fs_prompty ) csv_data = merged_df . to_csv ( index = False ) . encode ( \"utf-8\" ) self . ui . download_button ( label = \"Download Results\" , data = csv_data , file_name = \"AI_Generated_Categorization.csv\" , mime = \"text/csv\" , )","title":"handle_single_upload"},{"location":"LabeLMaker/streamlit_interface.html#LabeLMaker.streamlit_interface.Categorize.process_multiple_files","text":"Process multiple file uploads using document analysis. Parameters: uploaded_files ( Any ) \u2013 List of uploaded file objects. Returns: Tuple [ list , list ] \u2013 A tuple of (filenames, texts) extracted from the files. Source code in LabeLMaker/streamlit_interface.py 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 def process_multiple_files ( self , uploaded_files : Any ) -> Tuple [ list , list ]: \"\"\" Process multiple file uploads using document analysis. Args: uploaded_files: List of uploaded file objects. Returns: A tuple of (filenames, texts) extracted from the files. \"\"\" filenames = [] texts = [] document_analysis_client = Config . DOCUMENT_ANALYSIS_CLIENT if hasattr ( Config , \"AZURE_DOCAI_KEY\" ) else None for file in uploaded_files : upload_manager = StreamlitUploadManager ( file , accept_multiple_files = True , document_analysis_client = document_analysis_client ) self . ui . spinner ( 'Reading in Files...' ) file_data , _ = upload_manager . process_upload () if file_data is not None : filenames . append ( file . name ) texts . append ( file_data ) return filenames , texts","title":"process_multiple_files"},{"location":"LabeLMaker/streamlit_interface.html#LabeLMaker.streamlit_interface.Categorize.setup_workflow","text":"Gather workflow parameters from the UI. Includes unique ID setup, mode selection, text column selection, etc. Parameters: df ( DataFrame ) \u2013 The input DataFrame. Returns: dict \u2013 A dictionary of workflow parameters. Source code in LabeLMaker/streamlit_interface.py 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 def setup_workflow ( self , df : pd . DataFrame ) -> dict : \"\"\" Gather workflow parameters from the UI. Includes unique ID setup, mode selection, text column selection, etc. Args: df: The input DataFrame. Returns: A dictionary of workflow parameters. \"\"\" params = {} # Unique Identifier Setup if not self . ui . session_state . get ( \"uniqueIdSetup_done\" ): self . ui . markdown ( \"### Unique Identifier Setup\" ) id_choice = self . ui . radio ( \"How would you like to specify a unique identifier?\" , options = [ \"Create new ID column\" , \"Use an existing column\" ], index = 0 , key = \"id_choice\" , ) if id_choice == \"Create new ID column\" : new_id_col = self . ui . text_input ( \"Enter name for the new ID column\" , value = \"id\" , key = \"new_id_column\" ) if self . ui . button ( \"Create ID Column\" , key = \"create_id_column\" ): if new_id_col not in df . columns : df [ new_id_col ] = df . index . astype ( str ) self . ui . success ( f \"New ID column ' { new_id_col } ' created.\" ) else : self . ui . info ( f \"Column ' { new_id_col } ' already exists; using it.\" ) self . ui . session_state [ \"single_file_df\" ] = df self . ui . session_state [ \"selected_id_column\" ] = new_id_col self . ui . session_state [ \"uniqueIdSetup_done\" ] = True else : selected_existing = self . ui . selectbox ( \"Select the column to use as the unique identifier\" , options = df . columns . tolist (), key = \"existing_id_column\" , ) if self . ui . button ( \"Confirm ID Column\" , key = \"confirm_id_column\" ): self . ui . session_state [ \"selected_id_column\" ] = selected_existing self . ui . session_state [ \"uniqueIdSetup_done\" ] = True sel_id = self . ui . session_state . get ( \"selected_id_column\" ) if sel_id and sel_id not in df . columns : df [ sel_id ] = df . index . astype ( str ) self . ui . session_state [ \"single_file_df\" ] = df params [ \"index_column\" ] = sel_id self . ui . write ( f \"Using ' { params [ 'index_column' ] } ' as the unique identifier column.\" ) advanced_mode = self . ui . checkbox ( \"Advanced Mode\" , help = \"For users with ground truth labels. Check to run evaluation pipeline.\" , key = \"advanced_mode\" , ) params [ \"mode\" ] = \"Evaluation\" if advanced_mode else \"Production\" self . ui . info ( \"Advanced Mode activated.\" if advanced_mode else \"Software will automatically select labelling method\" ) df_columns = df . columns . tolist () params [ \"categorizing_column\" ] = self . ui . selectbox ( \"Select the column with text data you want to label\" , options = df_columns , key = \"categorizing_column\" ) if params [ \"mode\" ] == \"Evaluation\" : gt_col = self . ui . selectbox ( \"Select the column with ground truth labels\" , options = df_columns , key = \"ground_truth_column\" ) params [ \"ground_truth_column\" ] = gt_col eval_techniques = self . ui . multiselect ( \"Select the evaluation approaches to run:\" , options = [ \"Zero Shot\" , \"Few Shot\" , \"Many Shot\" ], default = [ \"Zero Shot\" ], key = \"evaluation_techniques\" , ) params [ \"evaluation_techniques\" ] = eval_techniques if \"Few Shot\" in eval_techniques : few_shot_count = self . ui . number_input ( \"Enter maximum examples per category (Few Shot)\" , min_value = 1 , value = 2 , key = \"few_shot_count\" ) params [ \"few_shot_count\" ] = few_shot_count if \"Many Shot\" in eval_techniques : many_shot_train_ratio = self . ui . number_input ( \"Enter train proportion for Many Shot (0 to 1)\" , min_value = 0.0 , max_value = 1.0 , value = 0.8 , key = \"many_shot_train_ratio\" ) params [ \"many_shot_train_ratio\" ] = many_shot_train_ratio else : ex_options = [ \"None\" ] + df_columns ex_col = self . ui . selectbox ( \"Select the column containing your examples (if any)\" , options = ex_options , key = \"examples_column\" ) params [ \"examples_column\" ] = None if ex_col == \"None\" else ex_col # Category definition using CategoryManager. from LabeLMaker.utils.category import CategoryManager default_col = params . get ( \"ground_truth_column\" ) if params [ \"mode\" ] == \"Evaluation\" else ( params . get ( \"examples_column\" ) or \"\" ) default_categories = \"\" if default_col and default_col in df . columns : unique_values = df [ default_col ] . dropna () . unique () if len ( unique_values ) <= Config . MAX_RECOMMENDED_GROUPS : default_categories = \",\" . join ([ str ( val ) for val in unique_values ]) else : self . ui . warning ( \"There are more than 10 unique values in the column. Auto-population of categories may not be practical.\" ) categories_dict , categories_with_descriptions = CategoryManager . define_categories ( self . ui , \"tab1\" , unique_values_str = default_categories ) params [ \"categories_dict\" ] = categories_dict params [ \"categories_with_descriptions\" ] = categories_with_descriptions return params","title":"setup_workflow"},{"location":"LabeLMaker/streamlit_interface.html#LabeLMaker.streamlit_interface.Evaluate","text":"Bases: BaseHandler Wraps the evaluation workflow. Source code in LabeLMaker/streamlit_interface.py 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 class Evaluate ( BaseHandler ): \"\"\" Wraps the evaluation workflow. \"\"\" def __init__ ( self , ui_helper : Any , azure_key : str = None ) -> None : super () . __init__ ( ui_helper , azure_key = azure_key ) self . eval_handler = StreamlitEvaluateHandler ( ui_helper , azure_key = azure_key ) def handle_evaluation ( self ) -> None : \"\"\" Execute the evaluation workflow. \"\"\" file = self . _ensure_file ( file = None , upload_message = \"Upload a CSV file for Evaluation\" , file_types = [ \"csv\" ], key = \"eval_file_uploader_handler\" , info_message = \"Please upload a CSV file to proceed.\" , ) if file is None : return try : df = self . _load_data ( file ) except Exception as error : self . ui . error ( f \"Error reading file: { error } \" ) return self . ui . subheader ( \"CSV Preview:\" ) self . ui . write ( df . head ()) self . ui . write ( f \"Total rows in dataframe: { len ( df ) } \" ) # Let the user select the ground truth column and evaluation methods. ground_truth_col = self . ui . selectbox ( \"Select Ground Truth Column\" , df . columns . tolist (), key = \"eval_gt_column\" ) selected_methods = self . ui . multiselect ( \"Select evaluation methods\" , [ \"Zero Shot\" , \"Few Shot\" , \"Many Shot\" ], default = [ \"Zero Shot\" , \"Few Shot\" , \"Many Shot\" ], key = \"eval_methods\" , ) # Move the button to the UI layer. if self . ui . button ( \"Calculate Results\" , key = \"calc_results_button\" ): # Use a spinner if desired. with self . ui . spinner ( \"Evaluating\" ): try : # Call your evaluation function (i.e. a pure function) common_df , results , confusion_matrices = self . eval_handler . compare_methods ( df , ground_truth_col , selected_methods ) # Now display the results. for method , metrics in results . items (): self . ui . subheader ( method ) self . ui . write ( metrics ) # Create and display a DOCX download button. docx_maker = StreamlitDocxCreator ( results , confusion_matrices ) doc = docx_maker . create_docx_report () docx_content = self . generate_docx_report_download ( doc ) self . ui . download_button ( label = \"Download DOCX Report\" , data = docx_content , file_name = \"evaluation_report.docx\" , mime = \"application/vnd.openxmlformats-officedocument.wordprocessingml.document\" , ) except Exception as e : self . ui . error ( f \"Error during evaluation: { e } \" )","title":"Evaluate"},{"location":"LabeLMaker/streamlit_interface.html#LabeLMaker.streamlit_interface.Evaluate.handle_evaluation","text":"Execute the evaluation workflow. Source code in LabeLMaker/streamlit_interface.py 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 def handle_evaluation ( self ) -> None : \"\"\" Execute the evaluation workflow. \"\"\" file = self . _ensure_file ( file = None , upload_message = \"Upload a CSV file for Evaluation\" , file_types = [ \"csv\" ], key = \"eval_file_uploader_handler\" , info_message = \"Please upload a CSV file to proceed.\" , ) if file is None : return try : df = self . _load_data ( file ) except Exception as error : self . ui . error ( f \"Error reading file: { error } \" ) return self . ui . subheader ( \"CSV Preview:\" ) self . ui . write ( df . head ()) self . ui . write ( f \"Total rows in dataframe: { len ( df ) } \" ) # Let the user select the ground truth column and evaluation methods. ground_truth_col = self . ui . selectbox ( \"Select Ground Truth Column\" , df . columns . tolist (), key = \"eval_gt_column\" ) selected_methods = self . ui . multiselect ( \"Select evaluation methods\" , [ \"Zero Shot\" , \"Few Shot\" , \"Many Shot\" ], default = [ \"Zero Shot\" , \"Few Shot\" , \"Many Shot\" ], key = \"eval_methods\" , ) # Move the button to the UI layer. if self . ui . button ( \"Calculate Results\" , key = \"calc_results_button\" ): # Use a spinner if desired. with self . ui . spinner ( \"Evaluating\" ): try : # Call your evaluation function (i.e. a pure function) common_df , results , confusion_matrices = self . eval_handler . compare_methods ( df , ground_truth_col , selected_methods ) # Now display the results. for method , metrics in results . items (): self . ui . subheader ( method ) self . ui . write ( metrics ) # Create and display a DOCX download button. docx_maker = StreamlitDocxCreator ( results , confusion_matrices ) doc = docx_maker . create_docx_report () docx_content = self . generate_docx_report_download ( doc ) self . ui . download_button ( label = \"Download DOCX Report\" , data = docx_content , file_name = \"evaluation_report.docx\" , mime = \"application/vnd.openxmlformats-officedocument.wordprocessingml.document\" , ) except Exception as e : self . ui . error ( f \"Error during evaluation: { e } \" )","title":"handle_evaluation"},{"location":"LabeLMaker/Categorize/categorizer.html","text":"The Categorizer Module orchestrates the activity associated with categorizing the text. It contains the logic for common aspects of categorization shared across zero and few shot cases. BaseCategorizer Bases: WorkflowHandler , ABC The BaseCategorizer class is a subclass of WorkflowHandler and an abstract base class (ABC) with an empty constructor. Source code in LabeLMaker/Categorize/categorizer.py 19 20 21 22 23 24 25 26 class BaseCategorizer ( WorkflowHandler , ABC ): \"\"\" The `BaseCategorizer` class is a subclass of `WorkflowHandler` and an abstract base class (ABC) with an empty constructor. \"\"\" def __init__ ( self ): pass LabeLMaker Bases: BaseCategorizer The function initializes attributes for a prompt generator class with specified parameters. Parameters: prompty_path ( Path ) \u2013 The prompty_path parameter is expected to be of type Path and represents the path to a file or directory related to the prompt. categorzation_request (CategorizationRequest): It looks like there is a typo in the parameter name categorzation_request . It should be categorization_request instead. llm_interface: The llm_interface parameter in the __init__ method is a default parameter with a default value of Config.LLM_INTERFACE . This means that if no value is provided for llm_interface when creating an instance of the class, it will default to the value specified in Source code in LabeLMaker/Categorize/categorizer.py 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 class LabeLMaker ( BaseCategorizer ): \"\"\" The function initializes attributes for a prompt generator class with specified parameters. Args: prompty_path (Path): The `prompty_path` parameter is expected to be of type `Path` and represents the path to a file or directory related to the prompt. categorzation_request (CategorizationRequest): It looks like there is a typo in the parameter name `categorzation_request`. It should be `categorization_request` instead. llm_interface: The `llm_interface` parameter in the `__init__` method is a default parameter with a default value of `Config.LLM_INTERFACE`. This means that if no value is provided for `llm_interface` when creating an instance of the class, it will default to the value specified in \"\"\" def __init__ ( self , prompty_path : Path , categorzation_request : CategorizationRequest , llm_interface = Config . LLM_INTERFACE , ): super () . __init__ () self . prompty_path = prompty_path self . categorzation_request = categorzation_request self . prompt_template = self . load_prompty () self . llm = llm_interface self . chain = self . create_chain () self . prompt_inputs = self . _prepare_prompt_inputs () self . valid_categories = [ category . name for category in self . categorzation_request . categories ] def _prepare_prompt_inputs ( self ): # This method should be overridden by child classes for specific validation raise NotImplementedError ( \"Subclasses must implement prepare_prompt_inputs.\" ) def _validate_prompt_template ( self , prompt_template ): # This method should be overridden by child classes for specific validation raise NotImplementedError ( \"Subclasses must implement _validate_prompt_template.\" ) def create_chain ( self ): \"\"\" The `create_chain` function returns the result of combining the `prompt_template` and `llm` attributes. :return: The `create_chain` method combines the instructions of `self.prompt_template` and `self.llm`. \"\"\" return self . prompt_template | self . llm def categorize_item ( self , item_args ): \"\"\" The `categorize_item` function categorizes an item based on input arguments and handles exceptions. :param item_args: The `item_args` parameter seems to be a dictionary that is being passed to the `categorize_item` method. It likely contains information or data related to an item that needs to be categorized :return: The `categorize_item` method returns the result of invoking the `chain` with the `item_args` provided. If the result is `None`, it raises a `ValueError` indicating that the chain returned `None` for the input. If an exception occurs during the processing of the item, it prints an error message with the item details and the exception message, then prints the traceback \"\"\" try : # Update item_args with all prompt inputs item_args . update ( self . prompt_inputs ) print ( \"ITEM ARGS - \" , item_args ) result = self . chain . invoke ( item_args ) if result is None : raise ValueError ( f \"Chain returned None for input: { item_args } \" ) return result except Exception as e : print ( f \"Error processing item: { item_args . get ( 'item' , '' ) } \" ) print ( f \"Exception: { e } \" ) traceback . print_exc () return None def categorize_text_with_retries ( self , text_to_categorize : str , max_retries : int = Config . MAX_RETRIES ): \"\"\" The function `categorize_text_with_retries` categorizes text with retry logic to ensure a valid category is obtained. :param text_to_categorize: The `text_to_categorize` parameter is a string that represents the text that needs to be categorized. It is the input text that will be processed and categorized by the `categorize_text_with_retries` method :type text_to_categorize: str :param max_retries: The `max_retries` parameter in the `categorize_text_with_retries` method specifies the maximum number of retries allowed when attempting to categorize the text. If the categorization process fails to produce a valid category within the specified number of retries, the method will return \"Uncategorized\" :type max_retries: int :return: The `categorize_text_with_retries` method returns a tuple containing the `rationale` and `category` of the text after attempting to categorize it with retry logic. If a valid category is not obtained after the maximum number of retries, it sets the category to \"Uncategorized\". \"\"\" # Common method to categorize text with retry logic retry_count = 0 rationale = None category = None while retry_count < max_retries : item_args = { \"item\" : text_to_categorize } content = self . categorize_item ( item_args ) if content is None : print ( f \"Warning: categorize_item() returned None for input: { text_to_categorize } \" ) else : # Add this line text_content = self . check_content_type ( content ) rationale = self . extract_rationale ( text_content ) category = self . extract_category ( text_content ) # And this line print ( f \"Extracted rationale: { rationale } , category: { category } \" ) if self . _is_valid_category ( category ): break else : print ( f \"Invalid category ' { category } ' received. Retrying ( { retry_count + 1 } / { max_retries } )...\" ) retry_count += 1 if not self . _is_valid_category ( category ): print ( f \"Failed to get a valid category for text: ' { text_to_categorize } ' after { max_retries } retries.\" ) category = \"Uncategorized\" return rationale , category @staticmethod def extract_rationale ( content : str ): \"\"\" This Python function extracts the rationale from a given content using a regular expression pattern. :param content: It looks like you have provided the code snippet for a function called `extract_rationale` that extracts the rationale from a given content string using a regular expression pattern. The rationale is expected to be found after the text \"Rationale:\" and before the text \"Category:\" or the end of the string :type content: str :return: The function `extract_rationale` returns the rationale extracted from the input `content` string based on the provided regex pattern. If a match is found, it returns the extracted rationale text after stripping any leading or trailing whitespaces. If no match is found, it returns `None`. \"\"\" # Adjust the regex pattern to match the actual content # TODO consider having model output JSON and parse that JSON here rationale_pattern = r \"Rationale:\\s*(.*?)\\s*(?:Category:|$)\" rationale_match = re . search ( rationale_pattern , content , re . DOTALL | re . IGNORECASE ) rationale = rationale_match . group ( 1 ) . strip () if rationale_match else None return rationale @staticmethod def extract_category ( content : str ): \"\"\" The function `extract_category` takes a string input and extracts the category information following the \"Category:\" keyword. :param content: Thank you for providing the code snippet. It looks like you are trying to extract the category from a given content string using a regular expression pattern :type content: str :return: The function `extract_category` returns the category extracted from the input `content` string. If the string contains a pattern \"Category: \" followed by any characters, the function will extract and return those characters as the category. If the pattern is not found in the input string, the function will return `None`. \"\"\" category_pattern = r \"Category:\\s*(.*)\" category_match = re . search ( category_pattern , content , re . IGNORECASE ) category = category_match . group ( 1 ) . strip () if category_match else None return category def _is_valid_category ( self , category : str ): \"\"\" The function `_is_valid_category` checks if a given category is valid within a list of valid categories. :param category: The `_is_valid_category` method takes a parameter `category` of type `str`. It checks if the `category` is present in the `valid_categories` attribute of the class and returns a boolean value indicating whether the category is valid or not :type category: str :return: a boolean value indicating whether the input category is found in the list of valid categories stored in the object. \"\"\" category_found = category in self . valid_categories return category_found def process ( self ): \"\"\" The `process` function categorizes text data with retries and returns the results. :return: The `process` method is returning a list of tuples where each tuple contains the original text, the category assigned to that text, and the rationale for the categorization. \"\"\" categorized_results = [] index_list = self . categorzation_request . unique_ids text_list = self . categorzation_request . text_to_label total_list_length = len ( text_list ) progress_bar = st . progress ( 1 , text = str ( self . __class__ . __name__ ) + \" operation in progress. Please wait...\" ) for i , ( idx , text ) in enumerate ( zip ( index_list , text_list )): normalized_text = normalize_text ( text ) progress = ( i + 1 ) / total_list_length progress_bar . progress ( progress , text = str ( self . __class__ . __name__ ) + \" operation in progress. Please wait...\" , ) rationale , category = self . categorize_text_with_retries ( normalized_text ) categorized_results . append (( idx , normalized_text , category , rationale )) return categorized_results categorize_item ( item_args ) The categorize_item function categorizes an item based on input arguments and handles exceptions. :param item_args: The item_args parameter seems to be a dictionary that is being passed to the categorize_item method. It likely contains information or data related to an item that needs to be categorized :return: The categorize_item method returns the result of invoking the chain with the item_args provided. If the result is None , it raises a ValueError indicating that the chain returned None for the input. If an exception occurs during the processing of the item, it prints an error message with the item details and the exception message, then prints the traceback Source code in LabeLMaker/Categorize/categorizer.py 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 def categorize_item ( self , item_args ): \"\"\" The `categorize_item` function categorizes an item based on input arguments and handles exceptions. :param item_args: The `item_args` parameter seems to be a dictionary that is being passed to the `categorize_item` method. It likely contains information or data related to an item that needs to be categorized :return: The `categorize_item` method returns the result of invoking the `chain` with the `item_args` provided. If the result is `None`, it raises a `ValueError` indicating that the chain returned `None` for the input. If an exception occurs during the processing of the item, it prints an error message with the item details and the exception message, then prints the traceback \"\"\" try : # Update item_args with all prompt inputs item_args . update ( self . prompt_inputs ) print ( \"ITEM ARGS - \" , item_args ) result = self . chain . invoke ( item_args ) if result is None : raise ValueError ( f \"Chain returned None for input: { item_args } \" ) return result except Exception as e : print ( f \"Error processing item: { item_args . get ( 'item' , '' ) } \" ) print ( f \"Exception: { e } \" ) traceback . print_exc () return None categorize_text_with_retries ( text_to_categorize , max_retries = Config . MAX_RETRIES ) The function categorize_text_with_retries categorizes text with retry logic to ensure a valid category is obtained. :param text_to_categorize: The text_to_categorize parameter is a string that represents the text that needs to be categorized. It is the input text that will be processed and categorized by the categorize_text_with_retries method :type text_to_categorize: str :param max_retries: The max_retries parameter in the categorize_text_with_retries method specifies the maximum number of retries allowed when attempting to categorize the text. If the categorization process fails to produce a valid category within the specified number of retries, the method will return \"Uncategorized\" :type max_retries: int :return: The categorize_text_with_retries method returns a tuple containing the rationale and category of the text after attempting to categorize it with retry logic. If a valid category is not obtained after the maximum number of retries, it sets the category to \"Uncategorized\". Source code in LabeLMaker/Categorize/categorizer.py 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 def categorize_text_with_retries ( self , text_to_categorize : str , max_retries : int = Config . MAX_RETRIES ): \"\"\" The function `categorize_text_with_retries` categorizes text with retry logic to ensure a valid category is obtained. :param text_to_categorize: The `text_to_categorize` parameter is a string that represents the text that needs to be categorized. It is the input text that will be processed and categorized by the `categorize_text_with_retries` method :type text_to_categorize: str :param max_retries: The `max_retries` parameter in the `categorize_text_with_retries` method specifies the maximum number of retries allowed when attempting to categorize the text. If the categorization process fails to produce a valid category within the specified number of retries, the method will return \"Uncategorized\" :type max_retries: int :return: The `categorize_text_with_retries` method returns a tuple containing the `rationale` and `category` of the text after attempting to categorize it with retry logic. If a valid category is not obtained after the maximum number of retries, it sets the category to \"Uncategorized\". \"\"\" # Common method to categorize text with retry logic retry_count = 0 rationale = None category = None while retry_count < max_retries : item_args = { \"item\" : text_to_categorize } content = self . categorize_item ( item_args ) if content is None : print ( f \"Warning: categorize_item() returned None for input: { text_to_categorize } \" ) else : # Add this line text_content = self . check_content_type ( content ) rationale = self . extract_rationale ( text_content ) category = self . extract_category ( text_content ) # And this line print ( f \"Extracted rationale: { rationale } , category: { category } \" ) if self . _is_valid_category ( category ): break else : print ( f \"Invalid category ' { category } ' received. Retrying ( { retry_count + 1 } / { max_retries } )...\" ) retry_count += 1 if not self . _is_valid_category ( category ): print ( f \"Failed to get a valid category for text: ' { text_to_categorize } ' after { max_retries } retries.\" ) category = \"Uncategorized\" return rationale , category create_chain () The create_chain function returns the result of combining the prompt_template and llm attributes. :return: The create_chain method combines the instructions of self.prompt_template and self.llm . Source code in LabeLMaker/Categorize/categorizer.py 69 70 71 72 73 74 75 76 def create_chain ( self ): \"\"\" The `create_chain` function returns the result of combining the `prompt_template` and `llm` attributes. :return: The `create_chain` method combines the instructions of `self.prompt_template` and `self.llm`. \"\"\" return self . prompt_template | self . llm extract_category ( content ) staticmethod The function extract_category takes a string input and extracts the category information following the \"Category:\" keyword. :param content: Thank you for providing the code snippet. It looks like you are trying to extract the category from a given content string using a regular expression pattern :type content: str :return: The function extract_category returns the category extracted from the input content string. If the string contains a pattern \"Category: \" followed by any characters, the function will extract and return those characters as the category. If the pattern is not found in the input string, the function will return None . Source code in LabeLMaker/Categorize/categorizer.py 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 @staticmethod def extract_category ( content : str ): \"\"\" The function `extract_category` takes a string input and extracts the category information following the \"Category:\" keyword. :param content: Thank you for providing the code snippet. It looks like you are trying to extract the category from a given content string using a regular expression pattern :type content: str :return: The function `extract_category` returns the category extracted from the input `content` string. If the string contains a pattern \"Category: \" followed by any characters, the function will extract and return those characters as the category. If the pattern is not found in the input string, the function will return `None`. \"\"\" category_pattern = r \"Category:\\s*(.*)\" category_match = re . search ( category_pattern , content , re . IGNORECASE ) category = category_match . group ( 1 ) . strip () if category_match else None return category extract_rationale ( content ) staticmethod This Python function extracts the rationale from a given content using a regular expression pattern. :param content: It looks like you have provided the code snippet for a function called extract_rationale that extracts the rationale from a given content string using a regular expression pattern. The rationale is expected to be found after the text \"Rationale:\" and before the text \"Category:\" or the end of the string :type content: str :return: The function extract_rationale returns the rationale extracted from the input content string based on the provided regex pattern. If a match is found, it returns the extracted rationale text after stripping any leading or trailing whitespaces. If no match is found, it returns None . Source code in LabeLMaker/Categorize/categorizer.py 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 @staticmethod def extract_rationale ( content : str ): \"\"\" This Python function extracts the rationale from a given content using a regular expression pattern. :param content: It looks like you have provided the code snippet for a function called `extract_rationale` that extracts the rationale from a given content string using a regular expression pattern. The rationale is expected to be found after the text \"Rationale:\" and before the text \"Category:\" or the end of the string :type content: str :return: The function `extract_rationale` returns the rationale extracted from the input `content` string based on the provided regex pattern. If a match is found, it returns the extracted rationale text after stripping any leading or trailing whitespaces. If no match is found, it returns `None`. \"\"\" # Adjust the regex pattern to match the actual content # TODO consider having model output JSON and parse that JSON here rationale_pattern = r \"Rationale:\\s*(.*?)\\s*(?:Category:|$)\" rationale_match = re . search ( rationale_pattern , content , re . DOTALL | re . IGNORECASE ) rationale = rationale_match . group ( 1 ) . strip () if rationale_match else None return rationale process () The process function categorizes text data with retries and returns the results. :return: The process method is returning a list of tuples where each tuple contains the original text, the category assigned to that text, and the rationale for the categorization. Source code in LabeLMaker/Categorize/categorizer.py 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 def process ( self ): \"\"\" The `process` function categorizes text data with retries and returns the results. :return: The `process` method is returning a list of tuples where each tuple contains the original text, the category assigned to that text, and the rationale for the categorization. \"\"\" categorized_results = [] index_list = self . categorzation_request . unique_ids text_list = self . categorzation_request . text_to_label total_list_length = len ( text_list ) progress_bar = st . progress ( 1 , text = str ( self . __class__ . __name__ ) + \" operation in progress. Please wait...\" ) for i , ( idx , text ) in enumerate ( zip ( index_list , text_list )): normalized_text = normalize_text ( text ) progress = ( i + 1 ) / total_list_length progress_bar . progress ( progress , text = str ( self . __class__ . __name__ ) + \" operation in progress. Please wait...\" , ) rationale , category = self . categorize_text_with_retries ( normalized_text ) categorized_results . append (( idx , normalized_text , category , rationale )) return categorized_results","title":"Categorizer"},{"location":"LabeLMaker/Categorize/categorizer.html#LabeLMaker.Categorize.categorizer.BaseCategorizer","text":"Bases: WorkflowHandler , ABC The BaseCategorizer class is a subclass of WorkflowHandler and an abstract base class (ABC) with an empty constructor. Source code in LabeLMaker/Categorize/categorizer.py 19 20 21 22 23 24 25 26 class BaseCategorizer ( WorkflowHandler , ABC ): \"\"\" The `BaseCategorizer` class is a subclass of `WorkflowHandler` and an abstract base class (ABC) with an empty constructor. \"\"\" def __init__ ( self ): pass","title":"BaseCategorizer"},{"location":"LabeLMaker/Categorize/categorizer.html#LabeLMaker.Categorize.categorizer.LabeLMaker","text":"Bases: BaseCategorizer The function initializes attributes for a prompt generator class with specified parameters. Parameters: prompty_path ( Path ) \u2013 The prompty_path parameter is expected to be of type Path and represents the path to a file or directory related to the prompt. categorzation_request (CategorizationRequest): It looks like there is a typo in the parameter name categorzation_request . It should be categorization_request instead. llm_interface: The llm_interface parameter in the __init__ method is a default parameter with a default value of Config.LLM_INTERFACE . This means that if no value is provided for llm_interface when creating an instance of the class, it will default to the value specified in Source code in LabeLMaker/Categorize/categorizer.py 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 class LabeLMaker ( BaseCategorizer ): \"\"\" The function initializes attributes for a prompt generator class with specified parameters. Args: prompty_path (Path): The `prompty_path` parameter is expected to be of type `Path` and represents the path to a file or directory related to the prompt. categorzation_request (CategorizationRequest): It looks like there is a typo in the parameter name `categorzation_request`. It should be `categorization_request` instead. llm_interface: The `llm_interface` parameter in the `__init__` method is a default parameter with a default value of `Config.LLM_INTERFACE`. This means that if no value is provided for `llm_interface` when creating an instance of the class, it will default to the value specified in \"\"\" def __init__ ( self , prompty_path : Path , categorzation_request : CategorizationRequest , llm_interface = Config . LLM_INTERFACE , ): super () . __init__ () self . prompty_path = prompty_path self . categorzation_request = categorzation_request self . prompt_template = self . load_prompty () self . llm = llm_interface self . chain = self . create_chain () self . prompt_inputs = self . _prepare_prompt_inputs () self . valid_categories = [ category . name for category in self . categorzation_request . categories ] def _prepare_prompt_inputs ( self ): # This method should be overridden by child classes for specific validation raise NotImplementedError ( \"Subclasses must implement prepare_prompt_inputs.\" ) def _validate_prompt_template ( self , prompt_template ): # This method should be overridden by child classes for specific validation raise NotImplementedError ( \"Subclasses must implement _validate_prompt_template.\" ) def create_chain ( self ): \"\"\" The `create_chain` function returns the result of combining the `prompt_template` and `llm` attributes. :return: The `create_chain` method combines the instructions of `self.prompt_template` and `self.llm`. \"\"\" return self . prompt_template | self . llm def categorize_item ( self , item_args ): \"\"\" The `categorize_item` function categorizes an item based on input arguments and handles exceptions. :param item_args: The `item_args` parameter seems to be a dictionary that is being passed to the `categorize_item` method. It likely contains information or data related to an item that needs to be categorized :return: The `categorize_item` method returns the result of invoking the `chain` with the `item_args` provided. If the result is `None`, it raises a `ValueError` indicating that the chain returned `None` for the input. If an exception occurs during the processing of the item, it prints an error message with the item details and the exception message, then prints the traceback \"\"\" try : # Update item_args with all prompt inputs item_args . update ( self . prompt_inputs ) print ( \"ITEM ARGS - \" , item_args ) result = self . chain . invoke ( item_args ) if result is None : raise ValueError ( f \"Chain returned None for input: { item_args } \" ) return result except Exception as e : print ( f \"Error processing item: { item_args . get ( 'item' , '' ) } \" ) print ( f \"Exception: { e } \" ) traceback . print_exc () return None def categorize_text_with_retries ( self , text_to_categorize : str , max_retries : int = Config . MAX_RETRIES ): \"\"\" The function `categorize_text_with_retries` categorizes text with retry logic to ensure a valid category is obtained. :param text_to_categorize: The `text_to_categorize` parameter is a string that represents the text that needs to be categorized. It is the input text that will be processed and categorized by the `categorize_text_with_retries` method :type text_to_categorize: str :param max_retries: The `max_retries` parameter in the `categorize_text_with_retries` method specifies the maximum number of retries allowed when attempting to categorize the text. If the categorization process fails to produce a valid category within the specified number of retries, the method will return \"Uncategorized\" :type max_retries: int :return: The `categorize_text_with_retries` method returns a tuple containing the `rationale` and `category` of the text after attempting to categorize it with retry logic. If a valid category is not obtained after the maximum number of retries, it sets the category to \"Uncategorized\". \"\"\" # Common method to categorize text with retry logic retry_count = 0 rationale = None category = None while retry_count < max_retries : item_args = { \"item\" : text_to_categorize } content = self . categorize_item ( item_args ) if content is None : print ( f \"Warning: categorize_item() returned None for input: { text_to_categorize } \" ) else : # Add this line text_content = self . check_content_type ( content ) rationale = self . extract_rationale ( text_content ) category = self . extract_category ( text_content ) # And this line print ( f \"Extracted rationale: { rationale } , category: { category } \" ) if self . _is_valid_category ( category ): break else : print ( f \"Invalid category ' { category } ' received. Retrying ( { retry_count + 1 } / { max_retries } )...\" ) retry_count += 1 if not self . _is_valid_category ( category ): print ( f \"Failed to get a valid category for text: ' { text_to_categorize } ' after { max_retries } retries.\" ) category = \"Uncategorized\" return rationale , category @staticmethod def extract_rationale ( content : str ): \"\"\" This Python function extracts the rationale from a given content using a regular expression pattern. :param content: It looks like you have provided the code snippet for a function called `extract_rationale` that extracts the rationale from a given content string using a regular expression pattern. The rationale is expected to be found after the text \"Rationale:\" and before the text \"Category:\" or the end of the string :type content: str :return: The function `extract_rationale` returns the rationale extracted from the input `content` string based on the provided regex pattern. If a match is found, it returns the extracted rationale text after stripping any leading or trailing whitespaces. If no match is found, it returns `None`. \"\"\" # Adjust the regex pattern to match the actual content # TODO consider having model output JSON and parse that JSON here rationale_pattern = r \"Rationale:\\s*(.*?)\\s*(?:Category:|$)\" rationale_match = re . search ( rationale_pattern , content , re . DOTALL | re . IGNORECASE ) rationale = rationale_match . group ( 1 ) . strip () if rationale_match else None return rationale @staticmethod def extract_category ( content : str ): \"\"\" The function `extract_category` takes a string input and extracts the category information following the \"Category:\" keyword. :param content: Thank you for providing the code snippet. It looks like you are trying to extract the category from a given content string using a regular expression pattern :type content: str :return: The function `extract_category` returns the category extracted from the input `content` string. If the string contains a pattern \"Category: \" followed by any characters, the function will extract and return those characters as the category. If the pattern is not found in the input string, the function will return `None`. \"\"\" category_pattern = r \"Category:\\s*(.*)\" category_match = re . search ( category_pattern , content , re . IGNORECASE ) category = category_match . group ( 1 ) . strip () if category_match else None return category def _is_valid_category ( self , category : str ): \"\"\" The function `_is_valid_category` checks if a given category is valid within a list of valid categories. :param category: The `_is_valid_category` method takes a parameter `category` of type `str`. It checks if the `category` is present in the `valid_categories` attribute of the class and returns a boolean value indicating whether the category is valid or not :type category: str :return: a boolean value indicating whether the input category is found in the list of valid categories stored in the object. \"\"\" category_found = category in self . valid_categories return category_found def process ( self ): \"\"\" The `process` function categorizes text data with retries and returns the results. :return: The `process` method is returning a list of tuples where each tuple contains the original text, the category assigned to that text, and the rationale for the categorization. \"\"\" categorized_results = [] index_list = self . categorzation_request . unique_ids text_list = self . categorzation_request . text_to_label total_list_length = len ( text_list ) progress_bar = st . progress ( 1 , text = str ( self . __class__ . __name__ ) + \" operation in progress. Please wait...\" ) for i , ( idx , text ) in enumerate ( zip ( index_list , text_list )): normalized_text = normalize_text ( text ) progress = ( i + 1 ) / total_list_length progress_bar . progress ( progress , text = str ( self . __class__ . __name__ ) + \" operation in progress. Please wait...\" , ) rationale , category = self . categorize_text_with_retries ( normalized_text ) categorized_results . append (( idx , normalized_text , category , rationale )) return categorized_results","title":"LabeLMaker"},{"location":"LabeLMaker/Categorize/categorizer.html#LabeLMaker.Categorize.categorizer.LabeLMaker.categorize_item","text":"The categorize_item function categorizes an item based on input arguments and handles exceptions. :param item_args: The item_args parameter seems to be a dictionary that is being passed to the categorize_item method. It likely contains information or data related to an item that needs to be categorized :return: The categorize_item method returns the result of invoking the chain with the item_args provided. If the result is None , it raises a ValueError indicating that the chain returned None for the input. If an exception occurs during the processing of the item, it prints an error message with the item details and the exception message, then prints the traceback Source code in LabeLMaker/Categorize/categorizer.py 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 def categorize_item ( self , item_args ): \"\"\" The `categorize_item` function categorizes an item based on input arguments and handles exceptions. :param item_args: The `item_args` parameter seems to be a dictionary that is being passed to the `categorize_item` method. It likely contains information or data related to an item that needs to be categorized :return: The `categorize_item` method returns the result of invoking the `chain` with the `item_args` provided. If the result is `None`, it raises a `ValueError` indicating that the chain returned `None` for the input. If an exception occurs during the processing of the item, it prints an error message with the item details and the exception message, then prints the traceback \"\"\" try : # Update item_args with all prompt inputs item_args . update ( self . prompt_inputs ) print ( \"ITEM ARGS - \" , item_args ) result = self . chain . invoke ( item_args ) if result is None : raise ValueError ( f \"Chain returned None for input: { item_args } \" ) return result except Exception as e : print ( f \"Error processing item: { item_args . get ( 'item' , '' ) } \" ) print ( f \"Exception: { e } \" ) traceback . print_exc () return None","title":"categorize_item"},{"location":"LabeLMaker/Categorize/categorizer.html#LabeLMaker.Categorize.categorizer.LabeLMaker.categorize_text_with_retries","text":"The function categorize_text_with_retries categorizes text with retry logic to ensure a valid category is obtained. :param text_to_categorize: The text_to_categorize parameter is a string that represents the text that needs to be categorized. It is the input text that will be processed and categorized by the categorize_text_with_retries method :type text_to_categorize: str :param max_retries: The max_retries parameter in the categorize_text_with_retries method specifies the maximum number of retries allowed when attempting to categorize the text. If the categorization process fails to produce a valid category within the specified number of retries, the method will return \"Uncategorized\" :type max_retries: int :return: The categorize_text_with_retries method returns a tuple containing the rationale and category of the text after attempting to categorize it with retry logic. If a valid category is not obtained after the maximum number of retries, it sets the category to \"Uncategorized\". Source code in LabeLMaker/Categorize/categorizer.py 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 def categorize_text_with_retries ( self , text_to_categorize : str , max_retries : int = Config . MAX_RETRIES ): \"\"\" The function `categorize_text_with_retries` categorizes text with retry logic to ensure a valid category is obtained. :param text_to_categorize: The `text_to_categorize` parameter is a string that represents the text that needs to be categorized. It is the input text that will be processed and categorized by the `categorize_text_with_retries` method :type text_to_categorize: str :param max_retries: The `max_retries` parameter in the `categorize_text_with_retries` method specifies the maximum number of retries allowed when attempting to categorize the text. If the categorization process fails to produce a valid category within the specified number of retries, the method will return \"Uncategorized\" :type max_retries: int :return: The `categorize_text_with_retries` method returns a tuple containing the `rationale` and `category` of the text after attempting to categorize it with retry logic. If a valid category is not obtained after the maximum number of retries, it sets the category to \"Uncategorized\". \"\"\" # Common method to categorize text with retry logic retry_count = 0 rationale = None category = None while retry_count < max_retries : item_args = { \"item\" : text_to_categorize } content = self . categorize_item ( item_args ) if content is None : print ( f \"Warning: categorize_item() returned None for input: { text_to_categorize } \" ) else : # Add this line text_content = self . check_content_type ( content ) rationale = self . extract_rationale ( text_content ) category = self . extract_category ( text_content ) # And this line print ( f \"Extracted rationale: { rationale } , category: { category } \" ) if self . _is_valid_category ( category ): break else : print ( f \"Invalid category ' { category } ' received. Retrying ( { retry_count + 1 } / { max_retries } )...\" ) retry_count += 1 if not self . _is_valid_category ( category ): print ( f \"Failed to get a valid category for text: ' { text_to_categorize } ' after { max_retries } retries.\" ) category = \"Uncategorized\" return rationale , category","title":"categorize_text_with_retries"},{"location":"LabeLMaker/Categorize/categorizer.html#LabeLMaker.Categorize.categorizer.LabeLMaker.create_chain","text":"The create_chain function returns the result of combining the prompt_template and llm attributes. :return: The create_chain method combines the instructions of self.prompt_template and self.llm . Source code in LabeLMaker/Categorize/categorizer.py 69 70 71 72 73 74 75 76 def create_chain ( self ): \"\"\" The `create_chain` function returns the result of combining the `prompt_template` and `llm` attributes. :return: The `create_chain` method combines the instructions of `self.prompt_template` and `self.llm`. \"\"\" return self . prompt_template | self . llm","title":"create_chain"},{"location":"LabeLMaker/Categorize/categorizer.html#LabeLMaker.Categorize.categorizer.LabeLMaker.extract_category","text":"The function extract_category takes a string input and extracts the category information following the \"Category:\" keyword. :param content: Thank you for providing the code snippet. It looks like you are trying to extract the category from a given content string using a regular expression pattern :type content: str :return: The function extract_category returns the category extracted from the input content string. If the string contains a pattern \"Category: \" followed by any characters, the function will extract and return those characters as the category. If the pattern is not found in the input string, the function will return None . Source code in LabeLMaker/Categorize/categorizer.py 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 @staticmethod def extract_category ( content : str ): \"\"\" The function `extract_category` takes a string input and extracts the category information following the \"Category:\" keyword. :param content: Thank you for providing the code snippet. It looks like you are trying to extract the category from a given content string using a regular expression pattern :type content: str :return: The function `extract_category` returns the category extracted from the input `content` string. If the string contains a pattern \"Category: \" followed by any characters, the function will extract and return those characters as the category. If the pattern is not found in the input string, the function will return `None`. \"\"\" category_pattern = r \"Category:\\s*(.*)\" category_match = re . search ( category_pattern , content , re . IGNORECASE ) category = category_match . group ( 1 ) . strip () if category_match else None return category","title":"extract_category"},{"location":"LabeLMaker/Categorize/categorizer.html#LabeLMaker.Categorize.categorizer.LabeLMaker.extract_rationale","text":"This Python function extracts the rationale from a given content using a regular expression pattern. :param content: It looks like you have provided the code snippet for a function called extract_rationale that extracts the rationale from a given content string using a regular expression pattern. The rationale is expected to be found after the text \"Rationale:\" and before the text \"Category:\" or the end of the string :type content: str :return: The function extract_rationale returns the rationale extracted from the input content string based on the provided regex pattern. If a match is found, it returns the extracted rationale text after stripping any leading or trailing whitespaces. If no match is found, it returns None . Source code in LabeLMaker/Categorize/categorizer.py 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 @staticmethod def extract_rationale ( content : str ): \"\"\" This Python function extracts the rationale from a given content using a regular expression pattern. :param content: It looks like you have provided the code snippet for a function called `extract_rationale` that extracts the rationale from a given content string using a regular expression pattern. The rationale is expected to be found after the text \"Rationale:\" and before the text \"Category:\" or the end of the string :type content: str :return: The function `extract_rationale` returns the rationale extracted from the input `content` string based on the provided regex pattern. If a match is found, it returns the extracted rationale text after stripping any leading or trailing whitespaces. If no match is found, it returns `None`. \"\"\" # Adjust the regex pattern to match the actual content # TODO consider having model output JSON and parse that JSON here rationale_pattern = r \"Rationale:\\s*(.*?)\\s*(?:Category:|$)\" rationale_match = re . search ( rationale_pattern , content , re . DOTALL | re . IGNORECASE ) rationale = rationale_match . group ( 1 ) . strip () if rationale_match else None return rationale","title":"extract_rationale"},{"location":"LabeLMaker/Categorize/categorizer.html#LabeLMaker.Categorize.categorizer.LabeLMaker.process","text":"The process function categorizes text data with retries and returns the results. :return: The process method is returning a list of tuples where each tuple contains the original text, the category assigned to that text, and the rationale for the categorization. Source code in LabeLMaker/Categorize/categorizer.py 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 def process ( self ): \"\"\" The `process` function categorizes text data with retries and returns the results. :return: The `process` method is returning a list of tuples where each tuple contains the original text, the category assigned to that text, and the rationale for the categorization. \"\"\" categorized_results = [] index_list = self . categorzation_request . unique_ids text_list = self . categorzation_request . text_to_label total_list_length = len ( text_list ) progress_bar = st . progress ( 1 , text = str ( self . __class__ . __name__ ) + \" operation in progress. Please wait...\" ) for i , ( idx , text ) in enumerate ( zip ( index_list , text_list )): normalized_text = normalize_text ( text ) progress = ( i + 1 ) / total_list_length progress_bar . progress ( progress , text = str ( self . __class__ . __name__ ) + \" operation in progress. Please wait...\" , ) rationale , category = self . categorize_text_with_retries ( normalized_text ) categorized_results . append (( idx , normalized_text , category , rationale )) return categorized_results","title":"process"},{"location":"LabeLMaker/Categorize/fewshot.html","text":"FewShotCategorizer Bases: LabeLMaker Source code in LabeLMaker/Categorize/fewshot.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 class FewShotCategorizer ( LabeLMaker ): def __init__ ( self , prompty_path : Path , category_request : CategorizationRequest ): super () . __init__ ( prompty_path , category_request ) self . _validate_examples () def _get_filename ( self ): return \"fewshot_categorizer_output.txt\" def _get_mime_type ( self ): return \"text/plain\" def _validate_prompt_template ( self , prompt_template ): \"\"\" The function `_validate_prompt_template` checks if certain expected variables are present in a given prompt template for few-shot prompts. :param prompt_template: It looks like you are trying to validate mandatory variables in a few-shot prompt template. The expected variables are 'item', 'categories_with_descriptions', and 'examples'. The code snippet you provided checks if these variables are present in the prompt template. If any of the expected variables are missing, a \"\"\" # Validate mandatory variables for few-shot prompts expected_variables = [ \"item\" , \"categories_with_descriptions\" , \"examples\" ] for var in expected_variables : if f \" { var } \" not in prompt_template : raise ValueError ( f \"Expected variable {{ var }} not found in the few-shot prompt template.\" ) def _validate_examples ( self ): \"\"\" This function validates that there is at least one example for each proposed category in a categorization request. \"\"\" # Ensure that there is at least one example for each proposed category category_examples = { category . name : 0 for category in self . categorzation_request . categories } for example in self . categorzation_request . examples or []: if isinstance ( example , Example ): if example . label in category_examples : category_examples [ example . label ] += 1 else : raise TypeError ( f \"Expected an instance of Example, but got { type ( example ) } instead\" ) for category , count in category_examples . items (): if count == 0 : raise ValueError ( f \"No examples provided for category: ' { category } '. Each category must have at least one example.\" ) def _prepare_prompt_inputs ( self ): \"\"\" The `_prepare_prompt_inputs` function prepares categories with descriptions and examples for a prompt input. :return: The `_prepare_prompt_inputs` method returns a dictionary containing the following keys and values: - 'categories_with_descriptions': a list of dictionaries where each dictionary contains a category name and its description (or a default message if no description is provided) - 'examples': examples prepared by the `_prepare_examples` method if available, otherwise it is set to None \"\"\" # Prepare categories with descriptions like in ZeroShotCategorizer categories_with_descriptions = [] for category in self . categorzation_request . categories : if isinstance ( category , Categories ): categories_with_descriptions . append ( { \"category\" : category . name , \"description\" : ( category . description if category . description else \"No description provided\" ), } ) else : raise TypeError ( f \"Expected an instance of Categories, but got { type ( category ) } instead\" ) # Prepare examples if available examples = self . _prepare_examples () if self . categorzation_request . examples else None # Combine prompt inputs prompt_inputs = { \"categories_with_descriptions\" : categories_with_descriptions , \"examples\" : examples , } return prompt_inputs def _prepare_examples ( self ): \"\"\" The `_prepare_examples` function prepares example prompt text by converting them into a list of dictionaries, ensuring they are instances of the `Example` class. :return: The method `_prepare_examples` is returning a list of dictionaries containing the text with label and label of each example in the categorization request. The method also includes a debug print statement to show the prepared examples before returning them. \"\"\" # Prepare the example prompt text as a list of dictionaries prompt_examples = [] for example in self . categorzation_request . examples : if isinstance ( example , Example ): prompt_examples . append ( { \"text_with_label\" : example . text_with_label , \"label\" : example . label } ) else : raise TypeError ( f \"Expected an instance of Example, but got { type ( example ) } instead\" ) return prompt_examples def categorize_item ( self , item_args ): \"\"\" The `categorize_item` function updates item arguments with prompt inputs, invokes a chain, handles exceptions, and returns the result or None. :param item_args: The `item_args` parameter in the `categorize_item` method seems to be a dictionary containing arguments related to an item. These arguments are updated with prompt inputs obtained from the `_prepare_prompt_inputs` method before being passed to the `chain.invoke` method for processing :return: The `categorize_item` method returns the result of invoking the `chain` with the `item_args` after updating it with prompt inputs. If the result is `None`, a `ValueError` is raised. If an exception occurs during the processing, an error message is printed along with the exception details, and `None` is returned. \"\"\" # Get the prompt inputs from _prepare_prompt_inputs and update item_args prompt_inputs = self . _prepare_prompt_inputs () item_args . update ( prompt_inputs ) try : result = self . chain . invoke ( item_args ) if result is None : raise ValueError ( f \"Chain returned None for input: { item_args } \" ) return result except Exception as e : print ( f \"Error processing item: { item_args . get ( 'item' , '' ) } \" ) print ( f \"Exception: { e } \" ) traceback . print_exc () return None categorize_item ( item_args ) The categorize_item function updates item arguments with prompt inputs, invokes a chain, handles exceptions, and returns the result or None. :param item_args: The item_args parameter in the categorize_item method seems to be a dictionary containing arguments related to an item. These arguments are updated with prompt inputs obtained from the _prepare_prompt_inputs method before being passed to the chain.invoke method for processing :return: The categorize_item method returns the result of invoking the chain with the item_args after updating it with prompt inputs. If the result is None , a ValueError is raised. If an exception occurs during the processing, an error message is printed along with the exception details, and None is returned. Source code in LabeLMaker/Categorize/fewshot.py 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 def categorize_item ( self , item_args ): \"\"\" The `categorize_item` function updates item arguments with prompt inputs, invokes a chain, handles exceptions, and returns the result or None. :param item_args: The `item_args` parameter in the `categorize_item` method seems to be a dictionary containing arguments related to an item. These arguments are updated with prompt inputs obtained from the `_prepare_prompt_inputs` method before being passed to the `chain.invoke` method for processing :return: The `categorize_item` method returns the result of invoking the `chain` with the `item_args` after updating it with prompt inputs. If the result is `None`, a `ValueError` is raised. If an exception occurs during the processing, an error message is printed along with the exception details, and `None` is returned. \"\"\" # Get the prompt inputs from _prepare_prompt_inputs and update item_args prompt_inputs = self . _prepare_prompt_inputs () item_args . update ( prompt_inputs ) try : result = self . chain . invoke ( item_args ) if result is None : raise ValueError ( f \"Chain returned None for input: { item_args } \" ) return result except Exception as e : print ( f \"Error processing item: { item_args . get ( 'item' , '' ) } \" ) print ( f \"Exception: { e } \" ) traceback . print_exc () return None","title":"Fewshot"},{"location":"LabeLMaker/Categorize/fewshot.html#LabeLMaker.Categorize.fewshot.FewShotCategorizer","text":"Bases: LabeLMaker Source code in LabeLMaker/Categorize/fewshot.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 class FewShotCategorizer ( LabeLMaker ): def __init__ ( self , prompty_path : Path , category_request : CategorizationRequest ): super () . __init__ ( prompty_path , category_request ) self . _validate_examples () def _get_filename ( self ): return \"fewshot_categorizer_output.txt\" def _get_mime_type ( self ): return \"text/plain\" def _validate_prompt_template ( self , prompt_template ): \"\"\" The function `_validate_prompt_template` checks if certain expected variables are present in a given prompt template for few-shot prompts. :param prompt_template: It looks like you are trying to validate mandatory variables in a few-shot prompt template. The expected variables are 'item', 'categories_with_descriptions', and 'examples'. The code snippet you provided checks if these variables are present in the prompt template. If any of the expected variables are missing, a \"\"\" # Validate mandatory variables for few-shot prompts expected_variables = [ \"item\" , \"categories_with_descriptions\" , \"examples\" ] for var in expected_variables : if f \" { var } \" not in prompt_template : raise ValueError ( f \"Expected variable {{ var }} not found in the few-shot prompt template.\" ) def _validate_examples ( self ): \"\"\" This function validates that there is at least one example for each proposed category in a categorization request. \"\"\" # Ensure that there is at least one example for each proposed category category_examples = { category . name : 0 for category in self . categorzation_request . categories } for example in self . categorzation_request . examples or []: if isinstance ( example , Example ): if example . label in category_examples : category_examples [ example . label ] += 1 else : raise TypeError ( f \"Expected an instance of Example, but got { type ( example ) } instead\" ) for category , count in category_examples . items (): if count == 0 : raise ValueError ( f \"No examples provided for category: ' { category } '. Each category must have at least one example.\" ) def _prepare_prompt_inputs ( self ): \"\"\" The `_prepare_prompt_inputs` function prepares categories with descriptions and examples for a prompt input. :return: The `_prepare_prompt_inputs` method returns a dictionary containing the following keys and values: - 'categories_with_descriptions': a list of dictionaries where each dictionary contains a category name and its description (or a default message if no description is provided) - 'examples': examples prepared by the `_prepare_examples` method if available, otherwise it is set to None \"\"\" # Prepare categories with descriptions like in ZeroShotCategorizer categories_with_descriptions = [] for category in self . categorzation_request . categories : if isinstance ( category , Categories ): categories_with_descriptions . append ( { \"category\" : category . name , \"description\" : ( category . description if category . description else \"No description provided\" ), } ) else : raise TypeError ( f \"Expected an instance of Categories, but got { type ( category ) } instead\" ) # Prepare examples if available examples = self . _prepare_examples () if self . categorzation_request . examples else None # Combine prompt inputs prompt_inputs = { \"categories_with_descriptions\" : categories_with_descriptions , \"examples\" : examples , } return prompt_inputs def _prepare_examples ( self ): \"\"\" The `_prepare_examples` function prepares example prompt text by converting them into a list of dictionaries, ensuring they are instances of the `Example` class. :return: The method `_prepare_examples` is returning a list of dictionaries containing the text with label and label of each example in the categorization request. The method also includes a debug print statement to show the prepared examples before returning them. \"\"\" # Prepare the example prompt text as a list of dictionaries prompt_examples = [] for example in self . categorzation_request . examples : if isinstance ( example , Example ): prompt_examples . append ( { \"text_with_label\" : example . text_with_label , \"label\" : example . label } ) else : raise TypeError ( f \"Expected an instance of Example, but got { type ( example ) } instead\" ) return prompt_examples def categorize_item ( self , item_args ): \"\"\" The `categorize_item` function updates item arguments with prompt inputs, invokes a chain, handles exceptions, and returns the result or None. :param item_args: The `item_args` parameter in the `categorize_item` method seems to be a dictionary containing arguments related to an item. These arguments are updated with prompt inputs obtained from the `_prepare_prompt_inputs` method before being passed to the `chain.invoke` method for processing :return: The `categorize_item` method returns the result of invoking the `chain` with the `item_args` after updating it with prompt inputs. If the result is `None`, a `ValueError` is raised. If an exception occurs during the processing, an error message is printed along with the exception details, and `None` is returned. \"\"\" # Get the prompt inputs from _prepare_prompt_inputs and update item_args prompt_inputs = self . _prepare_prompt_inputs () item_args . update ( prompt_inputs ) try : result = self . chain . invoke ( item_args ) if result is None : raise ValueError ( f \"Chain returned None for input: { item_args } \" ) return result except Exception as e : print ( f \"Error processing item: { item_args . get ( 'item' , '' ) } \" ) print ( f \"Exception: { e } \" ) traceback . print_exc () return None","title":"FewShotCategorizer"},{"location":"LabeLMaker/Categorize/fewshot.html#LabeLMaker.Categorize.fewshot.FewShotCategorizer.categorize_item","text":"The categorize_item function updates item arguments with prompt inputs, invokes a chain, handles exceptions, and returns the result or None. :param item_args: The item_args parameter in the categorize_item method seems to be a dictionary containing arguments related to an item. These arguments are updated with prompt inputs obtained from the _prepare_prompt_inputs method before being passed to the chain.invoke method for processing :return: The categorize_item method returns the result of invoking the chain with the item_args after updating it with prompt inputs. If the result is None , a ValueError is raised. If an exception occurs during the processing, an error message is printed along with the exception details, and None is returned. Source code in LabeLMaker/Categorize/fewshot.py 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 def categorize_item ( self , item_args ): \"\"\" The `categorize_item` function updates item arguments with prompt inputs, invokes a chain, handles exceptions, and returns the result or None. :param item_args: The `item_args` parameter in the `categorize_item` method seems to be a dictionary containing arguments related to an item. These arguments are updated with prompt inputs obtained from the `_prepare_prompt_inputs` method before being passed to the `chain.invoke` method for processing :return: The `categorize_item` method returns the result of invoking the `chain` with the `item_args` after updating it with prompt inputs. If the result is `None`, a `ValueError` is raised. If an exception occurs during the processing, an error message is printed along with the exception details, and `None` is returned. \"\"\" # Get the prompt inputs from _prepare_prompt_inputs and update item_args prompt_inputs = self . _prepare_prompt_inputs () item_args . update ( prompt_inputs ) try : result = self . chain . invoke ( item_args ) if result is None : raise ValueError ( f \"Chain returned None for input: { item_args } \" ) return result except Exception as e : print ( f \"Error processing item: { item_args . get ( 'item' , '' ) } \" ) print ( f \"Exception: { e } \" ) traceback . print_exc () return None","title":"categorize_item"},{"location":"LabeLMaker/Categorize/manyshot.html","text":"ManyshotClassifier Bases: BaseCategorizer Source code in LabeLMaker/Categorize/manyshot.py 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 class ManyshotClassifier ( BaseCategorizer ): def __init__ ( self , categorization_request : CategorizationRequest , min_class_count : int ): print ( \"Initializing Many-shot categorizer\" ) super () . __init__ () self . categorization_request = categorization_request self . min_class_count = min_class_count self . model = None self . client = Config . EMBEDDING_CLIENT self . encoder = tiktoken . encoding_for_model ( \"gpt-4\" ) self . max_context_length = 8192 def _get_filename ( self ): raise NotImplementedError def _get_mime_type ( self ): raise NotImplementedError def get_embeddings ( self , text ): \"\"\" This function takes a text input, encodes it, truncates it if necessary, and then uses an Azure OpenAI client to embed the truncated text. Args: text: The `get_embeddings` function takes a text input as a parameter. This text input is then encoded using the `self.encoder.encode` method. The encoded tokens are then truncated based on the `max_context_length` and decoded back into text. If the truncated text is empty after stripping, the function Returns: The `get_embeddings` method returns the response from the Azure OpenAI client after encoding and querying the input text. \"\"\" tokens = self . encoder . encode ( text ) truncated_text = self . encoder . decode ( tokens [: self . max_context_length ]) if not truncated_text . strip (): return None # Replace with your Azure OpenAI client code response = self . client . embed_query ( truncated_text ) return response def preprocess_data ( self ): \"\"\" The `preprocess_data` function normalizes text data in both unlabeled and labeled examples for categorization. \"\"\" # Unlabeled texts: these come from the full list. self . normalized_unlabeled_list = [ normalize_text ( text ) if isinstance ( text , str ) else text for text in self . categorization_request . text_to_label ] # Normalize the training examples. self . normalized_example_list = [] for example in self . categorization_request . examples : # Handle Example objects if hasattr ( example , \"text_with_label\" ): text_with_label = example . text_with_label # Allow tuple or list (text, label) format elif isinstance ( example , ( tuple , list )) and len ( example ) == 2 : text_with_label = example [ 0 ] else : raise TypeError ( \"Invalid example format. Expected Example object or tuple (text, label).\" ) self . normalized_example_list . append ( normalize_text ( text_with_label ) if isinstance ( text_with_label , str ) else text_with_label ) def select_model ( self ): \"\"\" The `select_model` function chooses between Nearest Centroid and Multinomial Logistic Regression models based on a minimum class count threshold. \"\"\" if self . min_class_count < Config . MIN_LOGISTIC_SAMPLES_PER_CLASS : self . model = NearestCentroid () self . model_name = \"Nearest Centroid\" else : self . model = LogisticRegression ( max_iter = 1000 , penalty = \"elasticnet\" , solver = \"saga\" , n_jobs =- 1 , l1_ratio = 0.5 , multi_class = \"multinomial\" , ) self . model_name = \"Multinomial Logistic Regression\" def embed_data ( self , texts ): \"\"\" The `embed_data` function takes a list of texts, retrieves embeddings for each text using the `get_embeddings` method, and returns a list of non-None embeddings. Args: texts: The `embed_data` method takes a list of texts as input. It then iterates over each text in the list, retrieves its embedding using the `get_embeddings` method, and appends the embedding to a list called `embeddings`. Finally, it returns the list of embeddings. Returns: The `embed_data` method returns a list of embeddings for the input texts. \"\"\" embeddings = [] for text in texts : embedding = self . get_embeddings ( text ) if embedding is not None : embeddings . append ( embedding ) return embeddings def train_model ( self ): \"\"\" The `train_model` function selects a model, extracts labels from examples, trains embeddings, and fits the model with the embeddings and labels. \"\"\" self . select_model () labels = [] for example in self . categorization_request . examples : # Handle Example objects if hasattr ( example , \"label\" ): labels . append ( example . label ) # Handle tuple or list (text, label) elif isinstance ( example , ( tuple , list )) and len ( example ) == 2 : labels . append ( example [ 1 ]) else : raise TypeError ( \"Invalid example format. Expected Example object or tuple (text, label).\" ) self . train_embeddings = self . embed_data ( self . normalized_example_list ) self . model . fit ( self . train_embeddings , labels ) def predict_unlabeled ( self ): \"\"\" The `predict_unlabeled` function predicts labels for unlabeled text data and provides prediction probabilities and rationales for each prediction. Returns: The `predict_unlabeled` method returns a list of 4-tuples, where each tuple contains the unique ID, text, predicted category, and rationale for the prediction of unlabeled data points. \"\"\" categorized_results = [] unlabeled_text_embeddings = self . embed_data ( self . normalized_unlabeled_list ) unlabeled_labels = self . model . predict ( unlabeled_text_embeddings ) # Get prediction probabilities for each class for the unlabeled data. prediction_probabilities = self . model . predict_proba ( unlabeled_text_embeddings ) rationales = [] for prob in prediction_probabilities : formatted_probs = [ f \" { cls } : { p : .4f } \" if abs ( p ) >= 1e-4 else f \" { cls } : { p : .4e } \" for cls , p in zip ( self . model . classes_ , prob ) ] rationale = \" \" . join ( formatted_probs ) rationales . append ( rationale ) # Build a 4-tuple for each prediction. # Assuming self.categorization_request.unique_ids exists. for uid , text , category , reason in zip ( self . categorization_request . unique_ids , self . categorization_request . text_to_label , unlabeled_labels , rationales , ): categorized_results . append (( uid , text , category , reason )) return categorized_results def process ( self ): \"\"\" Process the request by preprocessing data, training the model, and predicting labels for unlabeled text. Returns a list of 4-tuples (uid, text, predicted label, rationale). \"\"\" st . write ( str ( self . __class__ . __name__ ) + \" operation in progress. Please wait...\" ) self . preprocess_data () self . train_model () return self . predict_unlabeled () embed_data ( texts ) The embed_data function takes a list of texts, retrieves embeddings for each text using the get_embeddings method, and returns a list of non-None embeddings. Parameters: texts \u2013 The embed_data method takes a list of texts as input. It then iterates over each text in the list, retrieves its embedding using the get_embeddings method, and appends the embedding to a list called embeddings . Finally, it returns the list of embeddings. Returns: \u2013 The embed_data method returns a list of embeddings for the input texts. Source code in LabeLMaker/Categorize/manyshot.py 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 def embed_data ( self , texts ): \"\"\" The `embed_data` function takes a list of texts, retrieves embeddings for each text using the `get_embeddings` method, and returns a list of non-None embeddings. Args: texts: The `embed_data` method takes a list of texts as input. It then iterates over each text in the list, retrieves its embedding using the `get_embeddings` method, and appends the embedding to a list called `embeddings`. Finally, it returns the list of embeddings. Returns: The `embed_data` method returns a list of embeddings for the input texts. \"\"\" embeddings = [] for text in texts : embedding = self . get_embeddings ( text ) if embedding is not None : embeddings . append ( embedding ) return embeddings get_embeddings ( text ) This function takes a text input, encodes it, truncates it if necessary, and then uses an Azure OpenAI client to embed the truncated text. Parameters: text \u2013 The get_embeddings function takes a text input as a parameter. This text input is then encoded using the self.encoder.encode method. The encoded tokens are then truncated based on the max_context_length and decoded back into text. If the truncated text is empty after stripping, the function Returns: \u2013 The get_embeddings method returns the response from the Azure OpenAI client after encoding and querying the input text. Source code in LabeLMaker/Categorize/manyshot.py 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 def get_embeddings ( self , text ): \"\"\" This function takes a text input, encodes it, truncates it if necessary, and then uses an Azure OpenAI client to embed the truncated text. Args: text: The `get_embeddings` function takes a text input as a parameter. This text input is then encoded using the `self.encoder.encode` method. The encoded tokens are then truncated based on the `max_context_length` and decoded back into text. If the truncated text is empty after stripping, the function Returns: The `get_embeddings` method returns the response from the Azure OpenAI client after encoding and querying the input text. \"\"\" tokens = self . encoder . encode ( text ) truncated_text = self . encoder . decode ( tokens [: self . max_context_length ]) if not truncated_text . strip (): return None # Replace with your Azure OpenAI client code response = self . client . embed_query ( truncated_text ) return response predict_unlabeled () The predict_unlabeled function predicts labels for unlabeled text data and provides prediction probabilities and rationales for each prediction. Returns: \u2013 The predict_unlabeled method returns a list of 4-tuples, where each tuple contains the unique ID, text, predicted category, and rationale for the prediction of unlabeled data points. Source code in LabeLMaker/Categorize/manyshot.py 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 def predict_unlabeled ( self ): \"\"\" The `predict_unlabeled` function predicts labels for unlabeled text data and provides prediction probabilities and rationales for each prediction. Returns: The `predict_unlabeled` method returns a list of 4-tuples, where each tuple contains the unique ID, text, predicted category, and rationale for the prediction of unlabeled data points. \"\"\" categorized_results = [] unlabeled_text_embeddings = self . embed_data ( self . normalized_unlabeled_list ) unlabeled_labels = self . model . predict ( unlabeled_text_embeddings ) # Get prediction probabilities for each class for the unlabeled data. prediction_probabilities = self . model . predict_proba ( unlabeled_text_embeddings ) rationales = [] for prob in prediction_probabilities : formatted_probs = [ f \" { cls } : { p : .4f } \" if abs ( p ) >= 1e-4 else f \" { cls } : { p : .4e } \" for cls , p in zip ( self . model . classes_ , prob ) ] rationale = \" \" . join ( formatted_probs ) rationales . append ( rationale ) # Build a 4-tuple for each prediction. # Assuming self.categorization_request.unique_ids exists. for uid , text , category , reason in zip ( self . categorization_request . unique_ids , self . categorization_request . text_to_label , unlabeled_labels , rationales , ): categorized_results . append (( uid , text , category , reason )) return categorized_results preprocess_data () The preprocess_data function normalizes text data in both unlabeled and labeled examples for categorization. Source code in LabeLMaker/Categorize/manyshot.py 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 def preprocess_data ( self ): \"\"\" The `preprocess_data` function normalizes text data in both unlabeled and labeled examples for categorization. \"\"\" # Unlabeled texts: these come from the full list. self . normalized_unlabeled_list = [ normalize_text ( text ) if isinstance ( text , str ) else text for text in self . categorization_request . text_to_label ] # Normalize the training examples. self . normalized_example_list = [] for example in self . categorization_request . examples : # Handle Example objects if hasattr ( example , \"text_with_label\" ): text_with_label = example . text_with_label # Allow tuple or list (text, label) format elif isinstance ( example , ( tuple , list )) and len ( example ) == 2 : text_with_label = example [ 0 ] else : raise TypeError ( \"Invalid example format. Expected Example object or tuple (text, label).\" ) self . normalized_example_list . append ( normalize_text ( text_with_label ) if isinstance ( text_with_label , str ) else text_with_label ) process () Process the request by preprocessing data, training the model, and predicting labels for unlabeled text. Returns a list of 4-tuples (uid, text, predicted label, rationale). Source code in LabeLMaker/Categorize/manyshot.py 183 184 185 186 187 188 189 190 191 192 def process ( self ): \"\"\" Process the request by preprocessing data, training the model, and predicting labels for unlabeled text. Returns a list of 4-tuples (uid, text, predicted label, rationale). \"\"\" st . write ( str ( self . __class__ . __name__ ) + \" operation in progress. Please wait...\" ) self . preprocess_data () self . train_model () return self . predict_unlabeled () select_model () The select_model function chooses between Nearest Centroid and Multinomial Logistic Regression models based on a minimum class count threshold. Source code in LabeLMaker/Categorize/manyshot.py 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 def select_model ( self ): \"\"\" The `select_model` function chooses between Nearest Centroid and Multinomial Logistic Regression models based on a minimum class count threshold. \"\"\" if self . min_class_count < Config . MIN_LOGISTIC_SAMPLES_PER_CLASS : self . model = NearestCentroid () self . model_name = \"Nearest Centroid\" else : self . model = LogisticRegression ( max_iter = 1000 , penalty = \"elasticnet\" , solver = \"saga\" , n_jobs =- 1 , l1_ratio = 0.5 , multi_class = \"multinomial\" , ) self . model_name = \"Multinomial Logistic Regression\" train_model () The train_model function selects a model, extracts labels from examples, trains embeddings, and fits the model with the embeddings and labels. Source code in LabeLMaker/Categorize/manyshot.py 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 def train_model ( self ): \"\"\" The `train_model` function selects a model, extracts labels from examples, trains embeddings, and fits the model with the embeddings and labels. \"\"\" self . select_model () labels = [] for example in self . categorization_request . examples : # Handle Example objects if hasattr ( example , \"label\" ): labels . append ( example . label ) # Handle tuple or list (text, label) elif isinstance ( example , ( tuple , list )) and len ( example ) == 2 : labels . append ( example [ 1 ]) else : raise TypeError ( \"Invalid example format. Expected Example object or tuple (text, label).\" ) self . train_embeddings = self . embed_data ( self . normalized_example_list ) self . model . fit ( self . train_embeddings , labels )","title":"Manyshot"},{"location":"LabeLMaker/Categorize/manyshot.html#LabeLMaker.Categorize.manyshot.ManyshotClassifier","text":"Bases: BaseCategorizer Source code in LabeLMaker/Categorize/manyshot.py 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 class ManyshotClassifier ( BaseCategorizer ): def __init__ ( self , categorization_request : CategorizationRequest , min_class_count : int ): print ( \"Initializing Many-shot categorizer\" ) super () . __init__ () self . categorization_request = categorization_request self . min_class_count = min_class_count self . model = None self . client = Config . EMBEDDING_CLIENT self . encoder = tiktoken . encoding_for_model ( \"gpt-4\" ) self . max_context_length = 8192 def _get_filename ( self ): raise NotImplementedError def _get_mime_type ( self ): raise NotImplementedError def get_embeddings ( self , text ): \"\"\" This function takes a text input, encodes it, truncates it if necessary, and then uses an Azure OpenAI client to embed the truncated text. Args: text: The `get_embeddings` function takes a text input as a parameter. This text input is then encoded using the `self.encoder.encode` method. The encoded tokens are then truncated based on the `max_context_length` and decoded back into text. If the truncated text is empty after stripping, the function Returns: The `get_embeddings` method returns the response from the Azure OpenAI client after encoding and querying the input text. \"\"\" tokens = self . encoder . encode ( text ) truncated_text = self . encoder . decode ( tokens [: self . max_context_length ]) if not truncated_text . strip (): return None # Replace with your Azure OpenAI client code response = self . client . embed_query ( truncated_text ) return response def preprocess_data ( self ): \"\"\" The `preprocess_data` function normalizes text data in both unlabeled and labeled examples for categorization. \"\"\" # Unlabeled texts: these come from the full list. self . normalized_unlabeled_list = [ normalize_text ( text ) if isinstance ( text , str ) else text for text in self . categorization_request . text_to_label ] # Normalize the training examples. self . normalized_example_list = [] for example in self . categorization_request . examples : # Handle Example objects if hasattr ( example , \"text_with_label\" ): text_with_label = example . text_with_label # Allow tuple or list (text, label) format elif isinstance ( example , ( tuple , list )) and len ( example ) == 2 : text_with_label = example [ 0 ] else : raise TypeError ( \"Invalid example format. Expected Example object or tuple (text, label).\" ) self . normalized_example_list . append ( normalize_text ( text_with_label ) if isinstance ( text_with_label , str ) else text_with_label ) def select_model ( self ): \"\"\" The `select_model` function chooses between Nearest Centroid and Multinomial Logistic Regression models based on a minimum class count threshold. \"\"\" if self . min_class_count < Config . MIN_LOGISTIC_SAMPLES_PER_CLASS : self . model = NearestCentroid () self . model_name = \"Nearest Centroid\" else : self . model = LogisticRegression ( max_iter = 1000 , penalty = \"elasticnet\" , solver = \"saga\" , n_jobs =- 1 , l1_ratio = 0.5 , multi_class = \"multinomial\" , ) self . model_name = \"Multinomial Logistic Regression\" def embed_data ( self , texts ): \"\"\" The `embed_data` function takes a list of texts, retrieves embeddings for each text using the `get_embeddings` method, and returns a list of non-None embeddings. Args: texts: The `embed_data` method takes a list of texts as input. It then iterates over each text in the list, retrieves its embedding using the `get_embeddings` method, and appends the embedding to a list called `embeddings`. Finally, it returns the list of embeddings. Returns: The `embed_data` method returns a list of embeddings for the input texts. \"\"\" embeddings = [] for text in texts : embedding = self . get_embeddings ( text ) if embedding is not None : embeddings . append ( embedding ) return embeddings def train_model ( self ): \"\"\" The `train_model` function selects a model, extracts labels from examples, trains embeddings, and fits the model with the embeddings and labels. \"\"\" self . select_model () labels = [] for example in self . categorization_request . examples : # Handle Example objects if hasattr ( example , \"label\" ): labels . append ( example . label ) # Handle tuple or list (text, label) elif isinstance ( example , ( tuple , list )) and len ( example ) == 2 : labels . append ( example [ 1 ]) else : raise TypeError ( \"Invalid example format. Expected Example object or tuple (text, label).\" ) self . train_embeddings = self . embed_data ( self . normalized_example_list ) self . model . fit ( self . train_embeddings , labels ) def predict_unlabeled ( self ): \"\"\" The `predict_unlabeled` function predicts labels for unlabeled text data and provides prediction probabilities and rationales for each prediction. Returns: The `predict_unlabeled` method returns a list of 4-tuples, where each tuple contains the unique ID, text, predicted category, and rationale for the prediction of unlabeled data points. \"\"\" categorized_results = [] unlabeled_text_embeddings = self . embed_data ( self . normalized_unlabeled_list ) unlabeled_labels = self . model . predict ( unlabeled_text_embeddings ) # Get prediction probabilities for each class for the unlabeled data. prediction_probabilities = self . model . predict_proba ( unlabeled_text_embeddings ) rationales = [] for prob in prediction_probabilities : formatted_probs = [ f \" { cls } : { p : .4f } \" if abs ( p ) >= 1e-4 else f \" { cls } : { p : .4e } \" for cls , p in zip ( self . model . classes_ , prob ) ] rationale = \" \" . join ( formatted_probs ) rationales . append ( rationale ) # Build a 4-tuple for each prediction. # Assuming self.categorization_request.unique_ids exists. for uid , text , category , reason in zip ( self . categorization_request . unique_ids , self . categorization_request . text_to_label , unlabeled_labels , rationales , ): categorized_results . append (( uid , text , category , reason )) return categorized_results def process ( self ): \"\"\" Process the request by preprocessing data, training the model, and predicting labels for unlabeled text. Returns a list of 4-tuples (uid, text, predicted label, rationale). \"\"\" st . write ( str ( self . __class__ . __name__ ) + \" operation in progress. Please wait...\" ) self . preprocess_data () self . train_model () return self . predict_unlabeled ()","title":"ManyshotClassifier"},{"location":"LabeLMaker/Categorize/manyshot.html#LabeLMaker.Categorize.manyshot.ManyshotClassifier.embed_data","text":"The embed_data function takes a list of texts, retrieves embeddings for each text using the get_embeddings method, and returns a list of non-None embeddings. Parameters: texts \u2013 The embed_data method takes a list of texts as input. It then iterates over each text in the list, retrieves its embedding using the get_embeddings method, and appends the embedding to a list called embeddings . Finally, it returns the list of embeddings. Returns: \u2013 The embed_data method returns a list of embeddings for the input texts. Source code in LabeLMaker/Categorize/manyshot.py 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 def embed_data ( self , texts ): \"\"\" The `embed_data` function takes a list of texts, retrieves embeddings for each text using the `get_embeddings` method, and returns a list of non-None embeddings. Args: texts: The `embed_data` method takes a list of texts as input. It then iterates over each text in the list, retrieves its embedding using the `get_embeddings` method, and appends the embedding to a list called `embeddings`. Finally, it returns the list of embeddings. Returns: The `embed_data` method returns a list of embeddings for the input texts. \"\"\" embeddings = [] for text in texts : embedding = self . get_embeddings ( text ) if embedding is not None : embeddings . append ( embedding ) return embeddings","title":"embed_data"},{"location":"LabeLMaker/Categorize/manyshot.html#LabeLMaker.Categorize.manyshot.ManyshotClassifier.get_embeddings","text":"This function takes a text input, encodes it, truncates it if necessary, and then uses an Azure OpenAI client to embed the truncated text. Parameters: text \u2013 The get_embeddings function takes a text input as a parameter. This text input is then encoded using the self.encoder.encode method. The encoded tokens are then truncated based on the max_context_length and decoded back into text. If the truncated text is empty after stripping, the function Returns: \u2013 The get_embeddings method returns the response from the Azure OpenAI client after encoding and querying the input text. Source code in LabeLMaker/Categorize/manyshot.py 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 def get_embeddings ( self , text ): \"\"\" This function takes a text input, encodes it, truncates it if necessary, and then uses an Azure OpenAI client to embed the truncated text. Args: text: The `get_embeddings` function takes a text input as a parameter. This text input is then encoded using the `self.encoder.encode` method. The encoded tokens are then truncated based on the `max_context_length` and decoded back into text. If the truncated text is empty after stripping, the function Returns: The `get_embeddings` method returns the response from the Azure OpenAI client after encoding and querying the input text. \"\"\" tokens = self . encoder . encode ( text ) truncated_text = self . encoder . decode ( tokens [: self . max_context_length ]) if not truncated_text . strip (): return None # Replace with your Azure OpenAI client code response = self . client . embed_query ( truncated_text ) return response","title":"get_embeddings"},{"location":"LabeLMaker/Categorize/manyshot.html#LabeLMaker.Categorize.manyshot.ManyshotClassifier.predict_unlabeled","text":"The predict_unlabeled function predicts labels for unlabeled text data and provides prediction probabilities and rationales for each prediction. Returns: \u2013 The predict_unlabeled method returns a list of 4-tuples, where each tuple contains the unique ID, text, predicted category, and rationale for the prediction of unlabeled data points. Source code in LabeLMaker/Categorize/manyshot.py 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 def predict_unlabeled ( self ): \"\"\" The `predict_unlabeled` function predicts labels for unlabeled text data and provides prediction probabilities and rationales for each prediction. Returns: The `predict_unlabeled` method returns a list of 4-tuples, where each tuple contains the unique ID, text, predicted category, and rationale for the prediction of unlabeled data points. \"\"\" categorized_results = [] unlabeled_text_embeddings = self . embed_data ( self . normalized_unlabeled_list ) unlabeled_labels = self . model . predict ( unlabeled_text_embeddings ) # Get prediction probabilities for each class for the unlabeled data. prediction_probabilities = self . model . predict_proba ( unlabeled_text_embeddings ) rationales = [] for prob in prediction_probabilities : formatted_probs = [ f \" { cls } : { p : .4f } \" if abs ( p ) >= 1e-4 else f \" { cls } : { p : .4e } \" for cls , p in zip ( self . model . classes_ , prob ) ] rationale = \" \" . join ( formatted_probs ) rationales . append ( rationale ) # Build a 4-tuple for each prediction. # Assuming self.categorization_request.unique_ids exists. for uid , text , category , reason in zip ( self . categorization_request . unique_ids , self . categorization_request . text_to_label , unlabeled_labels , rationales , ): categorized_results . append (( uid , text , category , reason )) return categorized_results","title":"predict_unlabeled"},{"location":"LabeLMaker/Categorize/manyshot.html#LabeLMaker.Categorize.manyshot.ManyshotClassifier.preprocess_data","text":"The preprocess_data function normalizes text data in both unlabeled and labeled examples for categorization. Source code in LabeLMaker/Categorize/manyshot.py 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 def preprocess_data ( self ): \"\"\" The `preprocess_data` function normalizes text data in both unlabeled and labeled examples for categorization. \"\"\" # Unlabeled texts: these come from the full list. self . normalized_unlabeled_list = [ normalize_text ( text ) if isinstance ( text , str ) else text for text in self . categorization_request . text_to_label ] # Normalize the training examples. self . normalized_example_list = [] for example in self . categorization_request . examples : # Handle Example objects if hasattr ( example , \"text_with_label\" ): text_with_label = example . text_with_label # Allow tuple or list (text, label) format elif isinstance ( example , ( tuple , list )) and len ( example ) == 2 : text_with_label = example [ 0 ] else : raise TypeError ( \"Invalid example format. Expected Example object or tuple (text, label).\" ) self . normalized_example_list . append ( normalize_text ( text_with_label ) if isinstance ( text_with_label , str ) else text_with_label )","title":"preprocess_data"},{"location":"LabeLMaker/Categorize/manyshot.html#LabeLMaker.Categorize.manyshot.ManyshotClassifier.process","text":"Process the request by preprocessing data, training the model, and predicting labels for unlabeled text. Returns a list of 4-tuples (uid, text, predicted label, rationale). Source code in LabeLMaker/Categorize/manyshot.py 183 184 185 186 187 188 189 190 191 192 def process ( self ): \"\"\" Process the request by preprocessing data, training the model, and predicting labels for unlabeled text. Returns a list of 4-tuples (uid, text, predicted label, rationale). \"\"\" st . write ( str ( self . __class__ . __name__ ) + \" operation in progress. Please wait...\" ) self . preprocess_data () self . train_model () return self . predict_unlabeled ()","title":"process"},{"location":"LabeLMaker/Categorize/manyshot.html#LabeLMaker.Categorize.manyshot.ManyshotClassifier.select_model","text":"The select_model function chooses between Nearest Centroid and Multinomial Logistic Regression models based on a minimum class count threshold. Source code in LabeLMaker/Categorize/manyshot.py 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 def select_model ( self ): \"\"\" The `select_model` function chooses between Nearest Centroid and Multinomial Logistic Regression models based on a minimum class count threshold. \"\"\" if self . min_class_count < Config . MIN_LOGISTIC_SAMPLES_PER_CLASS : self . model = NearestCentroid () self . model_name = \"Nearest Centroid\" else : self . model = LogisticRegression ( max_iter = 1000 , penalty = \"elasticnet\" , solver = \"saga\" , n_jobs =- 1 , l1_ratio = 0.5 , multi_class = \"multinomial\" , ) self . model_name = \"Multinomial Logistic Regression\"","title":"select_model"},{"location":"LabeLMaker/Categorize/manyshot.html#LabeLMaker.Categorize.manyshot.ManyshotClassifier.train_model","text":"The train_model function selects a model, extracts labels from examples, trains embeddings, and fits the model with the embeddings and labels. Source code in LabeLMaker/Categorize/manyshot.py 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 def train_model ( self ): \"\"\" The `train_model` function selects a model, extracts labels from examples, trains embeddings, and fits the model with the embeddings and labels. \"\"\" self . select_model () labels = [] for example in self . categorization_request . examples : # Handle Example objects if hasattr ( example , \"label\" ): labels . append ( example . label ) # Handle tuple or list (text, label) elif isinstance ( example , ( tuple , list )) and len ( example ) == 2 : labels . append ( example [ 1 ]) else : raise TypeError ( \"Invalid example format. Expected Example object or tuple (text, label).\" ) self . train_embeddings = self . embed_data ( self . normalized_example_list ) self . model . fit ( self . train_embeddings , labels )","title":"train_model"},{"location":"LabeLMaker/Categorize/zeroshot.html","text":"ZeroShotCategorizer Bases: LabeLMaker Source code in LabeLMaker/Categorize/zeroshot.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 class ZeroShotCategorizer ( LabeLMaker ): def __init__ ( self , prompty_path : Path , category_request : CategorizationRequest ): super () . __init__ ( prompty_path , category_request ) def _get_filename ( self ): return \"zeroshot_categorizer_output.txt\" def _get_mime_type ( self ): return \"text/plain\" def _validate_prompt_template ( self , prompt_template ): \"\"\" The function `_validate_prompt_template` checks if mandatory variables for zero-shot prompts are present in a given prompt template. :param prompt_template: It looks like the code snippet you provided is a Python function for validating a prompt template against a list of expected variables. The function checks if certain variables ('item' and 'categories_with_descriptions') are present in the prompt template string \"\"\" # Validate mandatory variables for zero-shot prompts expected_variables = [ \"item\" , \"categories_with_descriptions\" ] for var in expected_variables : if f \" { var } \" not in prompt_template : raise ValueError ( f \"Expected variable {{ var }} not found in the zero-shot prompt template.\" ) def _prepare_prompt_inputs ( self ): \"\"\" The `_prepare_prompt_inputs` function prepares a list of categories with their descriptions, handling different types of categories appropriately. :return: A list of dictionaries containing the category name and description for each category in the categorization request. If a category does not have a description, it will be set to \"No description provided\". \"\"\" categories_with_descriptions = [] for category in self . categorzation_request . categories : if isinstance ( category , Categories ): categories_with_descriptions . append ( { \"category\" : category . name , \"description\" : ( category . description if category . description else \"No description provided\" ), } ) else : raise TypeError ( f \"Expected an instance of Categories, but got { type ( category ) } instead\" ) prompt_inputs = { \"categories_with_descriptions\" : categories_with_descriptions } return prompt_inputs","title":"Zeroshot"},{"location":"LabeLMaker/Categorize/zeroshot.html#LabeLMaker.Categorize.zeroshot.ZeroShotCategorizer","text":"Bases: LabeLMaker Source code in LabeLMaker/Categorize/zeroshot.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 class ZeroShotCategorizer ( LabeLMaker ): def __init__ ( self , prompty_path : Path , category_request : CategorizationRequest ): super () . __init__ ( prompty_path , category_request ) def _get_filename ( self ): return \"zeroshot_categorizer_output.txt\" def _get_mime_type ( self ): return \"text/plain\" def _validate_prompt_template ( self , prompt_template ): \"\"\" The function `_validate_prompt_template` checks if mandatory variables for zero-shot prompts are present in a given prompt template. :param prompt_template: It looks like the code snippet you provided is a Python function for validating a prompt template against a list of expected variables. The function checks if certain variables ('item' and 'categories_with_descriptions') are present in the prompt template string \"\"\" # Validate mandatory variables for zero-shot prompts expected_variables = [ \"item\" , \"categories_with_descriptions\" ] for var in expected_variables : if f \" { var } \" not in prompt_template : raise ValueError ( f \"Expected variable {{ var }} not found in the zero-shot prompt template.\" ) def _prepare_prompt_inputs ( self ): \"\"\" The `_prepare_prompt_inputs` function prepares a list of categories with their descriptions, handling different types of categories appropriately. :return: A list of dictionaries containing the category name and description for each category in the categorization request. If a category does not have a description, it will be set to \"No description provided\". \"\"\" categories_with_descriptions = [] for category in self . categorzation_request . categories : if isinstance ( category , Categories ): categories_with_descriptions . append ( { \"category\" : category . name , \"description\" : ( category . description if category . description else \"No description provided\" ), } ) else : raise TypeError ( f \"Expected an instance of Categories, but got { type ( category ) } instead\" ) prompt_inputs = { \"categories_with_descriptions\" : categories_with_descriptions } return prompt_inputs","title":"ZeroShotCategorizer"},{"location":"LabeLMaker/Evaluate/confidence_intervals.html","text":"The code defines functions to compute performance metric values and their bootstrap-based confidence intervals for classification tasks. :param y_true: y_true refers to the true labels in a classification task. It is an array-like object containing the actual class labels for the data points. In the provided example, y_true_example is a list of true labels for a set of observations, where each label corresponds to a specific class :param y_pred: The y_pred parameter represents the predicted labels for your classification task. It should be an array-like object containing the predicted labels for each corresponding sample in y_true . In the provided example usage, y_pred_example is a list of predicted labels for the dummy data samples :param score_func: The score_func parameter in the bootstrap_metric function is a callable function that computes a specific metric (e.g., accuracy, precision, recall, F1 score) based on the true labels ( y_true ) and predicted labels ( y_pred ). It allows you to pass different scoring functions :param n_bootstraps: The n_bootstraps parameter in the provided code refers to the number of bootstrap iterations to perform when estimating the confidence intervals for the performance metrics. It determines how many times the bootstrap resampling process will be repeated to calculate the mean score and confidence intervals for each metric, defaults to 1000 (optional) :param alpha: The alpha parameter in the provided code represents the significance level used to calculate the confidence intervals. In statistical hypothesis testing and confidence interval construction, the significance level is the probability of rejecting the null hypothesis when it is true :return: The compute_bootstrap_confidence_intervals function returns a dictionary containing performance metric values (Accuracy, Precision, Recall, and F1 Score) along with their corresponding bootstrap mean and 95% confidence intervals. Each metric in the dictionary has the following structure: bootstrap_metric ( y_true , y_pred , score_func , n_bootstraps = 1000 , alpha = 0.05 , ** kwargs ) Computes the bootstrap distribution for a given metric. Parameters: y_true ( array - like ) \u2013 True labels. y_pred ( array - like ) \u2013 Predicted labels. score_func ( callable ) \u2013 Function to compute the metric. n_bootstraps ( int , default: 1000 ) \u2013 Number of bootstrap samples. alpha ( float , default: 0.05 ) \u2013 Significance level (for a 95% CI, alpha=0.05). **kwargs \u2013 Additional arguments to pass to score_func. Returns: mean_score ( float ) \u2013 Mean score from bootstrap samples. ci ( tuple ) \u2013 (lower bound, upper bound) as the confidence interval. Source code in LabeLMaker/Evaluate/confidence_intervals.py 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 def bootstrap_metric ( y_true , y_pred , score_func , n_bootstraps = 1000 , alpha = 0.05 , ** kwargs ): \"\"\" Computes the bootstrap distribution for a given metric. Parameters: y_true (array-like): True labels. y_pred (array-like): Predicted labels. score_func (callable): Function to compute the metric. n_bootstraps (int): Number of bootstrap samples. alpha (float): Significance level (for a 95% CI, alpha=0.05). **kwargs: Additional arguments to pass to score_func. Returns: mean_score (float): Mean score from bootstrap samples. ci (tuple): (lower bound, upper bound) as the confidence interval. \"\"\" scores = [] y_true = np . array ( y_true ) y_pred = np . array ( y_pred ) n = len ( y_true ) for i in range ( n_bootstraps ): # sample indices with replacement indices = np . random . choice ( n , n , replace = True ) sample_y_true = y_true [ indices ] sample_y_pred = y_pred [ indices ] score = score_func ( sample_y_true , sample_y_pred , ** kwargs ) scores . append ( score ) # Compute percentile bounds for the desired confidence level lower = np . percentile ( scores , 100 * ( alpha / 2 )) upper = np . percentile ( scores , 100 * ( 1 - alpha / 2 )) mean_score = np . mean ( scores ) return mean_score , ( lower , upper ) compute_bootstrap_confidence_intervals ( y_true , y_pred , n_bootstraps = 1000 , alpha = 0.05 ) Computes performance metric values (accuracy, macro precision, recall, and F1) along with their bootstrap-based 95% confidence intervals. Parameters: y_true ( array - like ) \u2013 True labels. y_pred ( array - like ) \u2013 Predicted labels. n_bootstraps ( int , default: 1000 ) \u2013 Number of bootstrap iterations. alpha ( float , default: 0.05 ) \u2013 Significance level. Returns: results ( dict ) \u2013 Dictionary mapping each metric to its value, bootstrap mean, and 95% CI. Source code in LabeLMaker/Evaluate/confidence_intervals.py 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 def compute_bootstrap_confidence_intervals ( y_true , y_pred , n_bootstraps = 1000 , alpha = 0.05 ): \"\"\" Computes performance metric values (accuracy, macro precision, recall, and F1) along with their bootstrap-based 95% confidence intervals. Parameters: y_true (array-like): True labels. y_pred (array-like): Predicted labels. n_bootstraps (int): Number of bootstrap iterations. alpha (float): Significance level. Returns: results (dict): Dictionary mapping each metric to its value, bootstrap mean, and 95% CI. \"\"\" results = {} # Use macro-average for multiclass metrics. metrics = { \"Accuracy\" : accuracy_score , \"Precision\" : lambda yt , yp : precision_score ( yt , yp , average = \"macro\" , zero_division = 0 ), \"Recall\" : lambda yt , yp : recall_score ( yt , yp , average = \"macro\" , zero_division = 0 ), \"F1 Score\" : lambda yt , yp : f1_score ( yt , yp , average = \"macro\" , zero_division = 0 ), } # Compute values on full dataset and bootstrap estimates for metric_name , func in metrics . items (): full_value = func ( y_true , y_pred ) bs_mean , ci = bootstrap_metric ( y_true , y_pred , func , n_bootstraps = n_bootstraps , alpha = alpha ) results [ metric_name ] = { \"Value\" : full_value , \"Bootstrap Mean\" : bs_mean , \"95% CI\" : ci } return results","title":"Confidence Intervals"},{"location":"LabeLMaker/Evaluate/confidence_intervals.html#LabeLMaker.Evaluate.confidence_intervals.bootstrap_metric","text":"Computes the bootstrap distribution for a given metric. Parameters: y_true ( array - like ) \u2013 True labels. y_pred ( array - like ) \u2013 Predicted labels. score_func ( callable ) \u2013 Function to compute the metric. n_bootstraps ( int , default: 1000 ) \u2013 Number of bootstrap samples. alpha ( float , default: 0.05 ) \u2013 Significance level (for a 95% CI, alpha=0.05). **kwargs \u2013 Additional arguments to pass to score_func. Returns: mean_score ( float ) \u2013 Mean score from bootstrap samples. ci ( tuple ) \u2013 (lower bound, upper bound) as the confidence interval. Source code in LabeLMaker/Evaluate/confidence_intervals.py 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 def bootstrap_metric ( y_true , y_pred , score_func , n_bootstraps = 1000 , alpha = 0.05 , ** kwargs ): \"\"\" Computes the bootstrap distribution for a given metric. Parameters: y_true (array-like): True labels. y_pred (array-like): Predicted labels. score_func (callable): Function to compute the metric. n_bootstraps (int): Number of bootstrap samples. alpha (float): Significance level (for a 95% CI, alpha=0.05). **kwargs: Additional arguments to pass to score_func. Returns: mean_score (float): Mean score from bootstrap samples. ci (tuple): (lower bound, upper bound) as the confidence interval. \"\"\" scores = [] y_true = np . array ( y_true ) y_pred = np . array ( y_pred ) n = len ( y_true ) for i in range ( n_bootstraps ): # sample indices with replacement indices = np . random . choice ( n , n , replace = True ) sample_y_true = y_true [ indices ] sample_y_pred = y_pred [ indices ] score = score_func ( sample_y_true , sample_y_pred , ** kwargs ) scores . append ( score ) # Compute percentile bounds for the desired confidence level lower = np . percentile ( scores , 100 * ( alpha / 2 )) upper = np . percentile ( scores , 100 * ( 1 - alpha / 2 )) mean_score = np . mean ( scores ) return mean_score , ( lower , upper )","title":"bootstrap_metric"},{"location":"LabeLMaker/Evaluate/confidence_intervals.html#LabeLMaker.Evaluate.confidence_intervals.compute_bootstrap_confidence_intervals","text":"Computes performance metric values (accuracy, macro precision, recall, and F1) along with their bootstrap-based 95% confidence intervals. Parameters: y_true ( array - like ) \u2013 True labels. y_pred ( array - like ) \u2013 Predicted labels. n_bootstraps ( int , default: 1000 ) \u2013 Number of bootstrap iterations. alpha ( float , default: 0.05 ) \u2013 Significance level. Returns: results ( dict ) \u2013 Dictionary mapping each metric to its value, bootstrap mean, and 95% CI. Source code in LabeLMaker/Evaluate/confidence_intervals.py 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 def compute_bootstrap_confidence_intervals ( y_true , y_pred , n_bootstraps = 1000 , alpha = 0.05 ): \"\"\" Computes performance metric values (accuracy, macro precision, recall, and F1) along with their bootstrap-based 95% confidence intervals. Parameters: y_true (array-like): True labels. y_pred (array-like): Predicted labels. n_bootstraps (int): Number of bootstrap iterations. alpha (float): Significance level. Returns: results (dict): Dictionary mapping each metric to its value, bootstrap mean, and 95% CI. \"\"\" results = {} # Use macro-average for multiclass metrics. metrics = { \"Accuracy\" : accuracy_score , \"Precision\" : lambda yt , yp : precision_score ( yt , yp , average = \"macro\" , zero_division = 0 ), \"Recall\" : lambda yt , yp : recall_score ( yt , yp , average = \"macro\" , zero_division = 0 ), \"F1 Score\" : lambda yt , yp : f1_score ( yt , yp , average = \"macro\" , zero_division = 0 ), } # Compute values on full dataset and bootstrap estimates for metric_name , func in metrics . items (): full_value = func ( y_true , y_pred ) bs_mean , ci = bootstrap_metric ( y_true , y_pred , func , n_bootstraps = n_bootstraps , alpha = alpha ) results [ metric_name ] = { \"Value\" : full_value , \"Bootstrap Mean\" : bs_mean , \"95% CI\" : ci } return results","title":"compute_bootstrap_confidence_intervals"},{"location":"LabeLMaker/Evaluate/data_loader.html","text":"DataLoader Source code in LabeLMaker/Evaluate/data_loader.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 class DataLoader : def __init__ ( self , file : Optional [ str ] = None , dataframe : Optional [ pd . DataFrame ] = None ) -> None : \"\"\" Initializes the DataLoader with a file path or an existing DataFrame. Parameters: file (str, optional): Path to the CSV file to load. dataframe (pd.DataFrame, optional): An existing DataFrame. Raises: ValueError: If neither file nor dataframe is provided. \"\"\" if file is not None : self . df = self . load_csv_file ( file ) elif dataframe is not None : self . df = dataframe else : raise ValueError ( \"Either 'file' or 'dataframe' must be provided.\" ) def load_csv_file ( self , file : str ) -> pd . DataFrame : \"\"\" Loads a CSV file into a pandas DataFrame. Parameters: file (str): Path to the CSV file. Returns: pd.DataFrame: Loaded DataFrame. \"\"\" return pd . read_csv ( file , encoding = \"utf-8\" ) def preprocess_text_columns ( self , columns : List [ str ]) -> \"DataLoader\" : \"\"\" Preprocesses text columns by stripping whitespace and converting to lowercase. Parameters: columns (List[str]): List of column names to preprocess. Returns: DataLoader: Returns self for method chaining. \"\"\" for col in columns : self . df [ col ] = self . df [ col ] . astype ( str ) . str . strip () . str . lower () return self def drop_duplicates ( self , subset : List [ str ], keep : str = \"first\" ) -> \"DataLoader\" : \"\"\" Drops duplicate rows based on specified columns. Parameters: subset (List[str]): Columns to consider for identifying duplicates. keep (str, optional): Which duplicates to keep ('first', 'last', or False). Returns: DataLoader: Returns self for method chaining. \"\"\" self . df = self . df . drop_duplicates ( subset = subset , keep = keep ) return self __init__ ( file = None , dataframe = None ) Initializes the DataLoader with a file path or an existing DataFrame. Parameters: file ( str , default: None ) \u2013 Path to the CSV file to load. dataframe ( DataFrame , default: None ) \u2013 An existing DataFrame. Raises: ValueError \u2013 If neither file nor dataframe is provided. Source code in LabeLMaker/Evaluate/data_loader.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 def __init__ ( self , file : Optional [ str ] = None , dataframe : Optional [ pd . DataFrame ] = None ) -> None : \"\"\" Initializes the DataLoader with a file path or an existing DataFrame. Parameters: file (str, optional): Path to the CSV file to load. dataframe (pd.DataFrame, optional): An existing DataFrame. Raises: ValueError: If neither file nor dataframe is provided. \"\"\" if file is not None : self . df = self . load_csv_file ( file ) elif dataframe is not None : self . df = dataframe else : raise ValueError ( \"Either 'file' or 'dataframe' must be provided.\" ) drop_duplicates ( subset , keep = 'first' ) Drops duplicate rows based on specified columns. Parameters: subset ( List [ str ] ) \u2013 Columns to consider for identifying duplicates. keep ( str , default: 'first' ) \u2013 Which duplicates to keep ('first', 'last', or False). Returns: DataLoader ( DataLoader ) \u2013 Returns self for method chaining. Source code in LabeLMaker/Evaluate/data_loader.py 54 55 56 57 58 59 60 61 62 63 64 65 66 def drop_duplicates ( self , subset : List [ str ], keep : str = \"first\" ) -> \"DataLoader\" : \"\"\" Drops duplicate rows based on specified columns. Parameters: subset (List[str]): Columns to consider for identifying duplicates. keep (str, optional): Which duplicates to keep ('first', 'last', or False). Returns: DataLoader: Returns self for method chaining. \"\"\" self . df = self . df . drop_duplicates ( subset = subset , keep = keep ) return self load_csv_file ( file ) Loads a CSV file into a pandas DataFrame. Parameters: file ( str ) \u2013 Path to the CSV file. Returns: DataFrame \u2013 pd.DataFrame: Loaded DataFrame. Source code in LabeLMaker/Evaluate/data_loader.py 28 29 30 31 32 33 34 35 36 37 38 def load_csv_file ( self , file : str ) -> pd . DataFrame : \"\"\" Loads a CSV file into a pandas DataFrame. Parameters: file (str): Path to the CSV file. Returns: pd.DataFrame: Loaded DataFrame. \"\"\" return pd . read_csv ( file , encoding = \"utf-8\" ) preprocess_text_columns ( columns ) Preprocesses text columns by stripping whitespace and converting to lowercase. Parameters: columns ( List [ str ] ) \u2013 List of column names to preprocess. Returns: DataLoader ( DataLoader ) \u2013 Returns self for method chaining. Source code in LabeLMaker/Evaluate/data_loader.py 40 41 42 43 44 45 46 47 48 49 50 51 52 def preprocess_text_columns ( self , columns : List [ str ]) -> \"DataLoader\" : \"\"\" Preprocesses text columns by stripping whitespace and converting to lowercase. Parameters: columns (List[str]): List of column names to preprocess. Returns: DataLoader: Returns self for method chaining. \"\"\" for col in columns : self . df [ col ] = self . df [ col ] . astype ( str ) . str . strip () . str . lower () return self","title":"Data Loader"},{"location":"LabeLMaker/Evaluate/data_loader.html#LabeLMaker.Evaluate.data_loader.DataLoader","text":"Source code in LabeLMaker/Evaluate/data_loader.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 class DataLoader : def __init__ ( self , file : Optional [ str ] = None , dataframe : Optional [ pd . DataFrame ] = None ) -> None : \"\"\" Initializes the DataLoader with a file path or an existing DataFrame. Parameters: file (str, optional): Path to the CSV file to load. dataframe (pd.DataFrame, optional): An existing DataFrame. Raises: ValueError: If neither file nor dataframe is provided. \"\"\" if file is not None : self . df = self . load_csv_file ( file ) elif dataframe is not None : self . df = dataframe else : raise ValueError ( \"Either 'file' or 'dataframe' must be provided.\" ) def load_csv_file ( self , file : str ) -> pd . DataFrame : \"\"\" Loads a CSV file into a pandas DataFrame. Parameters: file (str): Path to the CSV file. Returns: pd.DataFrame: Loaded DataFrame. \"\"\" return pd . read_csv ( file , encoding = \"utf-8\" ) def preprocess_text_columns ( self , columns : List [ str ]) -> \"DataLoader\" : \"\"\" Preprocesses text columns by stripping whitespace and converting to lowercase. Parameters: columns (List[str]): List of column names to preprocess. Returns: DataLoader: Returns self for method chaining. \"\"\" for col in columns : self . df [ col ] = self . df [ col ] . astype ( str ) . str . strip () . str . lower () return self def drop_duplicates ( self , subset : List [ str ], keep : str = \"first\" ) -> \"DataLoader\" : \"\"\" Drops duplicate rows based on specified columns. Parameters: subset (List[str]): Columns to consider for identifying duplicates. keep (str, optional): Which duplicates to keep ('first', 'last', or False). Returns: DataLoader: Returns self for method chaining. \"\"\" self . df = self . df . drop_duplicates ( subset = subset , keep = keep ) return self","title":"DataLoader"},{"location":"LabeLMaker/Evaluate/data_loader.html#LabeLMaker.Evaluate.data_loader.DataLoader.__init__","text":"Initializes the DataLoader with a file path or an existing DataFrame. Parameters: file ( str , default: None ) \u2013 Path to the CSV file to load. dataframe ( DataFrame , default: None ) \u2013 An existing DataFrame. Raises: ValueError \u2013 If neither file nor dataframe is provided. Source code in LabeLMaker/Evaluate/data_loader.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 def __init__ ( self , file : Optional [ str ] = None , dataframe : Optional [ pd . DataFrame ] = None ) -> None : \"\"\" Initializes the DataLoader with a file path or an existing DataFrame. Parameters: file (str, optional): Path to the CSV file to load. dataframe (pd.DataFrame, optional): An existing DataFrame. Raises: ValueError: If neither file nor dataframe is provided. \"\"\" if file is not None : self . df = self . load_csv_file ( file ) elif dataframe is not None : self . df = dataframe else : raise ValueError ( \"Either 'file' or 'dataframe' must be provided.\" )","title":"__init__"},{"location":"LabeLMaker/Evaluate/data_loader.html#LabeLMaker.Evaluate.data_loader.DataLoader.drop_duplicates","text":"Drops duplicate rows based on specified columns. Parameters: subset ( List [ str ] ) \u2013 Columns to consider for identifying duplicates. keep ( str , default: 'first' ) \u2013 Which duplicates to keep ('first', 'last', or False). Returns: DataLoader ( DataLoader ) \u2013 Returns self for method chaining. Source code in LabeLMaker/Evaluate/data_loader.py 54 55 56 57 58 59 60 61 62 63 64 65 66 def drop_duplicates ( self , subset : List [ str ], keep : str = \"first\" ) -> \"DataLoader\" : \"\"\" Drops duplicate rows based on specified columns. Parameters: subset (List[str]): Columns to consider for identifying duplicates. keep (str, optional): Which duplicates to keep ('first', 'last', or False). Returns: DataLoader: Returns self for method chaining. \"\"\" self . df = self . df . drop_duplicates ( subset = subset , keep = keep ) return self","title":"drop_duplicates"},{"location":"LabeLMaker/Evaluate/data_loader.html#LabeLMaker.Evaluate.data_loader.DataLoader.load_csv_file","text":"Loads a CSV file into a pandas DataFrame. Parameters: file ( str ) \u2013 Path to the CSV file. Returns: DataFrame \u2013 pd.DataFrame: Loaded DataFrame. Source code in LabeLMaker/Evaluate/data_loader.py 28 29 30 31 32 33 34 35 36 37 38 def load_csv_file ( self , file : str ) -> pd . DataFrame : \"\"\" Loads a CSV file into a pandas DataFrame. Parameters: file (str): Path to the CSV file. Returns: pd.DataFrame: Loaded DataFrame. \"\"\" return pd . read_csv ( file , encoding = \"utf-8\" )","title":"load_csv_file"},{"location":"LabeLMaker/Evaluate/data_loader.html#LabeLMaker.Evaluate.data_loader.DataLoader.preprocess_text_columns","text":"Preprocesses text columns by stripping whitespace and converting to lowercase. Parameters: columns ( List [ str ] ) \u2013 List of column names to preprocess. Returns: DataLoader ( DataLoader ) \u2013 Returns self for method chaining. Source code in LabeLMaker/Evaluate/data_loader.py 40 41 42 43 44 45 46 47 48 49 50 51 52 def preprocess_text_columns ( self , columns : List [ str ]) -> \"DataLoader\" : \"\"\" Preprocesses text columns by stripping whitespace and converting to lowercase. Parameters: columns (List[str]): List of column names to preprocess. Returns: DataLoader: Returns self for method chaining. \"\"\" for col in columns : self . df [ col ] = self . df [ col ] . astype ( str ) . str . strip () . str . lower () return self","title":"preprocess_text_columns"},{"location":"LabeLMaker/Evaluate/evaluator.html","text":"The Evaluator class in Python calculates and displays evaluation metrics such as accuracy, precision, recall, F1 score, confusion matrix, and classification report for classification tasks. Evaluator This Python class Evaluator provides methods to calculate and display evaluation metrics for classification tasks, including precision, recall, F1 score, accuracy, confusion matrix, and classification report. Source code in LabeLMaker/Evaluate/evaluator.py 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 class Evaluator : \"\"\" This Python class `Evaluator` provides methods to calculate and display evaluation metrics for classification tasks, including precision, recall, F1 score, accuracy, confusion matrix, and classification report. \"\"\" def __init__ ( self , y_true : List , y_pred : List ) -> None : \"\"\" Initializes the Evaluator with true and predicted labels. Parameters: y_true (List): Ground truth labels. y_pred (List): Predicted labels. \"\"\" self . y_true = y_true self . y_pred = y_pred self . metrics : Dict [ str , Any ] = {} @staticmethod def _format_numeric ( value : Any ) -> Any : \"\"\" Format a numeric value to 5 significant figures. If the value is not numeric, it is returned unchanged. \"\"\" if isinstance ( value , ( int , float )): # Convert to float and then format. return float ( format ( value , \".4g\" )) return value @classmethod def _format_dict ( cls , d : Dict [ Any , Any ]) -> Dict [ Any , Any ]: \"\"\" Recursively format all numeric entries in a dictionary to 5 significant figures. \"\"\" formatted = {} for k , v in d . items (): if isinstance ( v , dict ): # Recursively process nested dictionaries. formatted [ k ] = cls . _format_dict ( v ) else : formatted [ k ] = cls . _format_numeric ( v ) return formatted def calculate_metrics ( self , average_options : Optional [ List [ str ]] = None ) -> Dict [ str , Any ]: \"\"\" Calculates evaluation metrics. Parameters: average_options (List[str], optional): Averaging methods (e.g., ['macro', 'weighted']). Returns: Dict[str, Any]: Dictionary of calculated metrics. \"\"\" if average_options is None : average_options = [ \"macro\" , \"weighted\" ] # Calculate and format accuracy. self . metrics [ \"Accuracy\" ] = self . _format_numeric ( accuracy_score ( self . y_true , self . y_pred )) # Calculate precision, recall, and f1 scores for each averaging option. for avg in average_options : self . metrics [ f \"Precision ( { avg } )\" ] = self . _format_numeric ( precision_score ( self . y_true , self . y_pred , average = avg , zero_division = 0 ) ) self . metrics [ f \"Recall ( { avg } )\" ] = self . _format_numeric ( recall_score ( self . y_true , self . y_pred , average = avg , zero_division = 0 ) ) self . metrics [ f \"F1 Score ( { avg } )\" ] = self . _format_numeric ( f1_score ( self . y_true , self . y_pred , average = avg , zero_division = 0 ) ) # Compute and save the confusion matrix without formatting. self . metrics [ \"Confusion Matrix\" ] = confusion_matrix ( self . y_true , self . y_pred ) # Compute and then recursively format the classification report. raw_report = classification_report ( self . y_true , self . y_pred , output_dict = True , zero_division = 0 ) self . metrics [ \"Classification Report\" ] = self . _format_dict ( raw_report ) return self . metrics def display_metrics ( self ) -> pd . DataFrame : \"\"\" Returns calculated metrics as a DataFrame. \"\"\" metrics_to_display = { k : v for k , v in self . metrics . items () if k not in [ \"Confusion Matrix\" , \"Classification Report\" ] } df = pd . DataFrame ( list ( metrics_to_display . items ()), columns = [ \"Metric\" , \"Value\" ]) return df def plot_confusion_matrix ( self , class_labels : Optional [ List [ str ]] = None ) -> plt . Figure : \"\"\" Plots the confusion matrix. Parameters: class_labels (List[str], optional): Labels for the classes. Raises: ValueError: If confusion matrix is not calculated. \"\"\" cm = self . metrics . get ( \"Confusion Matrix\" ) if cm is None : raise ValueError ( \"Confusion Matrix not calculated. Call calculate_metrics() first.\" ) if class_labels is None : class_labels = sorted ( set ( self . y_true ) | set ( self . y_pred )) fig , ax = plt . subplots ( figsize = ( 8 , 6 )) sns . heatmap ( cm , annot = True , fmt = \"d\" , cmap = \"Blues\" , xticklabels = class_labels , yticklabels = class_labels , ax = ax , ) ax . set_title ( \"Confusion Matrix\" ) ax . set_xlabel ( \"Predicted Label\" ) ax . set_ylabel ( \"True Label\" ) plt . close ( fig ) # Close the figure to prevent it from displaying automatically return fig __init__ ( y_true , y_pred ) Initializes the Evaluator with true and predicted labels. Parameters: y_true (List): Ground truth labels. y_pred (List): Predicted labels. Source code in LabeLMaker/Evaluate/evaluator.py 27 28 29 30 31 32 33 34 35 36 def __init__ ( self , y_true : List , y_pred : List ) -> None : \"\"\" Initializes the Evaluator with true and predicted labels. Parameters: y_true (List): Ground truth labels. y_pred (List): Predicted labels. \"\"\" self . y_true = y_true self . y_pred = y_pred self . metrics : Dict [ str , Any ] = {} calculate_metrics ( average_options = None ) Calculates evaluation metrics. Parameters: average_options (List[str], optional): Averaging methods (e.g., ['macro', 'weighted']). Returns: Dict[str, Any]: Dictionary of calculated metrics. Source code in LabeLMaker/Evaluate/evaluator.py 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 def calculate_metrics ( self , average_options : Optional [ List [ str ]] = None ) -> Dict [ str , Any ]: \"\"\" Calculates evaluation metrics. Parameters: average_options (List[str], optional): Averaging methods (e.g., ['macro', 'weighted']). Returns: Dict[str, Any]: Dictionary of calculated metrics. \"\"\" if average_options is None : average_options = [ \"macro\" , \"weighted\" ] # Calculate and format accuracy. self . metrics [ \"Accuracy\" ] = self . _format_numeric ( accuracy_score ( self . y_true , self . y_pred )) # Calculate precision, recall, and f1 scores for each averaging option. for avg in average_options : self . metrics [ f \"Precision ( { avg } )\" ] = self . _format_numeric ( precision_score ( self . y_true , self . y_pred , average = avg , zero_division = 0 ) ) self . metrics [ f \"Recall ( { avg } )\" ] = self . _format_numeric ( recall_score ( self . y_true , self . y_pred , average = avg , zero_division = 0 ) ) self . metrics [ f \"F1 Score ( { avg } )\" ] = self . _format_numeric ( f1_score ( self . y_true , self . y_pred , average = avg , zero_division = 0 ) ) # Compute and save the confusion matrix without formatting. self . metrics [ \"Confusion Matrix\" ] = confusion_matrix ( self . y_true , self . y_pred ) # Compute and then recursively format the classification report. raw_report = classification_report ( self . y_true , self . y_pred , output_dict = True , zero_division = 0 ) self . metrics [ \"Classification Report\" ] = self . _format_dict ( raw_report ) return self . metrics display_metrics () Returns calculated metrics as a DataFrame. Source code in LabeLMaker/Evaluate/evaluator.py 101 102 103 104 105 106 107 108 109 110 111 def display_metrics ( self ) -> pd . DataFrame : \"\"\" Returns calculated metrics as a DataFrame. \"\"\" metrics_to_display = { k : v for k , v in self . metrics . items () if k not in [ \"Confusion Matrix\" , \"Classification Report\" ] } df = pd . DataFrame ( list ( metrics_to_display . items ()), columns = [ \"Metric\" , \"Value\" ]) return df plot_confusion_matrix ( class_labels = None ) Plots the confusion matrix. Parameters: class_labels (List[str], optional): Labels for the classes. Raises: ValueError: If confusion matrix is not calculated. Source code in LabeLMaker/Evaluate/evaluator.py 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 def plot_confusion_matrix ( self , class_labels : Optional [ List [ str ]] = None ) -> plt . Figure : \"\"\" Plots the confusion matrix. Parameters: class_labels (List[str], optional): Labels for the classes. Raises: ValueError: If confusion matrix is not calculated. \"\"\" cm = self . metrics . get ( \"Confusion Matrix\" ) if cm is None : raise ValueError ( \"Confusion Matrix not calculated. Call calculate_metrics() first.\" ) if class_labels is None : class_labels = sorted ( set ( self . y_true ) | set ( self . y_pred )) fig , ax = plt . subplots ( figsize = ( 8 , 6 )) sns . heatmap ( cm , annot = True , fmt = \"d\" , cmap = \"Blues\" , xticklabels = class_labels , yticklabels = class_labels , ax = ax , ) ax . set_title ( \"Confusion Matrix\" ) ax . set_xlabel ( \"Predicted Label\" ) ax . set_ylabel ( \"True Label\" ) plt . close ( fig ) # Close the figure to prevent it from displaying automatically return fig","title":"Evaluator"},{"location":"LabeLMaker/Evaluate/evaluator.html#LabeLMaker.Evaluate.evaluator.Evaluator","text":"This Python class Evaluator provides methods to calculate and display evaluation metrics for classification tasks, including precision, recall, F1 score, accuracy, confusion matrix, and classification report. Source code in LabeLMaker/Evaluate/evaluator.py 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 class Evaluator : \"\"\" This Python class `Evaluator` provides methods to calculate and display evaluation metrics for classification tasks, including precision, recall, F1 score, accuracy, confusion matrix, and classification report. \"\"\" def __init__ ( self , y_true : List , y_pred : List ) -> None : \"\"\" Initializes the Evaluator with true and predicted labels. Parameters: y_true (List): Ground truth labels. y_pred (List): Predicted labels. \"\"\" self . y_true = y_true self . y_pred = y_pred self . metrics : Dict [ str , Any ] = {} @staticmethod def _format_numeric ( value : Any ) -> Any : \"\"\" Format a numeric value to 5 significant figures. If the value is not numeric, it is returned unchanged. \"\"\" if isinstance ( value , ( int , float )): # Convert to float and then format. return float ( format ( value , \".4g\" )) return value @classmethod def _format_dict ( cls , d : Dict [ Any , Any ]) -> Dict [ Any , Any ]: \"\"\" Recursively format all numeric entries in a dictionary to 5 significant figures. \"\"\" formatted = {} for k , v in d . items (): if isinstance ( v , dict ): # Recursively process nested dictionaries. formatted [ k ] = cls . _format_dict ( v ) else : formatted [ k ] = cls . _format_numeric ( v ) return formatted def calculate_metrics ( self , average_options : Optional [ List [ str ]] = None ) -> Dict [ str , Any ]: \"\"\" Calculates evaluation metrics. Parameters: average_options (List[str], optional): Averaging methods (e.g., ['macro', 'weighted']). Returns: Dict[str, Any]: Dictionary of calculated metrics. \"\"\" if average_options is None : average_options = [ \"macro\" , \"weighted\" ] # Calculate and format accuracy. self . metrics [ \"Accuracy\" ] = self . _format_numeric ( accuracy_score ( self . y_true , self . y_pred )) # Calculate precision, recall, and f1 scores for each averaging option. for avg in average_options : self . metrics [ f \"Precision ( { avg } )\" ] = self . _format_numeric ( precision_score ( self . y_true , self . y_pred , average = avg , zero_division = 0 ) ) self . metrics [ f \"Recall ( { avg } )\" ] = self . _format_numeric ( recall_score ( self . y_true , self . y_pred , average = avg , zero_division = 0 ) ) self . metrics [ f \"F1 Score ( { avg } )\" ] = self . _format_numeric ( f1_score ( self . y_true , self . y_pred , average = avg , zero_division = 0 ) ) # Compute and save the confusion matrix without formatting. self . metrics [ \"Confusion Matrix\" ] = confusion_matrix ( self . y_true , self . y_pred ) # Compute and then recursively format the classification report. raw_report = classification_report ( self . y_true , self . y_pred , output_dict = True , zero_division = 0 ) self . metrics [ \"Classification Report\" ] = self . _format_dict ( raw_report ) return self . metrics def display_metrics ( self ) -> pd . DataFrame : \"\"\" Returns calculated metrics as a DataFrame. \"\"\" metrics_to_display = { k : v for k , v in self . metrics . items () if k not in [ \"Confusion Matrix\" , \"Classification Report\" ] } df = pd . DataFrame ( list ( metrics_to_display . items ()), columns = [ \"Metric\" , \"Value\" ]) return df def plot_confusion_matrix ( self , class_labels : Optional [ List [ str ]] = None ) -> plt . Figure : \"\"\" Plots the confusion matrix. Parameters: class_labels (List[str], optional): Labels for the classes. Raises: ValueError: If confusion matrix is not calculated. \"\"\" cm = self . metrics . get ( \"Confusion Matrix\" ) if cm is None : raise ValueError ( \"Confusion Matrix not calculated. Call calculate_metrics() first.\" ) if class_labels is None : class_labels = sorted ( set ( self . y_true ) | set ( self . y_pred )) fig , ax = plt . subplots ( figsize = ( 8 , 6 )) sns . heatmap ( cm , annot = True , fmt = \"d\" , cmap = \"Blues\" , xticklabels = class_labels , yticklabels = class_labels , ax = ax , ) ax . set_title ( \"Confusion Matrix\" ) ax . set_xlabel ( \"Predicted Label\" ) ax . set_ylabel ( \"True Label\" ) plt . close ( fig ) # Close the figure to prevent it from displaying automatically return fig","title":"Evaluator"},{"location":"LabeLMaker/Evaluate/evaluator.html#LabeLMaker.Evaluate.evaluator.Evaluator.__init__","text":"Initializes the Evaluator with true and predicted labels. Parameters: y_true (List): Ground truth labels. y_pred (List): Predicted labels. Source code in LabeLMaker/Evaluate/evaluator.py 27 28 29 30 31 32 33 34 35 36 def __init__ ( self , y_true : List , y_pred : List ) -> None : \"\"\" Initializes the Evaluator with true and predicted labels. Parameters: y_true (List): Ground truth labels. y_pred (List): Predicted labels. \"\"\" self . y_true = y_true self . y_pred = y_pred self . metrics : Dict [ str , Any ] = {}","title":"__init__"},{"location":"LabeLMaker/Evaluate/evaluator.html#LabeLMaker.Evaluate.evaluator.Evaluator.calculate_metrics","text":"Calculates evaluation metrics. Parameters: average_options (List[str], optional): Averaging methods (e.g., ['macro', 'weighted']). Returns: Dict[str, Any]: Dictionary of calculated metrics. Source code in LabeLMaker/Evaluate/evaluator.py 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 def calculate_metrics ( self , average_options : Optional [ List [ str ]] = None ) -> Dict [ str , Any ]: \"\"\" Calculates evaluation metrics. Parameters: average_options (List[str], optional): Averaging methods (e.g., ['macro', 'weighted']). Returns: Dict[str, Any]: Dictionary of calculated metrics. \"\"\" if average_options is None : average_options = [ \"macro\" , \"weighted\" ] # Calculate and format accuracy. self . metrics [ \"Accuracy\" ] = self . _format_numeric ( accuracy_score ( self . y_true , self . y_pred )) # Calculate precision, recall, and f1 scores for each averaging option. for avg in average_options : self . metrics [ f \"Precision ( { avg } )\" ] = self . _format_numeric ( precision_score ( self . y_true , self . y_pred , average = avg , zero_division = 0 ) ) self . metrics [ f \"Recall ( { avg } )\" ] = self . _format_numeric ( recall_score ( self . y_true , self . y_pred , average = avg , zero_division = 0 ) ) self . metrics [ f \"F1 Score ( { avg } )\" ] = self . _format_numeric ( f1_score ( self . y_true , self . y_pred , average = avg , zero_division = 0 ) ) # Compute and save the confusion matrix without formatting. self . metrics [ \"Confusion Matrix\" ] = confusion_matrix ( self . y_true , self . y_pred ) # Compute and then recursively format the classification report. raw_report = classification_report ( self . y_true , self . y_pred , output_dict = True , zero_division = 0 ) self . metrics [ \"Classification Report\" ] = self . _format_dict ( raw_report ) return self . metrics","title":"calculate_metrics"},{"location":"LabeLMaker/Evaluate/evaluator.html#LabeLMaker.Evaluate.evaluator.Evaluator.display_metrics","text":"Returns calculated metrics as a DataFrame. Source code in LabeLMaker/Evaluate/evaluator.py 101 102 103 104 105 106 107 108 109 110 111 def display_metrics ( self ) -> pd . DataFrame : \"\"\" Returns calculated metrics as a DataFrame. \"\"\" metrics_to_display = { k : v for k , v in self . metrics . items () if k not in [ \"Confusion Matrix\" , \"Classification Report\" ] } df = pd . DataFrame ( list ( metrics_to_display . items ()), columns = [ \"Metric\" , \"Value\" ]) return df","title":"display_metrics"},{"location":"LabeLMaker/Evaluate/evaluator.html#LabeLMaker.Evaluate.evaluator.Evaluator.plot_confusion_matrix","text":"Plots the confusion matrix. Parameters: class_labels (List[str], optional): Labels for the classes. Raises: ValueError: If confusion matrix is not calculated. Source code in LabeLMaker/Evaluate/evaluator.py 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 def plot_confusion_matrix ( self , class_labels : Optional [ List [ str ]] = None ) -> plt . Figure : \"\"\" Plots the confusion matrix. Parameters: class_labels (List[str], optional): Labels for the classes. Raises: ValueError: If confusion matrix is not calculated. \"\"\" cm = self . metrics . get ( \"Confusion Matrix\" ) if cm is None : raise ValueError ( \"Confusion Matrix not calculated. Call calculate_metrics() first.\" ) if class_labels is None : class_labels = sorted ( set ( self . y_true ) | set ( self . y_pred )) fig , ax = plt . subplots ( figsize = ( 8 , 6 )) sns . heatmap ( cm , annot = True , fmt = \"d\" , cmap = \"Blues\" , xticklabels = class_labels , yticklabels = class_labels , ax = ax , ) ax . set_title ( \"Confusion Matrix\" ) ax . set_xlabel ( \"Predicted Label\" ) ax . set_ylabel ( \"True Label\" ) plt . close ( fig ) # Close the figure to prevent it from displaying automatically return fig","title":"plot_confusion_matrix"},{"location":"LabeLMaker/utils/category.html","text":"CategoryManager Source code in LabeLMaker/utils/category.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 class CategoryManager : @staticmethod def define_categories ( ui_helper , key_prefix , unique_values_str = None , get_file_examples = False ): \"\"\" ui_helper: a helper object wrapping Streamlit calls so that category logic stays separate. \"\"\" ui_helper . markdown ( \"---\" ) if unique_values_str : unique_values = [ val . strip () for val in unique_values_str . split ( \",\" ) if val . strip () . lower () not in [ \"nan\" , \"none\" ] ] num_categories = len ( unique_values ) else : num_categories = int ( ui_helper . number_input ( \"Enter the number of categories\" , min_value = 2 , value = 2 , step = 1 , key = f \" { key_prefix } _categories\" , ) ) unique_values = None categories_dict = {} all_examples = [] for i in range ( num_categories ): with ui_helper . expander ( f \"Category { i + 1 } \" , expanded = True ): if unique_values : category_value = ui_helper . text_input ( f \"Enter label for category { i + 1 } \" , value = unique_values [ i ] . title (), key = f \" { key_prefix } _text_input_ { i + 1 } \" , ) else : category_value = ui_helper . text_input ( f \"Enter label for category { i + 1 } \" , key = f \" { key_prefix } _text_input_ { i + 1 } \" ) category_description = ui_helper . text_input ( f \"Enter description for category { i + 1 } (optional but recommended)\" , \"\" , key = f \" { key_prefix } _desc_input_ { i + 1 } \" , ) categories_dict [ category_value . lower ()] = category_description or \"\" if get_file_examples : uploaded_files = ui_helper . file_uploader ( \"Upload example files for this category\" , type = [ \"docx\" , \"pdf\" ], accept_multiple_files = True , key = f \"example_ { i } \" , ) if uploaded_files : fm = FileManager () filenames , texts = fm . process_multiple_files ( uploaded_files ) if texts : examples_for_category = [ Example ( text_with_label = text , label = category_value ) for text in texts ] all_examples . extend ( examples_for_category ) return categories_dict , all_examples @staticmethod def create_request ( index_list , df_text , categories_dict , examples = None ): categories = [ Categories ( name = name , description = desc ) for name , desc in categories_dict . items () ] # Convert examples from tuple to Example objects if necessary. if examples : new_examples = [] for ex in examples : # If ex is already an Example instance, leave it as is. if isinstance ( ex , Example ): new_examples . append ( ex ) # Otherwise, assume it's a tuple (text, label) and convert it. elif isinstance ( ex , ( list , tuple )) and len ( ex ) == 2 : new_examples . append ( Example ( text_with_label = ex [ 0 ], label = ex [ 1 ])) else : raise ValueError ( \"Example must be an Example object or a tuple of (text, label).\" ) examples = new_examples cat_req = CategorizationRequest ( unique_ids = index_list , text_to_label = df_text , categories = categories , examples = examples ) return cat_req define_categories ( ui_helper , key_prefix , unique_values_str = None , get_file_examples = False ) staticmethod ui_helper: a helper object wrapping Streamlit calls so that category logic stays separate. Source code in LabeLMaker/utils/category.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 @staticmethod def define_categories ( ui_helper , key_prefix , unique_values_str = None , get_file_examples = False ): \"\"\" ui_helper: a helper object wrapping Streamlit calls so that category logic stays separate. \"\"\" ui_helper . markdown ( \"---\" ) if unique_values_str : unique_values = [ val . strip () for val in unique_values_str . split ( \",\" ) if val . strip () . lower () not in [ \"nan\" , \"none\" ] ] num_categories = len ( unique_values ) else : num_categories = int ( ui_helper . number_input ( \"Enter the number of categories\" , min_value = 2 , value = 2 , step = 1 , key = f \" { key_prefix } _categories\" , ) ) unique_values = None categories_dict = {} all_examples = [] for i in range ( num_categories ): with ui_helper . expander ( f \"Category { i + 1 } \" , expanded = True ): if unique_values : category_value = ui_helper . text_input ( f \"Enter label for category { i + 1 } \" , value = unique_values [ i ] . title (), key = f \" { key_prefix } _text_input_ { i + 1 } \" , ) else : category_value = ui_helper . text_input ( f \"Enter label for category { i + 1 } \" , key = f \" { key_prefix } _text_input_ { i + 1 } \" ) category_description = ui_helper . text_input ( f \"Enter description for category { i + 1 } (optional but recommended)\" , \"\" , key = f \" { key_prefix } _desc_input_ { i + 1 } \" , ) categories_dict [ category_value . lower ()] = category_description or \"\" if get_file_examples : uploaded_files = ui_helper . file_uploader ( \"Upload example files for this category\" , type = [ \"docx\" , \"pdf\" ], accept_multiple_files = True , key = f \"example_ { i } \" , ) if uploaded_files : fm = FileManager () filenames , texts = fm . process_multiple_files ( uploaded_files ) if texts : examples_for_category = [ Example ( text_with_label = text , label = category_value ) for text in texts ] all_examples . extend ( examples_for_category ) return categories_dict , all_examples","title":"Category"},{"location":"LabeLMaker/utils/category.html#LabeLMaker.utils.category.CategoryManager","text":"Source code in LabeLMaker/utils/category.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 class CategoryManager : @staticmethod def define_categories ( ui_helper , key_prefix , unique_values_str = None , get_file_examples = False ): \"\"\" ui_helper: a helper object wrapping Streamlit calls so that category logic stays separate. \"\"\" ui_helper . markdown ( \"---\" ) if unique_values_str : unique_values = [ val . strip () for val in unique_values_str . split ( \",\" ) if val . strip () . lower () not in [ \"nan\" , \"none\" ] ] num_categories = len ( unique_values ) else : num_categories = int ( ui_helper . number_input ( \"Enter the number of categories\" , min_value = 2 , value = 2 , step = 1 , key = f \" { key_prefix } _categories\" , ) ) unique_values = None categories_dict = {} all_examples = [] for i in range ( num_categories ): with ui_helper . expander ( f \"Category { i + 1 } \" , expanded = True ): if unique_values : category_value = ui_helper . text_input ( f \"Enter label for category { i + 1 } \" , value = unique_values [ i ] . title (), key = f \" { key_prefix } _text_input_ { i + 1 } \" , ) else : category_value = ui_helper . text_input ( f \"Enter label for category { i + 1 } \" , key = f \" { key_prefix } _text_input_ { i + 1 } \" ) category_description = ui_helper . text_input ( f \"Enter description for category { i + 1 } (optional but recommended)\" , \"\" , key = f \" { key_prefix } _desc_input_ { i + 1 } \" , ) categories_dict [ category_value . lower ()] = category_description or \"\" if get_file_examples : uploaded_files = ui_helper . file_uploader ( \"Upload example files for this category\" , type = [ \"docx\" , \"pdf\" ], accept_multiple_files = True , key = f \"example_ { i } \" , ) if uploaded_files : fm = FileManager () filenames , texts = fm . process_multiple_files ( uploaded_files ) if texts : examples_for_category = [ Example ( text_with_label = text , label = category_value ) for text in texts ] all_examples . extend ( examples_for_category ) return categories_dict , all_examples @staticmethod def create_request ( index_list , df_text , categories_dict , examples = None ): categories = [ Categories ( name = name , description = desc ) for name , desc in categories_dict . items () ] # Convert examples from tuple to Example objects if necessary. if examples : new_examples = [] for ex in examples : # If ex is already an Example instance, leave it as is. if isinstance ( ex , Example ): new_examples . append ( ex ) # Otherwise, assume it's a tuple (text, label) and convert it. elif isinstance ( ex , ( list , tuple )) and len ( ex ) == 2 : new_examples . append ( Example ( text_with_label = ex [ 0 ], label = ex [ 1 ])) else : raise ValueError ( \"Example must be an Example object or a tuple of (text, label).\" ) examples = new_examples cat_req = CategorizationRequest ( unique_ids = index_list , text_to_label = df_text , categories = categories , examples = examples ) return cat_req","title":"CategoryManager"},{"location":"LabeLMaker/utils/category.html#LabeLMaker.utils.category.CategoryManager.define_categories","text":"ui_helper: a helper object wrapping Streamlit calls so that category logic stays separate. Source code in LabeLMaker/utils/category.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 @staticmethod def define_categories ( ui_helper , key_prefix , unique_values_str = None , get_file_examples = False ): \"\"\" ui_helper: a helper object wrapping Streamlit calls so that category logic stays separate. \"\"\" ui_helper . markdown ( \"---\" ) if unique_values_str : unique_values = [ val . strip () for val in unique_values_str . split ( \",\" ) if val . strip () . lower () not in [ \"nan\" , \"none\" ] ] num_categories = len ( unique_values ) else : num_categories = int ( ui_helper . number_input ( \"Enter the number of categories\" , min_value = 2 , value = 2 , step = 1 , key = f \" { key_prefix } _categories\" , ) ) unique_values = None categories_dict = {} all_examples = [] for i in range ( num_categories ): with ui_helper . expander ( f \"Category { i + 1 } \" , expanded = True ): if unique_values : category_value = ui_helper . text_input ( f \"Enter label for category { i + 1 } \" , value = unique_values [ i ] . title (), key = f \" { key_prefix } _text_input_ { i + 1 } \" , ) else : category_value = ui_helper . text_input ( f \"Enter label for category { i + 1 } \" , key = f \" { key_prefix } _text_input_ { i + 1 } \" ) category_description = ui_helper . text_input ( f \"Enter description for category { i + 1 } (optional but recommended)\" , \"\" , key = f \" { key_prefix } _desc_input_ { i + 1 } \" , ) categories_dict [ category_value . lower ()] = category_description or \"\" if get_file_examples : uploaded_files = ui_helper . file_uploader ( \"Upload example files for this category\" , type = [ \"docx\" , \"pdf\" ], accept_multiple_files = True , key = f \"example_ { i } \" , ) if uploaded_files : fm = FileManager () filenames , texts = fm . process_multiple_files ( uploaded_files ) if texts : examples_for_category = [ Example ( text_with_label = text , label = category_value ) for text in texts ] all_examples . extend ( examples_for_category ) return categories_dict , all_examples","title":"define_categories"},{"location":"LabeLMaker/utils/class_balance.html","text":"ClassBalance Source code in LabeLMaker/utils/class_balance.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 class ClassBalance : def __init__ ( self , df : pd . DataFrame , class_column : str ) -> None : \"\"\" Initializes the ClassBalance with a DataFrame and class column name. Parameters: df (pd.DataFrame): DataFrame containing the class column. class_column (str): Name of the class column. \"\"\" self . df = df self . class_column = class_column self . balance_df : Optional [ pd . DataFrame ] = None def compute_balance ( self ) -> pd . DataFrame : \"\"\" Computes class balance. Returns: pd.DataFrame: DataFrame containing counts and percentages of each class. \"\"\" counts = self . df [ self . class_column ] . value_counts () percentages = ( counts / len ( self . df )) * 100 self . balance_df = pd . DataFrame ({ \"Count\" : counts , \"Percentage\" : percentages }) return self . balance_df def display_balance ( self ) -> None : \"\"\" Displays class balance. Raises: ValueError: If balance data has not been computed. \"\"\" if self . balance_df is None : raise ValueError ( \"Balance data not computed. Call compute_balance() first.\" ) print ( f \"=== Class Balance for ' { self . class_column } ' ===\" ) print ( self . balance_df ) __init__ ( df , class_column ) Initializes the ClassBalance with a DataFrame and class column name. Parameters: df ( DataFrame ) \u2013 DataFrame containing the class column. class_column ( str ) \u2013 Name of the class column. Source code in LabeLMaker/utils/class_balance.py 7 8 9 10 11 12 13 14 15 16 17 def __init__ ( self , df : pd . DataFrame , class_column : str ) -> None : \"\"\" Initializes the ClassBalance with a DataFrame and class column name. Parameters: df (pd.DataFrame): DataFrame containing the class column. class_column (str): Name of the class column. \"\"\" self . df = df self . class_column = class_column self . balance_df : Optional [ pd . DataFrame ] = None compute_balance () Computes class balance. Returns: DataFrame \u2013 pd.DataFrame: DataFrame containing counts and percentages of each class. Source code in LabeLMaker/utils/class_balance.py 19 20 21 22 23 24 25 26 27 28 29 def compute_balance ( self ) -> pd . DataFrame : \"\"\" Computes class balance. Returns: pd.DataFrame: DataFrame containing counts and percentages of each class. \"\"\" counts = self . df [ self . class_column ] . value_counts () percentages = ( counts / len ( self . df )) * 100 self . balance_df = pd . DataFrame ({ \"Count\" : counts , \"Percentage\" : percentages }) return self . balance_df display_balance () Displays class balance. Raises: ValueError \u2013 If balance data has not been computed. Source code in LabeLMaker/utils/class_balance.py 31 32 33 34 35 36 37 38 39 40 41 def display_balance ( self ) -> None : \"\"\" Displays class balance. Raises: ValueError: If balance data has not been computed. \"\"\" if self . balance_df is None : raise ValueError ( \"Balance data not computed. Call compute_balance() first.\" ) print ( f \"=== Class Balance for ' { self . class_column } ' ===\" ) print ( self . balance_df )","title":"Class Balance"},{"location":"LabeLMaker/utils/class_balance.html#LabeLMaker.utils.class_balance.ClassBalance","text":"Source code in LabeLMaker/utils/class_balance.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 class ClassBalance : def __init__ ( self , df : pd . DataFrame , class_column : str ) -> None : \"\"\" Initializes the ClassBalance with a DataFrame and class column name. Parameters: df (pd.DataFrame): DataFrame containing the class column. class_column (str): Name of the class column. \"\"\" self . df = df self . class_column = class_column self . balance_df : Optional [ pd . DataFrame ] = None def compute_balance ( self ) -> pd . DataFrame : \"\"\" Computes class balance. Returns: pd.DataFrame: DataFrame containing counts and percentages of each class. \"\"\" counts = self . df [ self . class_column ] . value_counts () percentages = ( counts / len ( self . df )) * 100 self . balance_df = pd . DataFrame ({ \"Count\" : counts , \"Percentage\" : percentages }) return self . balance_df def display_balance ( self ) -> None : \"\"\" Displays class balance. Raises: ValueError: If balance data has not been computed. \"\"\" if self . balance_df is None : raise ValueError ( \"Balance data not computed. Call compute_balance() first.\" ) print ( f \"=== Class Balance for ' { self . class_column } ' ===\" ) print ( self . balance_df )","title":"ClassBalance"},{"location":"LabeLMaker/utils/class_balance.html#LabeLMaker.utils.class_balance.ClassBalance.__init__","text":"Initializes the ClassBalance with a DataFrame and class column name. Parameters: df ( DataFrame ) \u2013 DataFrame containing the class column. class_column ( str ) \u2013 Name of the class column. Source code in LabeLMaker/utils/class_balance.py 7 8 9 10 11 12 13 14 15 16 17 def __init__ ( self , df : pd . DataFrame , class_column : str ) -> None : \"\"\" Initializes the ClassBalance with a DataFrame and class column name. Parameters: df (pd.DataFrame): DataFrame containing the class column. class_column (str): Name of the class column. \"\"\" self . df = df self . class_column = class_column self . balance_df : Optional [ pd . DataFrame ] = None","title":"__init__"},{"location":"LabeLMaker/utils/class_balance.html#LabeLMaker.utils.class_balance.ClassBalance.compute_balance","text":"Computes class balance. Returns: DataFrame \u2013 pd.DataFrame: DataFrame containing counts and percentages of each class. Source code in LabeLMaker/utils/class_balance.py 19 20 21 22 23 24 25 26 27 28 29 def compute_balance ( self ) -> pd . DataFrame : \"\"\" Computes class balance. Returns: pd.DataFrame: DataFrame containing counts and percentages of each class. \"\"\" counts = self . df [ self . class_column ] . value_counts () percentages = ( counts / len ( self . df )) * 100 self . balance_df = pd . DataFrame ({ \"Count\" : counts , \"Percentage\" : percentages }) return self . balance_df","title":"compute_balance"},{"location":"LabeLMaker/utils/class_balance.html#LabeLMaker.utils.class_balance.ClassBalance.display_balance","text":"Displays class balance. Raises: ValueError \u2013 If balance data has not been computed. Source code in LabeLMaker/utils/class_balance.py 31 32 33 34 35 36 37 38 39 40 41 def display_balance ( self ) -> None : \"\"\" Displays class balance. Raises: ValueError: If balance data has not been computed. \"\"\" if self . balance_df is None : raise ValueError ( \"Balance data not computed. Call compute_balance() first.\" ) print ( f \"=== Class Balance for ' { self . class_column } ' ===\" ) print ( self . balance_df )","title":"display_balance"},{"location":"LabeLMaker/utils/file_manager.html","text":"","title":"File Manager"},{"location":"LabeLMaker/utils/normalize_text.html","text":"","title":"Normalize Text"},{"location":"LabeLMaker/utils/page_renderer.html","text":"UIHelper A thin wrapper around Streamlit calls so the core logic isn\u2019t tied directly to st.xxx calls. Source code in LabeLMaker/utils/page_renderer.py 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 class UIHelper : \"\"\" A thin wrapper around Streamlit calls so the core logic isn\u2019t tied directly to st.xxx calls. \"\"\" def __init__ ( self ): self . session_state = st . session_state def balloons ( self ): st . balloons () def button ( self , label , key = None ): return st . button ( label , key = key ) def checkbox ( self , label , help , key = None ): return st . checkbox ( label = label , help = help , key = key ) def download_button ( self , label , data , file_name , mime , ** kwargs ): return st . download_button ( label , data , file_name , mime , ** kwargs ) def error ( self , text ): st . error ( text ) def expander ( self , label , expanded = True ): return st . expander ( label , expanded = expanded ) def file_uploader ( self , label , type , accept_multiple_files , key ): return st . file_uploader ( label , type = type , accept_multiple_files = accept_multiple_files , key = key ) def header ( self , text ): return st . header ( text ) def info ( self , text ): st . info ( text ) def markdown ( self , text ): st . markdown ( text ) def multiselect ( self , label , options , ** kwargs ): return st . multiselect ( label , options , ** kwargs ) def number_input ( self , label , ** kwargs ): return st . number_input ( label , ** kwargs ) def pyplot ( self , figure ): st . pyplot ( figure ) def radio ( self , label , options , ** kwargs ): return st . radio ( label , options , ** kwargs ) def rerun ( self ): st . rerun ( scope = \"app\" ) def selectbox ( self , label , options , ** kwargs ): return st . selectbox ( label , options , ** kwargs ) def spinner ( self , text ): return st . spinner ( text ) def subheader ( self , text ): return st . subheader ( text ) def success ( self , text ): st . success ( text ) def text_input ( self , label , value = \"\" , key = None ): return st . text_input ( label , value = value , key = key ) def warning ( self , text ): st . warning ( text , icon = \"\ud83d\udea8\" ) def write ( self , text ): st . write ( text )","title":"Page Renderer"},{"location":"LabeLMaker/utils/page_renderer.html#LabeLMaker.utils.page_renderer.UIHelper","text":"A thin wrapper around Streamlit calls so the core logic isn\u2019t tied directly to st.xxx calls. Source code in LabeLMaker/utils/page_renderer.py 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 class UIHelper : \"\"\" A thin wrapper around Streamlit calls so the core logic isn\u2019t tied directly to st.xxx calls. \"\"\" def __init__ ( self ): self . session_state = st . session_state def balloons ( self ): st . balloons () def button ( self , label , key = None ): return st . button ( label , key = key ) def checkbox ( self , label , help , key = None ): return st . checkbox ( label = label , help = help , key = key ) def download_button ( self , label , data , file_name , mime , ** kwargs ): return st . download_button ( label , data , file_name , mime , ** kwargs ) def error ( self , text ): st . error ( text ) def expander ( self , label , expanded = True ): return st . expander ( label , expanded = expanded ) def file_uploader ( self , label , type , accept_multiple_files , key ): return st . file_uploader ( label , type = type , accept_multiple_files = accept_multiple_files , key = key ) def header ( self , text ): return st . header ( text ) def info ( self , text ): st . info ( text ) def markdown ( self , text ): st . markdown ( text ) def multiselect ( self , label , options , ** kwargs ): return st . multiselect ( label , options , ** kwargs ) def number_input ( self , label , ** kwargs ): return st . number_input ( label , ** kwargs ) def pyplot ( self , figure ): st . pyplot ( figure ) def radio ( self , label , options , ** kwargs ): return st . radio ( label , options , ** kwargs ) def rerun ( self ): st . rerun ( scope = \"app\" ) def selectbox ( self , label , options , ** kwargs ): return st . selectbox ( label , options , ** kwargs ) def spinner ( self , text ): return st . spinner ( text ) def subheader ( self , text ): return st . subheader ( text ) def success ( self , text ): st . success ( text ) def text_input ( self , label , value = \"\" , key = None ): return st . text_input ( label , value = value , key = key ) def warning ( self , text ): st . warning ( text , icon = \"\ud83d\udea8\" ) def write ( self , text ): st . write ( text )","title":"UIHelper"},{"location":"LabeLMaker_config/config.html","text":"Configuration Module containing hardcoded varaibles. Config is a class for easy calling Config Configuration class for the Generative Categorizer application. This class centralizes all the configurable settings and file paths used throughout the application, and it is automatically included in the mkdocs web documentation. Attributes: Development ( Directories ) \u2013 BASE_DIR (Path): The root directory of the project (two levels up from this file). CONFIG_DIR (Path): Directory containing configuration files for the application. LOGS_DIR (Path): Directory where application log files are stored. Data ( Directories ) \u2013 DATA_DIR (Path): Root directory for data storage. RAW_DATA (Path): Directory where raw input data is kept. INTERMEDIATE_DIR (Path): Directory for intermediate data products during processing. RESULTS_DIR (Path): Directory where output results are stored. Configurable ( Variables ) \u2013 MAX_RETRIES (int): The maximum number of retry attempts for operations. MAX_RECOMMENDED_GROUPS (int): The maximum number of recommended groups to display. MIN_SAMPLES_MANY_SHOT (int): Minimum number of samples required for many-shot learning. MIN_SAMPLES_FEW_SHOT (int): Minimum number of samples required for few-shot learning. MIN_LOGISTIC_SAMPLES_PER_CLASS (int): Minimum number of samples required per class in logistic regression. Automated ( Evaluation Settings ) \u2013 MANY_SHOT_TRAIN_RATIO (float): Ratio of training samples to be used in many-shot scenarios. FEW_SHOT_COUNT (int): Number of examples per category for few-shot learning. Database ( Interface Definitions ) \u2013 DB_SERVER (str): The database server address. DB_NAME (str): Name of the database. DB_USER (str): Database user for authentication. DB_PASSWORD (str): Database password for authentication. GPT4_API_KEY (str): API key for GPT-4 (via Azure proxy). AZURE_DOCAI_KEY (str): API key for accessing Azure Document Analysis. AZURE_API_KEY (str): Alias for GPT4_API_KEY used with Azure services. Assets ( Interface Definitions ) \u2013 ASSETS_DIR (Path): Directory containing static assets (like prompt files). ZS_PROMPTY (Path): File path for the zero-shot prompt template. FS_PROMPTY (Path): File path for the few-shot prompt template. LLM ( Specific Settings ) \u2013 LLM_INTERFACE (OpenAI): Configured OpenAI interface using the GPT-4 model. document_analysis_client (DocumentAnalysisClient): Configured Azure client for document analysis. EMBEDDING_CLIENT (OpenAIEmbeddings): Configured OpenAI client for computing text embeddings. Page ( Rendering Configuration ) \u2013 HEADER_MARKDOWN (str): Markdown template used as a header for rendered pages, including important notices and policy information. For additional details on configuration and usage, please refer to the mkdocs documentation. Source code in LabeLMaker_config/config.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 class Config : \"\"\" Configuration class for the Generative Categorizer application. This class centralizes all the configurable settings and file paths used throughout the application, and it is automatically included in the mkdocs web documentation. Attributes: Development Directories: - BASE_DIR (Path): The root directory of the project (two levels up from this file). - CONFIG_DIR (Path): Directory containing configuration files for the application. - LOGS_DIR (Path): Directory where application log files are stored. Data Directories: - DATA_DIR (Path): Root directory for data storage. - RAW_DATA (Path): Directory where raw input data is kept. - INTERMEDIATE_DIR (Path): Directory for intermediate data products during processing. - RESULTS_DIR (Path): Directory where output results are stored. Configurable Variables: - MAX_RETRIES (int): The maximum number of retry attempts for operations. - MAX_RECOMMENDED_GROUPS (int): The maximum number of recommended groups to display. - MIN_SAMPLES_MANY_SHOT (int): Minimum number of samples required for many-shot learning. - MIN_SAMPLES_FEW_SHOT (int): Minimum number of samples required for few-shot learning. - MIN_LOGISTIC_SAMPLES_PER_CLASS (int): Minimum number of samples required per class in logistic regression. Automated Evaluation Settings: - MANY_SHOT_TRAIN_RATIO (float): Ratio of training samples to be used in many-shot scenarios. - FEW_SHOT_COUNT (int): Number of examples per category for few-shot learning. Database Interface Definitions: - DB_SERVER (str): The database server address. - DB_NAME (str): Name of the database. - DB_USER (str): Database user for authentication. - DB_PASSWORD (str): Database password for authentication. - GPT4_API_KEY (str): API key for GPT-4 (via Azure proxy). - AZURE_DOCAI_KEY (str): API key for accessing Azure Document Analysis. - AZURE_API_KEY (str): Alias for GPT4_API_KEY used with Azure services. Assets: - ASSETS_DIR (Path): Directory containing static assets (like prompt files). - ZS_PROMPTY (Path): File path for the zero-shot prompt template. - FS_PROMPTY (Path): File path for the few-shot prompt template. LLM Specific Settings: - LLM_INTERFACE (OpenAI): Configured OpenAI interface using the GPT-4 model. document_analysis_client (DocumentAnalysisClient): Configured Azure client for document analysis. - EMBEDDING_CLIENT (OpenAIEmbeddings): Configured OpenAI client for computing text embeddings. Page Rendering Configuration: - HEADER_MARKDOWN (str): Markdown template used as a header for rendered pages, including important notices and policy information. For additional details on configuration and usage, please refer to the mkdocs documentation. \"\"\" OPENAI_COMPATIBLE_KEY = \"sk-_jMrFbwNpOtZ5j46iCVKdg\" # Enter your OpenAI compatible key here OPENAI_COMPATIBLE_ENDPOINT = \"https://proxy-ai-anes-uabmc-awefchfueccrddhf.eastus2-01.azurewebsites.net/v1\" # Enter your OpenAI compatible endpoint here # Note: Can delete the azure keys if don't want (or care for) OCR on PDR upload. AZURE_DOCAI_COMPATIBLE_KEY = \"1v3mHESIbZIgSOdfQE1EZ1nU0wKmnZ1dChpfBSME6Rz4fTM66N8vJQQJ99AKACHYHv6XJ3w3AAALACOGp8BQ\" # Enter your Azure DocAI compatible key here AZURE_DOCAI_COMPATIBLE_ENDPOINT = \"https://read-ai-anes-uabmc.cognitiveservices.azure.com/\" # Enter your Azure DocAI compatible endpoint here # Development Directories BASE_DIR = Path ( __file__ ) . parent . parent . absolute () CONFIG_DIR = Path ( BASE_DIR , \"LabeLMaker_config\" ) LOGS_DIR = Path ( BASE_DIR , \"logs\" ) # Data Directories DATA_DIR = Path ( \"/data/DATASCI\" ) RAW_DATA = Path ( DATA_DIR , \"raw\" ) INTERMEDIATE_DIR = Path ( DATA_DIR , \"intermediate\" ) RESULTS_DIR = Path ( DATA_DIR , \"results\" ) # Configurable variables MAX_RETRIES = 3 MAX_RECOMMENDED_GROUPS = 10 MIN_SAMPLES_MANY_SHOT = 25 MIN_SAMPLES_FEW_SHOT = 1 MIN_LOGISTIC_SAMPLES_PER_CLASS = 100 # Automated selection for eval mode MANY_SHOT_TRAIN_RATIO = 0.8 FEW_SHOT_COUNT = 2 # number of examples per category # Assets ASSETS_DIR = Path ( BASE_DIR , \"assets\" ) ZS_PROMPTY = Path ( ASSETS_DIR , \"gencat_zeroshot.prompty\" ) FS_PROMPTY = Path ( ASSETS_DIR , \"gencat_fewshot.prompty\" ) # LLM specific LLM_INTERFACE = OpenAI ( base_url = OPENAI_COMPATIBLE_ENDPOINT , model = \"gpt-4o-mini\" , api_key = OPENAI_COMPATIBLE_KEY , ) document_analysis_client = DocumentAnalysisClient ( endpoint = AZURE_DOCAI_COMPATIBLE_ENDPOINT , credential = AzureKeyCredential ( AZURE_DOCAI_COMPATIBLE_KEY ), ) EMBEDDING_CLIENT = OpenAIEmbeddings ( api_key = OPENAI_COMPATIBLE_KEY , base_url = OPENAI_COMPATIBLE_ENDPOINT , model = \"text-embedding-3-small\" , ) # Page rendering configuration (separate file?) HEADER_MARKDOWN = \"\"\" --- **Categorize data using AI** Brought to you by the Perioperative Data Science Team at UAB _Not recommended for use with protected patient data_ --- \"\"\"","title":"Config"},{"location":"LabeLMaker_config/config.html#LabeLMaker_config.config.Config","text":"Configuration class for the Generative Categorizer application. This class centralizes all the configurable settings and file paths used throughout the application, and it is automatically included in the mkdocs web documentation. Attributes: Development ( Directories ) \u2013 BASE_DIR (Path): The root directory of the project (two levels up from this file). CONFIG_DIR (Path): Directory containing configuration files for the application. LOGS_DIR (Path): Directory where application log files are stored. Data ( Directories ) \u2013 DATA_DIR (Path): Root directory for data storage. RAW_DATA (Path): Directory where raw input data is kept. INTERMEDIATE_DIR (Path): Directory for intermediate data products during processing. RESULTS_DIR (Path): Directory where output results are stored. Configurable ( Variables ) \u2013 MAX_RETRIES (int): The maximum number of retry attempts for operations. MAX_RECOMMENDED_GROUPS (int): The maximum number of recommended groups to display. MIN_SAMPLES_MANY_SHOT (int): Minimum number of samples required for many-shot learning. MIN_SAMPLES_FEW_SHOT (int): Minimum number of samples required for few-shot learning. MIN_LOGISTIC_SAMPLES_PER_CLASS (int): Minimum number of samples required per class in logistic regression. Automated ( Evaluation Settings ) \u2013 MANY_SHOT_TRAIN_RATIO (float): Ratio of training samples to be used in many-shot scenarios. FEW_SHOT_COUNT (int): Number of examples per category for few-shot learning. Database ( Interface Definitions ) \u2013 DB_SERVER (str): The database server address. DB_NAME (str): Name of the database. DB_USER (str): Database user for authentication. DB_PASSWORD (str): Database password for authentication. GPT4_API_KEY (str): API key for GPT-4 (via Azure proxy). AZURE_DOCAI_KEY (str): API key for accessing Azure Document Analysis. AZURE_API_KEY (str): Alias for GPT4_API_KEY used with Azure services. Assets ( Interface Definitions ) \u2013 ASSETS_DIR (Path): Directory containing static assets (like prompt files). ZS_PROMPTY (Path): File path for the zero-shot prompt template. FS_PROMPTY (Path): File path for the few-shot prompt template. LLM ( Specific Settings ) \u2013 LLM_INTERFACE (OpenAI): Configured OpenAI interface using the GPT-4 model. document_analysis_client (DocumentAnalysisClient): Configured Azure client for document analysis. EMBEDDING_CLIENT (OpenAIEmbeddings): Configured OpenAI client for computing text embeddings. Page ( Rendering Configuration ) \u2013 HEADER_MARKDOWN (str): Markdown template used as a header for rendered pages, including important notices and policy information. For additional details on configuration and usage, please refer to the mkdocs documentation. Source code in LabeLMaker_config/config.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 class Config : \"\"\" Configuration class for the Generative Categorizer application. This class centralizes all the configurable settings and file paths used throughout the application, and it is automatically included in the mkdocs web documentation. Attributes: Development Directories: - BASE_DIR (Path): The root directory of the project (two levels up from this file). - CONFIG_DIR (Path): Directory containing configuration files for the application. - LOGS_DIR (Path): Directory where application log files are stored. Data Directories: - DATA_DIR (Path): Root directory for data storage. - RAW_DATA (Path): Directory where raw input data is kept. - INTERMEDIATE_DIR (Path): Directory for intermediate data products during processing. - RESULTS_DIR (Path): Directory where output results are stored. Configurable Variables: - MAX_RETRIES (int): The maximum number of retry attempts for operations. - MAX_RECOMMENDED_GROUPS (int): The maximum number of recommended groups to display. - MIN_SAMPLES_MANY_SHOT (int): Minimum number of samples required for many-shot learning. - MIN_SAMPLES_FEW_SHOT (int): Minimum number of samples required for few-shot learning. - MIN_LOGISTIC_SAMPLES_PER_CLASS (int): Minimum number of samples required per class in logistic regression. Automated Evaluation Settings: - MANY_SHOT_TRAIN_RATIO (float): Ratio of training samples to be used in many-shot scenarios. - FEW_SHOT_COUNT (int): Number of examples per category for few-shot learning. Database Interface Definitions: - DB_SERVER (str): The database server address. - DB_NAME (str): Name of the database. - DB_USER (str): Database user for authentication. - DB_PASSWORD (str): Database password for authentication. - GPT4_API_KEY (str): API key for GPT-4 (via Azure proxy). - AZURE_DOCAI_KEY (str): API key for accessing Azure Document Analysis. - AZURE_API_KEY (str): Alias for GPT4_API_KEY used with Azure services. Assets: - ASSETS_DIR (Path): Directory containing static assets (like prompt files). - ZS_PROMPTY (Path): File path for the zero-shot prompt template. - FS_PROMPTY (Path): File path for the few-shot prompt template. LLM Specific Settings: - LLM_INTERFACE (OpenAI): Configured OpenAI interface using the GPT-4 model. document_analysis_client (DocumentAnalysisClient): Configured Azure client for document analysis. - EMBEDDING_CLIENT (OpenAIEmbeddings): Configured OpenAI client for computing text embeddings. Page Rendering Configuration: - HEADER_MARKDOWN (str): Markdown template used as a header for rendered pages, including important notices and policy information. For additional details on configuration and usage, please refer to the mkdocs documentation. \"\"\" OPENAI_COMPATIBLE_KEY = \"sk-_jMrFbwNpOtZ5j46iCVKdg\" # Enter your OpenAI compatible key here OPENAI_COMPATIBLE_ENDPOINT = \"https://proxy-ai-anes-uabmc-awefchfueccrddhf.eastus2-01.azurewebsites.net/v1\" # Enter your OpenAI compatible endpoint here # Note: Can delete the azure keys if don't want (or care for) OCR on PDR upload. AZURE_DOCAI_COMPATIBLE_KEY = \"1v3mHESIbZIgSOdfQE1EZ1nU0wKmnZ1dChpfBSME6Rz4fTM66N8vJQQJ99AKACHYHv6XJ3w3AAALACOGp8BQ\" # Enter your Azure DocAI compatible key here AZURE_DOCAI_COMPATIBLE_ENDPOINT = \"https://read-ai-anes-uabmc.cognitiveservices.azure.com/\" # Enter your Azure DocAI compatible endpoint here # Development Directories BASE_DIR = Path ( __file__ ) . parent . parent . absolute () CONFIG_DIR = Path ( BASE_DIR , \"LabeLMaker_config\" ) LOGS_DIR = Path ( BASE_DIR , \"logs\" ) # Data Directories DATA_DIR = Path ( \"/data/DATASCI\" ) RAW_DATA = Path ( DATA_DIR , \"raw\" ) INTERMEDIATE_DIR = Path ( DATA_DIR , \"intermediate\" ) RESULTS_DIR = Path ( DATA_DIR , \"results\" ) # Configurable variables MAX_RETRIES = 3 MAX_RECOMMENDED_GROUPS = 10 MIN_SAMPLES_MANY_SHOT = 25 MIN_SAMPLES_FEW_SHOT = 1 MIN_LOGISTIC_SAMPLES_PER_CLASS = 100 # Automated selection for eval mode MANY_SHOT_TRAIN_RATIO = 0.8 FEW_SHOT_COUNT = 2 # number of examples per category # Assets ASSETS_DIR = Path ( BASE_DIR , \"assets\" ) ZS_PROMPTY = Path ( ASSETS_DIR , \"gencat_zeroshot.prompty\" ) FS_PROMPTY = Path ( ASSETS_DIR , \"gencat_fewshot.prompty\" ) # LLM specific LLM_INTERFACE = OpenAI ( base_url = OPENAI_COMPATIBLE_ENDPOINT , model = \"gpt-4o-mini\" , api_key = OPENAI_COMPATIBLE_KEY , ) document_analysis_client = DocumentAnalysisClient ( endpoint = AZURE_DOCAI_COMPATIBLE_ENDPOINT , credential = AzureKeyCredential ( AZURE_DOCAI_COMPATIBLE_KEY ), ) EMBEDDING_CLIENT = OpenAIEmbeddings ( api_key = OPENAI_COMPATIBLE_KEY , base_url = OPENAI_COMPATIBLE_ENDPOINT , model = \"text-embedding-3-small\" , ) # Page rendering configuration (separate file?) HEADER_MARKDOWN = \"\"\" --- **Categorize data using AI** Brought to you by the Perioperative Data Science Team at UAB _Not recommended for use with protected patient data_ --- \"\"\"","title":"Config"},{"location":"llm_utils/index.html","text":"Common code documentation AIWeb Common : Common code used throughout various projects of the UABPeriopAI team FastAPI helper apis schemas validators File Operations docx creator file config file handling text format upload manager Generate AugmentedResponse AugmentedServicer ChatResponse ChatSchemas ChatServicer PromptAssembler PromptyResponse PromptyServicer QueryInterface Response SingleResponse SingleResponseServicer Resourcing default resource config NIH RePorter Interface PubMed Interface PubMed Query Streamlit Bring Your Own Key (BYOK) Streamlit Common Object Factory Workflow Handler","title":"Home"},{"location":"llm_utils/index.html#common-code-documentation","text":"AIWeb Common : Common code used throughout various projects of the UABPeriopAI team FastAPI helper apis schemas validators File Operations docx creator file config file handling text format upload manager Generate AugmentedResponse AugmentedServicer ChatResponse ChatSchemas ChatServicer PromptAssembler PromptyResponse PromptyServicer QueryInterface Response SingleResponse SingleResponseServicer Resourcing default resource config NIH RePorter Interface PubMed Interface PubMed Query Streamlit Bring Your Own Key (BYOK) Streamlit Common Object Factory Workflow Handler","title":"Common code documentation"},{"location":"llm_utils/aiweb_common/ObjectFactory.html","text":"","title":"Object Factory"},{"location":"llm_utils/aiweb_common/WorkflowHandler.html","text":"WorkflowHandler Bases: ABC Source code in llm_utils/aiweb_common/WorkflowHandler.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 class WorkflowHandler ( ABC ): def __init__ ( self ): self . total_cost = 0.0 def _get_filename ( self ): # should not be forced. datafeasibility, for example, wouldn't use. raise NotImplementedError def _get_mime_type ( self ): # should not be forced. datafeasibility, for example, wouldn't use. raise NotImplementedError @abstractmethod def process ( self ): raise NotImplementedError def _update_total_cost ( self , response_meta ): self . total_cost += response_meta . total_cost def _get_db_connection ( self , db_server , db_name , db_user , db_password ): \"\"\" The function `get_db_connection` creates a database connection using the provided server, database name, user, and password. Args: db_server: The `db_server` parameter refers to the server where the database is hosted. This could be an IP address or a domain name pointing to the server where the SQL Server instance is running. db_name: The `db_name` parameter in the `get_db_connection` function refers to the name of the database you want to connect to on the specified database server. This parameter is used to construct the connection string that includes information such as the database name, server details, user credentials, and driver information for db_user: The `db_user` parameter in the `get_db_connection` function refers to the username used to authenticate and access the database. It is typically associated with a specific user account that has the necessary permissions to interact with the database specified by `db_name` on the server `db_server`. db_password: It seems like you were about to provide information about the `db_password` parameter but the text got cut off. Could you please provide the details or let me know how I can assist you further with the `db_password` parameter? Returns: The function `get_db_connection` returns a connection object to a SQL Server database using the provided server, database name, user, and password details. \"\"\" conn_str = ( \"DRIVER={ODBC Driver 17 for SQL Server};SERVER=\" + db_server + \";DATABASE=\" + db_name + \";UID=\" + db_user + \";PWD=\" + db_password ) return pyodbc . connect ( conn_str ) def _write_to_db ( self , app_config , input_as_json , submit_time , response_time , cost , name_suffix = \"\" , ): \"\"\" The `write_to_db` function inserts data into a database table `api_interactions` with specified columns using the provided parameters. Args: app_config: The `app_config` parameter is a configuration object that contains information about the database server, database name, database user, and database password needed to establish a connection to the database. It likely contains attributes like `DB_SERVER`, `DB_NAME`, `DB_USER`, `DB_PASSWORD`, and `NAME input_as_json: The `input_as_json` parameter in the `write_to_db` function is expected to be a JSON object representing the user input data that you want to store in the database. This JSON object should be a serializable format that can be stored in a database column, typically a string representation of the submit_time: Submit time is the timestamp when the request was submitted to the API. It typically includes the date and time when the request was made. response_time: Response time is the time taken for the server to process a request and send a response back to the client. It is usually measured in milliseconds and indicates the efficiency of the system in handling requests. cost: The `cost` parameter in the `write_to_db` function represents the total cost associated with the API interaction being recorded in the database. This cost could be related to any expenses incurred during the interaction, such as processing fees, data storage costs, or any other relevant expenses. It is a numerical name_suffix: The `name_suffix` parameter in the `write_to_db` function is an optional parameter that allows you to append a suffix to the `app_name` field in the database. This can be useful if you need to differentiate between multiple instances of the same application in the database. If a `name \"\"\" with self . _get_db_connection ( db_server = app_config . DB_SERVER , db_name = app_config . DB_NAME , db_user = app_config . DB_USER , db_password = app_config . DB_PASSWORD , ) as conn : cursor = conn . cursor () query = \"\"\" INSERT INTO api_interactions (app_name, user_input, submit_time, response_time, total_cost) VALUES (?, ?, ?, ?, ?) \"\"\" cursor . execute ( query , ( app_config . NAME + name_suffix , input_as_json , submit_time , response_time , cost , ), ) conn . commit () def check_content_type ( self , returned_content ): # TODO: consider changing to if hasattr content if isinstance ( returned_content , AIMessage ): extracted_content = returned_content . content if isinstance ( returned_content , str ): extracted_content = returned_content else : raise TypeError ( \"Content not of type AIMessage or str. Check what invoke is returning. Langchain interfaces are inconsistent per API provider.\" ) return extracted_content # TODO is there a way to make this cleaner since self.promtpy_path and self._validate are only called in grandchildren def load_prompty ( self ): # self.prompty_path initialized by child # This should likely be broken up more with to isolate functionality further if not self . prompty_path . exists (): raise FileNotFoundError ( f \"Prompty file not found at: { self . prompty_path } \" ) with open ( self . prompty_path , \"r\" ) as f : prompty_content = f . read () prompty_data = list ( yaml . safe_load_all ( prompty_content )) if not prompty_data or len ( prompty_data ) < 2 : raise ValueError ( \"Invalid prompty file format.\" ) prompt_section = prompty_data [ 1 ] prompt_template = prompt_section . get ( \"prompt\" , {}) . get ( \"template\" , None ) if prompt_template is None : raise ValueError ( \"Prompt template not found in prompty file.\" ) self . _validate_prompt_template ( prompt_template ) return ChatPromptTemplate . from_template ( prompt_template , template_format = \"jinja2\" ) def log_to_database ( self , app_config , content_to_log , start , finish , background_tasks , label = \"\" ): \"\"\" This Python function logs content to a database using background tasks and handles KeyError exceptions. Args: app_config: The `app_config` parameter likely contains configuration settings for the application, such as database connection details, API keys, or other settings needed for logging to the database. content_to_log: The `content_to_log` parameter typically refers to the data or information that you want to log into the database. This could be any relevant information that you want to store for later analysis or reference. It could be a string, a dictionary, a list, or any other data structure that you find start: The `start` parameter in the `log_to_database` method likely represents the start time of the logging operation. It is used to indicate when the logging process began. finish: Finish is a parameter representing the time when the logging process finishes. It is used in the `log_to_database` method to log information to a database along with other parameters such as `app_config`, `content_to_log`, `start`, `background_tasks`, and an optional `label`. background_tasks: The `background_tasks` parameter in the `log_to_database` method seems to be an instance of some class that allows you to add tasks to be executed in the background. In this method, a task is added to write data to a database in the background using the `_write_to_db` method label: The `label` parameter in the `log_to_database` method is a string that can be used to provide additional information or context for the log entry being written to the database. It is an optional parameter with a default value of an empty string. If provided, the `label` will be included \"\"\" try : background_tasks . add_task ( self . _write_to_db , app_config , content_to_log , start , finish , self . total_cost , label , ) except KeyError : raise KeyError ( \"Failed writing to database. Check interface configuration and try again.\" ) log_to_database ( app_config , content_to_log , start , finish , background_tasks , label = '' ) This Python function logs content to a database using background tasks and handles KeyError exceptions. Parameters: app_config \u2013 The app_config parameter likely contains configuration settings for the application, such as database connection details, API keys, or other settings needed for logging to the database. content_to_log: The content_to_log parameter typically refers to the data or information that you want to log into the database. This could be any relevant information that you want to store for later analysis or reference. It could be a string, a dictionary, a list, or any other data structure that you find start: The start parameter in the log_to_database method likely represents the start time of the logging operation. It is used to indicate when the logging process began. finish: Finish is a parameter representing the time when the logging process finishes. It is used in the log_to_database method to log information to a database along with other parameters such as app_config , content_to_log , start , background_tasks , and an optional label . background_tasks: The background_tasks parameter in the log_to_database method seems to be an instance of some class that allows you to add tasks to be executed in the background. In this method, a task is added to write data to a database in the background using the _write_to_db method label: The label parameter in the log_to_database method is a string that can be used to provide additional information or context for the log entry being written to the database. It is an optional parameter with a default value of an empty string. If provided, the label will be included Source code in llm_utils/aiweb_common/WorkflowHandler.py 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 def log_to_database ( self , app_config , content_to_log , start , finish , background_tasks , label = \"\" ): \"\"\" This Python function logs content to a database using background tasks and handles KeyError exceptions. Args: app_config: The `app_config` parameter likely contains configuration settings for the application, such as database connection details, API keys, or other settings needed for logging to the database. content_to_log: The `content_to_log` parameter typically refers to the data or information that you want to log into the database. This could be any relevant information that you want to store for later analysis or reference. It could be a string, a dictionary, a list, or any other data structure that you find start: The `start` parameter in the `log_to_database` method likely represents the start time of the logging operation. It is used to indicate when the logging process began. finish: Finish is a parameter representing the time when the logging process finishes. It is used in the `log_to_database` method to log information to a database along with other parameters such as `app_config`, `content_to_log`, `start`, `background_tasks`, and an optional `label`. background_tasks: The `background_tasks` parameter in the `log_to_database` method seems to be an instance of some class that allows you to add tasks to be executed in the background. In this method, a task is added to write data to a database in the background using the `_write_to_db` method label: The `label` parameter in the `log_to_database` method is a string that can be used to provide additional information or context for the log entry being written to the database. It is an optional parameter with a default value of an empty string. If provided, the `label` will be included \"\"\" try : background_tasks . add_task ( self . _write_to_db , app_config , content_to_log , start , finish , self . total_cost , label , ) except KeyError : raise KeyError ( \"Failed writing to database. Check interface configuration and try again.\" ) manage_sensitive ( name ) The manage_sensitive function retrieves sensitive information from different sources based on deployment and development paths, as well as environment variables, and raises an error if the secret is not found. Parameters: name \u2013 The name parameter in the manage_sensitive function is used to specify the name of the sensitive information or secret that the function is trying to retrieve. The function first checks for the existence of the secret in a deployment path, then in a development path using glob, and finally as an environment Returns: \u2013 The manage_sensitive function is designed to manage sensitive information retrieval from different sources. It first checks for the secret file in the deployment path, then in the development path using glob, and finally as an environment variable. If the secret is found in any of these sources, it is returned. If no secret is found, a KeyError is raised with a message indicating that the secret with Source code in llm_utils/aiweb_common/WorkflowHandler.py 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 def manage_sensitive ( name ): \"\"\" The `manage_sensitive` function retrieves sensitive information from different sources based on deployment and development paths, as well as environment variables, and raises an error if the secret is not found. Args: name: The `name` parameter in the `manage_sensitive` function is used to specify the name of the sensitive information or secret that the function is trying to retrieve. The function first checks for the existence of the secret in a deployment path, then in a development path using glob, and finally as an environment Returns: The `manage_sensitive` function is designed to manage sensitive information retrieval from different sources. It first checks for the secret file in the deployment path, then in the development path using glob, and finally as an environment variable. If the secret is found in any of these sources, it is returned. If no secret is found, a `KeyError` is raised with a message indicating that the secret with \"\"\" # Check in deployment path deploy_secret_fpath = f \"/run/secrets/ { name } \" if os . path . exists ( deploy_secret_fpath ): with open ( deploy_secret_fpath , \"r\" ) as file : return file . read () . rstrip ( \" \\n \" ) # Check in development path using glob develop_secret_paths = glob . glob ( f \"/workspaces/*/secrets/ { name } .txt\" ) if develop_secret_paths : # Assumes the first matching file is the correct one, adjust if necessary with open ( develop_secret_paths [ 0 ], \"r\" ) as file : return file . read () . rstrip ( \" \\n \" ) # Check environment variable last v1 = os . getenv ( name ) if v1 is not None : return v1 # If no secret is found raise KeyError ( f \"Secret { name } not found\" )","title":"Workflow Handler"},{"location":"llm_utils/aiweb_common/WorkflowHandler.html#aiweb_common.WorkflowHandler.WorkflowHandler","text":"Bases: ABC Source code in llm_utils/aiweb_common/WorkflowHandler.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 class WorkflowHandler ( ABC ): def __init__ ( self ): self . total_cost = 0.0 def _get_filename ( self ): # should not be forced. datafeasibility, for example, wouldn't use. raise NotImplementedError def _get_mime_type ( self ): # should not be forced. datafeasibility, for example, wouldn't use. raise NotImplementedError @abstractmethod def process ( self ): raise NotImplementedError def _update_total_cost ( self , response_meta ): self . total_cost += response_meta . total_cost def _get_db_connection ( self , db_server , db_name , db_user , db_password ): \"\"\" The function `get_db_connection` creates a database connection using the provided server, database name, user, and password. Args: db_server: The `db_server` parameter refers to the server where the database is hosted. This could be an IP address or a domain name pointing to the server where the SQL Server instance is running. db_name: The `db_name` parameter in the `get_db_connection` function refers to the name of the database you want to connect to on the specified database server. This parameter is used to construct the connection string that includes information such as the database name, server details, user credentials, and driver information for db_user: The `db_user` parameter in the `get_db_connection` function refers to the username used to authenticate and access the database. It is typically associated with a specific user account that has the necessary permissions to interact with the database specified by `db_name` on the server `db_server`. db_password: It seems like you were about to provide information about the `db_password` parameter but the text got cut off. Could you please provide the details or let me know how I can assist you further with the `db_password` parameter? Returns: The function `get_db_connection` returns a connection object to a SQL Server database using the provided server, database name, user, and password details. \"\"\" conn_str = ( \"DRIVER={ODBC Driver 17 for SQL Server};SERVER=\" + db_server + \";DATABASE=\" + db_name + \";UID=\" + db_user + \";PWD=\" + db_password ) return pyodbc . connect ( conn_str ) def _write_to_db ( self , app_config , input_as_json , submit_time , response_time , cost , name_suffix = \"\" , ): \"\"\" The `write_to_db` function inserts data into a database table `api_interactions` with specified columns using the provided parameters. Args: app_config: The `app_config` parameter is a configuration object that contains information about the database server, database name, database user, and database password needed to establish a connection to the database. It likely contains attributes like `DB_SERVER`, `DB_NAME`, `DB_USER`, `DB_PASSWORD`, and `NAME input_as_json: The `input_as_json` parameter in the `write_to_db` function is expected to be a JSON object representing the user input data that you want to store in the database. This JSON object should be a serializable format that can be stored in a database column, typically a string representation of the submit_time: Submit time is the timestamp when the request was submitted to the API. It typically includes the date and time when the request was made. response_time: Response time is the time taken for the server to process a request and send a response back to the client. It is usually measured in milliseconds and indicates the efficiency of the system in handling requests. cost: The `cost` parameter in the `write_to_db` function represents the total cost associated with the API interaction being recorded in the database. This cost could be related to any expenses incurred during the interaction, such as processing fees, data storage costs, or any other relevant expenses. It is a numerical name_suffix: The `name_suffix` parameter in the `write_to_db` function is an optional parameter that allows you to append a suffix to the `app_name` field in the database. This can be useful if you need to differentiate between multiple instances of the same application in the database. If a `name \"\"\" with self . _get_db_connection ( db_server = app_config . DB_SERVER , db_name = app_config . DB_NAME , db_user = app_config . DB_USER , db_password = app_config . DB_PASSWORD , ) as conn : cursor = conn . cursor () query = \"\"\" INSERT INTO api_interactions (app_name, user_input, submit_time, response_time, total_cost) VALUES (?, ?, ?, ?, ?) \"\"\" cursor . execute ( query , ( app_config . NAME + name_suffix , input_as_json , submit_time , response_time , cost , ), ) conn . commit () def check_content_type ( self , returned_content ): # TODO: consider changing to if hasattr content if isinstance ( returned_content , AIMessage ): extracted_content = returned_content . content if isinstance ( returned_content , str ): extracted_content = returned_content else : raise TypeError ( \"Content not of type AIMessage or str. Check what invoke is returning. Langchain interfaces are inconsistent per API provider.\" ) return extracted_content # TODO is there a way to make this cleaner since self.promtpy_path and self._validate are only called in grandchildren def load_prompty ( self ): # self.prompty_path initialized by child # This should likely be broken up more with to isolate functionality further if not self . prompty_path . exists (): raise FileNotFoundError ( f \"Prompty file not found at: { self . prompty_path } \" ) with open ( self . prompty_path , \"r\" ) as f : prompty_content = f . read () prompty_data = list ( yaml . safe_load_all ( prompty_content )) if not prompty_data or len ( prompty_data ) < 2 : raise ValueError ( \"Invalid prompty file format.\" ) prompt_section = prompty_data [ 1 ] prompt_template = prompt_section . get ( \"prompt\" , {}) . get ( \"template\" , None ) if prompt_template is None : raise ValueError ( \"Prompt template not found in prompty file.\" ) self . _validate_prompt_template ( prompt_template ) return ChatPromptTemplate . from_template ( prompt_template , template_format = \"jinja2\" ) def log_to_database ( self , app_config , content_to_log , start , finish , background_tasks , label = \"\" ): \"\"\" This Python function logs content to a database using background tasks and handles KeyError exceptions. Args: app_config: The `app_config` parameter likely contains configuration settings for the application, such as database connection details, API keys, or other settings needed for logging to the database. content_to_log: The `content_to_log` parameter typically refers to the data or information that you want to log into the database. This could be any relevant information that you want to store for later analysis or reference. It could be a string, a dictionary, a list, or any other data structure that you find start: The `start` parameter in the `log_to_database` method likely represents the start time of the logging operation. It is used to indicate when the logging process began. finish: Finish is a parameter representing the time when the logging process finishes. It is used in the `log_to_database` method to log information to a database along with other parameters such as `app_config`, `content_to_log`, `start`, `background_tasks`, and an optional `label`. background_tasks: The `background_tasks` parameter in the `log_to_database` method seems to be an instance of some class that allows you to add tasks to be executed in the background. In this method, a task is added to write data to a database in the background using the `_write_to_db` method label: The `label` parameter in the `log_to_database` method is a string that can be used to provide additional information or context for the log entry being written to the database. It is an optional parameter with a default value of an empty string. If provided, the `label` will be included \"\"\" try : background_tasks . add_task ( self . _write_to_db , app_config , content_to_log , start , finish , self . total_cost , label , ) except KeyError : raise KeyError ( \"Failed writing to database. Check interface configuration and try again.\" )","title":"WorkflowHandler"},{"location":"llm_utils/aiweb_common/WorkflowHandler.html#aiweb_common.WorkflowHandler.WorkflowHandler.log_to_database","text":"This Python function logs content to a database using background tasks and handles KeyError exceptions. Parameters: app_config \u2013 The app_config parameter likely contains configuration settings for the application, such as database connection details, API keys, or other settings needed for logging to the database. content_to_log: The content_to_log parameter typically refers to the data or information that you want to log into the database. This could be any relevant information that you want to store for later analysis or reference. It could be a string, a dictionary, a list, or any other data structure that you find start: The start parameter in the log_to_database method likely represents the start time of the logging operation. It is used to indicate when the logging process began. finish: Finish is a parameter representing the time when the logging process finishes. It is used in the log_to_database method to log information to a database along with other parameters such as app_config , content_to_log , start , background_tasks , and an optional label . background_tasks: The background_tasks parameter in the log_to_database method seems to be an instance of some class that allows you to add tasks to be executed in the background. In this method, a task is added to write data to a database in the background using the _write_to_db method label: The label parameter in the log_to_database method is a string that can be used to provide additional information or context for the log entry being written to the database. It is an optional parameter with a default value of an empty string. If provided, the label will be included Source code in llm_utils/aiweb_common/WorkflowHandler.py 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 def log_to_database ( self , app_config , content_to_log , start , finish , background_tasks , label = \"\" ): \"\"\" This Python function logs content to a database using background tasks and handles KeyError exceptions. Args: app_config: The `app_config` parameter likely contains configuration settings for the application, such as database connection details, API keys, or other settings needed for logging to the database. content_to_log: The `content_to_log` parameter typically refers to the data or information that you want to log into the database. This could be any relevant information that you want to store for later analysis or reference. It could be a string, a dictionary, a list, or any other data structure that you find start: The `start` parameter in the `log_to_database` method likely represents the start time of the logging operation. It is used to indicate when the logging process began. finish: Finish is a parameter representing the time when the logging process finishes. It is used in the `log_to_database` method to log information to a database along with other parameters such as `app_config`, `content_to_log`, `start`, `background_tasks`, and an optional `label`. background_tasks: The `background_tasks` parameter in the `log_to_database` method seems to be an instance of some class that allows you to add tasks to be executed in the background. In this method, a task is added to write data to a database in the background using the `_write_to_db` method label: The `label` parameter in the `log_to_database` method is a string that can be used to provide additional information or context for the log entry being written to the database. It is an optional parameter with a default value of an empty string. If provided, the `label` will be included \"\"\" try : background_tasks . add_task ( self . _write_to_db , app_config , content_to_log , start , finish , self . total_cost , label , ) except KeyError : raise KeyError ( \"Failed writing to database. Check interface configuration and try again.\" )","title":"log_to_database"},{"location":"llm_utils/aiweb_common/WorkflowHandler.html#aiweb_common.WorkflowHandler.manage_sensitive","text":"The manage_sensitive function retrieves sensitive information from different sources based on deployment and development paths, as well as environment variables, and raises an error if the secret is not found. Parameters: name \u2013 The name parameter in the manage_sensitive function is used to specify the name of the sensitive information or secret that the function is trying to retrieve. The function first checks for the existence of the secret in a deployment path, then in a development path using glob, and finally as an environment Returns: \u2013 The manage_sensitive function is designed to manage sensitive information retrieval from different sources. It first checks for the secret file in the deployment path, then in the development path using glob, and finally as an environment variable. If the secret is found in any of these sources, it is returned. If no secret is found, a KeyError is raised with a message indicating that the secret with Source code in llm_utils/aiweb_common/WorkflowHandler.py 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 def manage_sensitive ( name ): \"\"\" The `manage_sensitive` function retrieves sensitive information from different sources based on deployment and development paths, as well as environment variables, and raises an error if the secret is not found. Args: name: The `name` parameter in the `manage_sensitive` function is used to specify the name of the sensitive information or secret that the function is trying to retrieve. The function first checks for the existence of the secret in a deployment path, then in a development path using glob, and finally as an environment Returns: The `manage_sensitive` function is designed to manage sensitive information retrieval from different sources. It first checks for the secret file in the deployment path, then in the development path using glob, and finally as an environment variable. If the secret is found in any of these sources, it is returned. If no secret is found, a `KeyError` is raised with a message indicating that the secret with \"\"\" # Check in deployment path deploy_secret_fpath = f \"/run/secrets/ { name } \" if os . path . exists ( deploy_secret_fpath ): with open ( deploy_secret_fpath , \"r\" ) as file : return file . read () . rstrip ( \" \\n \" ) # Check in development path using glob develop_secret_paths = glob . glob ( f \"/workspaces/*/secrets/ { name } .txt\" ) if develop_secret_paths : # Assumes the first matching file is the correct one, adjust if necessary with open ( develop_secret_paths [ 0 ], \"r\" ) as file : return file . read () . rstrip ( \" \\n \" ) # Check environment variable last v1 = os . getenv ( name ) if v1 is not None : return v1 # If no secret is found raise KeyError ( f \"Secret { name } not found\" )","title":"manage_sensitive"},{"location":"llm_utils/aiweb_common/fastapi/helper_apis.html","text":"convert_file_to_base64 ( file = File ( ... )) async Internal API endpoint to convert files to base64-encoded strings. This endpoint is intended for use by API developers for testing. Source code in llm_utils/aiweb_common/fastapi/helper_apis.py 20 21 22 23 24 25 26 27 28 29 30 31 32 @router . post ( \"/internal/convert-to-base64/\" , include_in_schema = True ) async def convert_file_to_base64 ( file : UploadFile = File ( ... )): \"\"\" Internal API endpoint to convert files to base64-encoded strings. This endpoint is intended for use by API developers for testing. \"\"\" try : # Convert the uploaded file to base64 content = await file . read () encoded_string = base64 . b64encode ( content ) . decode ( \"utf-8\" ) return { \"base64\" : encoded_string } except Exception as e : raise HTTPException ( status_code = 500 , detail = str ( e )) decode_to_file ( request , background_tasks ) async Internal API endpoint to convert a base64-encoded string to a downloadable file. This endpoint is intended for use by API developers for testing. Source code in llm_utils/aiweb_common/fastapi/helper_apis.py 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 @router . post ( \"/internal/decode-to-file/\" , include_in_schema = True ) async def decode_to_file ( request : DecodeRequest , background_tasks : BackgroundTasks ): \"\"\" Internal API endpoint to convert a base64-encoded string to a downloadable file. This endpoint is intended for use by API developers for testing. \"\"\" try : # Decode the base64 string file_bytes = base64 . b64decode ( request . encoded_data ) # Write the decoded bytes to a temporary file with tempfile . NamedTemporaryFile ( delete = False , suffix = f \". { request . file_extension } \" ) as tmp_file : tmp_file . write ( file_bytes ) tmp_file_path = tmp_file . name response = FileResponse ( tmp_file_path , filename = f \"decoded_file. { request . file_extension } \" , media_type = \"application/octet-stream\" , ) # Return a FileResponse that allows the file to be downloaded background_tasks . add_task ( os . unlink , tmp_file_path ) return response except Exception as e : raise HTTPException ( status_code = 500 , detail = f \"Failed to decode and generate file: { str ( e ) } \" )","title":"Helper APIS"},{"location":"llm_utils/aiweb_common/fastapi/helper_apis.html#aiweb_common.fastapi.helper_apis.convert_file_to_base64","text":"Internal API endpoint to convert files to base64-encoded strings. This endpoint is intended for use by API developers for testing. Source code in llm_utils/aiweb_common/fastapi/helper_apis.py 20 21 22 23 24 25 26 27 28 29 30 31 32 @router . post ( \"/internal/convert-to-base64/\" , include_in_schema = True ) async def convert_file_to_base64 ( file : UploadFile = File ( ... )): \"\"\" Internal API endpoint to convert files to base64-encoded strings. This endpoint is intended for use by API developers for testing. \"\"\" try : # Convert the uploaded file to base64 content = await file . read () encoded_string = base64 . b64encode ( content ) . decode ( \"utf-8\" ) return { \"base64\" : encoded_string } except Exception as e : raise HTTPException ( status_code = 500 , detail = str ( e ))","title":"convert_file_to_base64"},{"location":"llm_utils/aiweb_common/fastapi/helper_apis.html#aiweb_common.fastapi.helper_apis.decode_to_file","text":"Internal API endpoint to convert a base64-encoded string to a downloadable file. This endpoint is intended for use by API developers for testing. Source code in llm_utils/aiweb_common/fastapi/helper_apis.py 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 @router . post ( \"/internal/decode-to-file/\" , include_in_schema = True ) async def decode_to_file ( request : DecodeRequest , background_tasks : BackgroundTasks ): \"\"\" Internal API endpoint to convert a base64-encoded string to a downloadable file. This endpoint is intended for use by API developers for testing. \"\"\" try : # Decode the base64 string file_bytes = base64 . b64decode ( request . encoded_data ) # Write the decoded bytes to a temporary file with tempfile . NamedTemporaryFile ( delete = False , suffix = f \". { request . file_extension } \" ) as tmp_file : tmp_file . write ( file_bytes ) tmp_file_path = tmp_file . name response = FileResponse ( tmp_file_path , filename = f \"decoded_file. { request . file_extension } \" , media_type = \"application/octet-stream\" , ) # Return a FileResponse that allows the file to be downloaded background_tasks . add_task ( os . unlink , tmp_file_path ) return response except Exception as e : raise HTTPException ( status_code = 500 , detail = f \"Failed to decode and generate file: { str ( e ) } \" )","title":"decode_to_file"},{"location":"llm_utils/aiweb_common/fastapi/schemas.html","text":"MSWordResponse Bases: BaseModel This class represents a response object containing a base64-encoded DOCX file that can be decoded to obtain the DOCX file. Source code in llm_utils/aiweb_common/fastapi/schemas.py 5 6 7 8 9 10 11 12 13 class MSWordResponse ( BaseModel ): \"\"\" This class represents a response object containing a base64-encoded DOCX file that can be decoded to obtain the DOCX file. \"\"\" encoded_docx : str = Field ( ... , description = \"Base64-encoded DOCX file. Decode to obtain the DOCX file.\" )","title":"Schemas"},{"location":"llm_utils/aiweb_common/fastapi/schemas.html#aiweb_common.fastapi.schemas.MSWordResponse","text":"Bases: BaseModel This class represents a response object containing a base64-encoded DOCX file that can be decoded to obtain the DOCX file. Source code in llm_utils/aiweb_common/fastapi/schemas.py 5 6 7 8 9 10 11 12 13 class MSWordResponse ( BaseModel ): \"\"\" This class represents a response object containing a base64-encoded DOCX file that can be decoded to obtain the DOCX file. \"\"\" encoded_docx : str = Field ( ... , description = \"Base64-encoded DOCX file. Decode to obtain the DOCX file.\" )","title":"MSWordResponse"},{"location":"llm_utils/aiweb_common/fastapi/validators.html","text":"","title":"Validators"},{"location":"llm_utils/aiweb_common/file_operations/docx_creator.html","text":"DocxCreator Parent class that holds the common methods for creating DOCX reports, including the results summary and figures. Source code in llm_utils/aiweb_common/file_operations/docx_creator.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 class DocxCreator : \"\"\" Parent class that holds the common methods for creating DOCX reports, including the results summary and figures. \"\"\" def __init__ ( self , results = None , figures = None ): self . results = results self . figures = figures def _add_table ( self , doc : Document , table_data : pd . DataFrame , heading : str , style : str = \"LightShading-Accent1\" ): \"\"\" Insert a table into the DOCX document. \"\"\" doc . add_heading ( heading , level = 2 ) table = doc . add_table ( rows = 1 , cols = len ( table_data . columns )) table . style = style # Header row. hdr_cells = table . rows [ 0 ] . cells for i , col_name in enumerate ( table_data . columns ): hdr_cells [ i ] . text = col_name # Populate the table with data. for _ , row in table_data . iterrows (): row_cells = table . add_row () . cells for i , cell_value in enumerate ( row ): row_cells [ i ] . text = str ( cell_value ) doc . add_paragraph () def _add_results_to_docx ( self , doc : Document , results : dict ): \"\"\" Insert results into the DOCX, including tables of metrics. \"\"\" doc . add_heading ( \"Results\" , level = 1 ) for method , metrics_data in results . items (): print ( \"method - \" , method ) print ( \"metrics - \" , metrics_data ) doc . add_heading ( method , level = 2 ) # Check if the data is already structured into separate sections if \"metrics\" in metrics_data : overall_metrics_df = metrics_data [ \"metrics\" ] else : # Create a dataframe from the flat metrics (excluding the classification report) # Here we assume that 'Classification Report' is separate; # Everything else is part of overall metrics. overall_metrics = { k : v for k , v in metrics_data . items () if k != \"Classification Report\" and not isinstance ( v , dict )} overall_metrics_df = pd . DataFrame ( list ( overall_metrics . items ()), columns = [ \"Metric\" , \"Value\" ]) self . _add_table ( doc , overall_metrics_df , \"Overall Metrics\" ) # For classification report, try to get it from either 'report' or 'Classification Report' if \"report\" in metrics_data : class_report_df = metrics_data [ \"report\" ] elif \"Classification Report\" in metrics_data : class_report_df = pd . DataFrame ( metrics_data [ \"Classification Report\" ]) . transpose () else : class_report_df = None if class_report_df is not None : self . _add_table ( doc , class_report_df , \"Classification Report\" ) # If there is any bootstrap information if \"bootstrap\" in metrics_data : self . _add_table ( doc , metrics_data [ \"bootstrap\" ], \"Bootstrap Confidence Intervals\" ) doc . add_paragraph () def create_docx_report ( self ) -> Document : doc = Document () doc . add_heading ( \"Model Evaluation Report\" , 0 ) # Add the method comparisons / metrics overview. if self . results : try : self . _add_results_to_docx ( doc , self . results ) except Exception as e : print ( \"Error in _add_results_to_docx:\" , e ) raise # Add figures / confusion matrices. if self . figures : doc . add_heading ( \"Figures\" , level = 1 ) for method , cm_fig in self . figures . items (): doc . add_heading ( method , level = 2 ) buf = io . BytesIO () try : cm_fig . savefig ( buf , format = \"png\" , bbox_inches = \"tight\" ) except Exception as e : print ( f \"Error saving figure for { method } : { e } \" ) raise buf . seek ( 0 ) try : doc . add_picture ( buf , width = Inches ( 5 )) except Exception as e : print ( f \"Error adding picture for { method } : { e } \" ) raise doc . add_paragraph () return doc FastAPIDocxCreator Bases: DocxCreator A specialized DocxCreator for FastAPI usage. Inherits the common DOCX-report functionality and adds a method to handle markdown-to-docx conversion. Source code in llm_utils/aiweb_common/file_operations/docx_creator.py 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 class FastAPIDocxCreator ( DocxCreator ): \"\"\" A specialized DocxCreator for FastAPI usage. Inherits the common DOCX-report functionality and adds a method to handle markdown-to-docx conversion. \"\"\" def __init__ ( self , background_tasks : BackgroundTasks , results = None , figures = None ): super () . __init__ ( results = results , figures = figures ) self . background_tasks = background_tasks def convert_markdown_to_docx_bytes ( self , generated_response , template_filepath = None ) -> str : \"\"\" Convert a markdown-based response to a temporary DOCX, then base64-encode it for returning via FastAPI. \"\"\" print ( \"Converting markdown to temporary file\" ) temp_file_path = markdown_to_docx_temporary_file ( generated_response , template_location = template_filepath ) encoded_file = file_to_base64 ( temp_file_path ) # Convert file to base64 print ( \"File encoded\" ) self . background_tasks . add_task ( os . unlink , temp_file_path ) return encoded_file convert_markdown_to_docx_bytes ( generated_response , template_filepath = None ) Convert a markdown-based response to a temporary DOCX, then base64-encode it for returning via FastAPI. Source code in llm_utils/aiweb_common/file_operations/docx_creator.py 138 139 140 141 142 143 144 145 146 147 148 149 150 def convert_markdown_to_docx_bytes ( self , generated_response , template_filepath = None ) -> str : \"\"\" Convert a markdown-based response to a temporary DOCX, then base64-encode it for returning via FastAPI. \"\"\" print ( \"Converting markdown to temporary file\" ) temp_file_path = markdown_to_docx_temporary_file ( generated_response , template_location = template_filepath ) encoded_file = file_to_base64 ( temp_file_path ) # Convert file to base64 print ( \"File encoded\" ) self . background_tasks . add_task ( os . unlink , temp_file_path ) return encoded_file StreamlitDocxCreator Bases: DocxCreator A specialized DocxCreator for Streamlit usage. It inherits the _add_results_to_docx and create_docx_report methods directly from the parent class now, so there's no need to redefine them. Source code in llm_utils/aiweb_common/file_operations/docx_creator.py 114 115 116 117 118 119 120 121 122 class StreamlitDocxCreator ( DocxCreator ): \"\"\" A specialized DocxCreator for Streamlit usage. It inherits the _add_results_to_docx and create_docx_report methods directly from the parent class now, so there's no need to redefine them. \"\"\" def __init__ ( self , results , figures ): super () . __init__ ( results = results , figures = figures )","title":"Docx Creator"},{"location":"llm_utils/aiweb_common/file_operations/docx_creator.html#aiweb_common.file_operations.docx_creator.DocxCreator","text":"Parent class that holds the common methods for creating DOCX reports, including the results summary and figures. Source code in llm_utils/aiweb_common/file_operations/docx_creator.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 class DocxCreator : \"\"\" Parent class that holds the common methods for creating DOCX reports, including the results summary and figures. \"\"\" def __init__ ( self , results = None , figures = None ): self . results = results self . figures = figures def _add_table ( self , doc : Document , table_data : pd . DataFrame , heading : str , style : str = \"LightShading-Accent1\" ): \"\"\" Insert a table into the DOCX document. \"\"\" doc . add_heading ( heading , level = 2 ) table = doc . add_table ( rows = 1 , cols = len ( table_data . columns )) table . style = style # Header row. hdr_cells = table . rows [ 0 ] . cells for i , col_name in enumerate ( table_data . columns ): hdr_cells [ i ] . text = col_name # Populate the table with data. for _ , row in table_data . iterrows (): row_cells = table . add_row () . cells for i , cell_value in enumerate ( row ): row_cells [ i ] . text = str ( cell_value ) doc . add_paragraph () def _add_results_to_docx ( self , doc : Document , results : dict ): \"\"\" Insert results into the DOCX, including tables of metrics. \"\"\" doc . add_heading ( \"Results\" , level = 1 ) for method , metrics_data in results . items (): print ( \"method - \" , method ) print ( \"metrics - \" , metrics_data ) doc . add_heading ( method , level = 2 ) # Check if the data is already structured into separate sections if \"metrics\" in metrics_data : overall_metrics_df = metrics_data [ \"metrics\" ] else : # Create a dataframe from the flat metrics (excluding the classification report) # Here we assume that 'Classification Report' is separate; # Everything else is part of overall metrics. overall_metrics = { k : v for k , v in metrics_data . items () if k != \"Classification Report\" and not isinstance ( v , dict )} overall_metrics_df = pd . DataFrame ( list ( overall_metrics . items ()), columns = [ \"Metric\" , \"Value\" ]) self . _add_table ( doc , overall_metrics_df , \"Overall Metrics\" ) # For classification report, try to get it from either 'report' or 'Classification Report' if \"report\" in metrics_data : class_report_df = metrics_data [ \"report\" ] elif \"Classification Report\" in metrics_data : class_report_df = pd . DataFrame ( metrics_data [ \"Classification Report\" ]) . transpose () else : class_report_df = None if class_report_df is not None : self . _add_table ( doc , class_report_df , \"Classification Report\" ) # If there is any bootstrap information if \"bootstrap\" in metrics_data : self . _add_table ( doc , metrics_data [ \"bootstrap\" ], \"Bootstrap Confidence Intervals\" ) doc . add_paragraph () def create_docx_report ( self ) -> Document : doc = Document () doc . add_heading ( \"Model Evaluation Report\" , 0 ) # Add the method comparisons / metrics overview. if self . results : try : self . _add_results_to_docx ( doc , self . results ) except Exception as e : print ( \"Error in _add_results_to_docx:\" , e ) raise # Add figures / confusion matrices. if self . figures : doc . add_heading ( \"Figures\" , level = 1 ) for method , cm_fig in self . figures . items (): doc . add_heading ( method , level = 2 ) buf = io . BytesIO () try : cm_fig . savefig ( buf , format = \"png\" , bbox_inches = \"tight\" ) except Exception as e : print ( f \"Error saving figure for { method } : { e } \" ) raise buf . seek ( 0 ) try : doc . add_picture ( buf , width = Inches ( 5 )) except Exception as e : print ( f \"Error adding picture for { method } : { e } \" ) raise doc . add_paragraph () return doc","title":"DocxCreator"},{"location":"llm_utils/aiweb_common/file_operations/docx_creator.html#aiweb_common.file_operations.docx_creator.FastAPIDocxCreator","text":"Bases: DocxCreator A specialized DocxCreator for FastAPI usage. Inherits the common DOCX-report functionality and adds a method to handle markdown-to-docx conversion. Source code in llm_utils/aiweb_common/file_operations/docx_creator.py 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 class FastAPIDocxCreator ( DocxCreator ): \"\"\" A specialized DocxCreator for FastAPI usage. Inherits the common DOCX-report functionality and adds a method to handle markdown-to-docx conversion. \"\"\" def __init__ ( self , background_tasks : BackgroundTasks , results = None , figures = None ): super () . __init__ ( results = results , figures = figures ) self . background_tasks = background_tasks def convert_markdown_to_docx_bytes ( self , generated_response , template_filepath = None ) -> str : \"\"\" Convert a markdown-based response to a temporary DOCX, then base64-encode it for returning via FastAPI. \"\"\" print ( \"Converting markdown to temporary file\" ) temp_file_path = markdown_to_docx_temporary_file ( generated_response , template_location = template_filepath ) encoded_file = file_to_base64 ( temp_file_path ) # Convert file to base64 print ( \"File encoded\" ) self . background_tasks . add_task ( os . unlink , temp_file_path ) return encoded_file","title":"FastAPIDocxCreator"},{"location":"llm_utils/aiweb_common/file_operations/docx_creator.html#aiweb_common.file_operations.docx_creator.FastAPIDocxCreator.convert_markdown_to_docx_bytes","text":"Convert a markdown-based response to a temporary DOCX, then base64-encode it for returning via FastAPI. Source code in llm_utils/aiweb_common/file_operations/docx_creator.py 138 139 140 141 142 143 144 145 146 147 148 149 150 def convert_markdown_to_docx_bytes ( self , generated_response , template_filepath = None ) -> str : \"\"\" Convert a markdown-based response to a temporary DOCX, then base64-encode it for returning via FastAPI. \"\"\" print ( \"Converting markdown to temporary file\" ) temp_file_path = markdown_to_docx_temporary_file ( generated_response , template_location = template_filepath ) encoded_file = file_to_base64 ( temp_file_path ) # Convert file to base64 print ( \"File encoded\" ) self . background_tasks . add_task ( os . unlink , temp_file_path ) return encoded_file","title":"convert_markdown_to_docx_bytes"},{"location":"llm_utils/aiweb_common/file_operations/docx_creator.html#aiweb_common.file_operations.docx_creator.StreamlitDocxCreator","text":"Bases: DocxCreator A specialized DocxCreator for Streamlit usage. It inherits the _add_results_to_docx and create_docx_report methods directly from the parent class now, so there's no need to redefine them. Source code in llm_utils/aiweb_common/file_operations/docx_creator.py 114 115 116 117 118 119 120 121 122 class StreamlitDocxCreator ( DocxCreator ): \"\"\" A specialized DocxCreator for Streamlit usage. It inherits the _add_results_to_docx and create_docx_report methods directly from the parent class now, so there's no need to redefine them. \"\"\" def __init__ ( self , results , figures ): super () . __init__ ( results = results , figures = figures )","title":"StreamlitDocxCreator"},{"location":"llm_utils/aiweb_common/file_operations/file_config.html","text":"","title":"File Config"},{"location":"llm_utils/aiweb_common/file_operations/file_handling.html","text":"create_base64_file_validator ( * allowed_mime_types ) Creates a function that validates the MIME type of a base64-encoded file. Parameters: allowed_mime_types ( tuple , default: () ) \u2013 A tuple of strings representing the allowed MIME types. Returns: \u2013 Callable[[str, ValidationInfo], str]: A function to validate base64-encoded files. Source code in llm_utils/aiweb_common/file_operations/file_handling.py 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 def create_base64_file_validator ( * allowed_mime_types ): \"\"\" Creates a function that validates the MIME type of a base64-encoded file. Args: allowed_mime_types (tuple): A tuple of strings representing the allowed MIME types. Returns: Callable[[str, ValidationInfo], str]: A function to validate base64-encoded files. \"\"\" def validate_base64_encoded_file ( cls , v , info ): \"\"\" Validate the MIME type of a base64-encoded file. Raises ValueError if the MIME type is not what is expected. \"\"\" try : file_bytes = base64 . b64decode ( v , validate = True ) except ValueError : raise ValueError ( \"Invalid base64 encoding\" ) # Use python-magic to check MIME type mime = magic . Magic ( mime = True ) mime_type = mime . from_buffer ( file_bytes ) if mime_type not in allowed_mime_types : allowed_types_formatted = \", \" . join ( allowed_mime_types ) raise ValueError ( f \"Incorrect file type. Required types: { allowed_types_formatted } \" ) return v return validate_base64_encoded_file create_file_validator ( * allowed_mime_types ) Creates a dependency function that validates the MIME type of an uploaded file. Parameters: allowed_mime_types ( tuple , default: () ) \u2013 A tuple of strings representing the allowed MIME types. Returns: \u2013 Callable[[UploadFile], UploadFile]: A function that checks if the uploaded file's MIME type \u2013 is in the allowed MIME types. Examples: validate_docx_file = create_file_validator(\"application/vnd.openxmlformats-officedocument.wordprocessingml.document\") validate_pdf_file = create_file_validator(\"application/pdf\", \"application/x-pdf\") Source code in llm_utils/aiweb_common/file_operations/file_handling.py 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 def create_file_validator ( * allowed_mime_types ): \"\"\" Creates a dependency function that validates the MIME type of an uploaded file. Args: allowed_mime_types (tuple): A tuple of strings representing the allowed MIME types. Returns: Callable[[UploadFile], UploadFile]: A function that checks if the uploaded file's MIME type is in the allowed MIME types. Examples: validate_docx_file = create_file_validator(\"application/vnd.openxmlformats-officedocument.wordprocessingml.document\") validate_pdf_file = create_file_validator(\"application/pdf\", \"application/x-pdf\") \"\"\" def validate_file ( file : UploadFile = File ( ... )): \"\"\" Validate the MIME type of the uploaded file. Raises HTTPException if the MIME type is not what is expected. \"\"\" if file . content_type not in allowed_mime_types : allowed_types_formatted = \", \" . join ( allowed_mime_types ) raise HTTPException ( status_code = 415 , detail = \"Incorrect file type. Required type: \" + allowed_types_formatted , ) return file return validate_file file_to_base64 ( filepath ) Converts a file to a base64-encoded string. Source code in llm_utils/aiweb_common/file_operations/file_handling.py 12 13 14 15 def file_to_base64 ( filepath ): \"\"\"Converts a file to a base64-encoded string.\"\"\" with open ( filepath , \"rb\" ) as file : return base64 . b64encode ( file . read ()) . decode ( \"utf-8\" ) ingest_docx ( file ) async The function ingest_docx reads the content of a DOCX file as bytes, writes it to a temporary file, and then loads the document from the temporary file. :param file: The file parameter in the ingest_docx function seems to be a file-like object that supports asynchronous reading operations. When await file.read() is called, it reads the content of the file as bytes. This content is then written to a temporary file with a .docx :return: The function ingest_docx returns a tuple containing two values: 1. The name of the temporary file where the DOCX content was written. 2. An instance of the Document` class representing the loaded document from the temporary file. Source code in llm_utils/aiweb_common/file_operations/file_handling.py 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 async def ingest_docx ( file ): \"\"\" The function `ingest_docx` reads the content of a DOCX file as bytes, writes it to a temporary file, and then loads the document from the temporary file. :param file: The `file` parameter in the `ingest_docx` function seems to be a file-like object that supports asynchronous reading operations. When `await file.read()` is called, it reads the content of the file as bytes. This content is then written to a temporary file with a `.docx :return: The function `ingest_docx` returns a tuple containing two values: 1. The name of the temporary file where the DOCX content was written. 2. An instance of the `Document` class representing the loaded document from the temporary file. \"\"\" with tempfile . NamedTemporaryFile ( delete = False , suffix = \".docx\" ) as temp_doc : content = await file . read () # Read file content as bytes # have to do this because of async context, otherwise calling function moves on temp_doc . write ( content ) temp_doc . flush () # Ensure all content is written to disk # Load the document from the temporary file return temp_doc . name , Document ( temp_doc . name ) # Load the document here ingest_docx_bytes ( content ) The function ingest_docx_bytes reads the content of a DOCX file from bytes, saves it to a temporary file, and then loads the document using the Document class. :param content: The ingest_docx_bytes function you provided seems to be designed to ingest the content of a DOCX file as bytes and load it using the python-docx library. However, it looks like the content parameter is missing in your message. Could you please provide the content parameter so :return: The ingest_docx_bytes function returns a tuple containing two values: 1. The name of the temporary file where the content was written (temp_doc.name) 2. The Document object representing the content loaded from the temporary file Source code in llm_utils/aiweb_common/file_operations/file_handling.py 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 def ingest_docx_bytes ( content ): \"\"\" The function `ingest_docx_bytes` reads the content of a DOCX file from bytes, saves it to a temporary file, and then loads the document using the `Document` class. :param content: The `ingest_docx_bytes` function you provided seems to be designed to ingest the content of a DOCX file as bytes and load it using the `python-docx` library. However, it looks like the content parameter is missing in your message. Could you please provide the content parameter so :return: The `ingest_docx_bytes` function returns a tuple containing two values: 1. The name of the temporary file where the content was written (temp_doc.name) 2. The Document object representing the content loaded from the temporary file \"\"\" with tempfile . NamedTemporaryFile ( delete = False , suffix = \".docx\" ) as temp_doc : temp_doc . write ( content ) temp_doc . flush () # Ensure all content is written to disk # Load the document from the temporary file return temp_doc . name , Document ( temp_doc . name ) # Load the document here markdown_to_docx_temporary_file ( content , template_location = None ) The function prepare_docx_response converts Markdown content to a DOCX file and returns the temporary file path. :param content: The content parameter in the prepare_docx_response function is the text or data that you want to convert to a DOCX file. This content will be processed and converted into a DOCX file using the convert_markdown_docx function :param template_location: The prepare_docx_response function takes two parameters: :return: The function prepare_docx_response returns the file path of the temporary .docx file that is created after converting the provided content (in markdown format) to a .docx file using the specified template location. Source code in llm_utils/aiweb_common/file_operations/file_handling.py 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 def markdown_to_docx_temporary_file ( content , template_location = None ): \"\"\" The function `prepare_docx_response` converts Markdown content to a DOCX file and returns the temporary file path. :param content: The `content` parameter in the `prepare_docx_response` function is the text or data that you want to convert to a DOCX file. This content will be processed and converted into a DOCX file using the `convert_markdown_docx` function :param template_location: The `prepare_docx_response` function takes two parameters: :return: The function `prepare_docx_response` returns the file path of the temporary .docx file that is created after converting the provided content (in markdown format) to a .docx file using the specified template location. \"\"\" docx_data = convert_markdown_docx ( content , template_location ) # Using tempfile to save the output file temporarily with tempfile . NamedTemporaryFile ( delete = False , suffix = \".docx\" ) as temp_file : temp_file . write ( docx_data ) temp_file_path = temp_file . name return temp_file_path validate_date ( date_str = Query ( ... , description = 'The start date in YYYY-MM-DD format' )) Custom dependency that validates and parses a date string. Args: date_str (str): A date string in the YYYY-MM-DD format. Returns: datetime: The parsed datetime object. Raises: HTTPException: If the date string is not in the correct format. Source code in llm_utils/aiweb_common/file_operations/file_handling.py 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 def validate_date ( date_str : str = Query ( ... , description = \"The start date in YYYY-MM-DD format\" ) ) -> datetime : \"\"\" Custom dependency that validates and parses a date string. Args: date_str (str): A date string in the YYYY-MM-DD format. Returns: datetime: The parsed datetime object. Raises: HTTPException: If the date string is not in the correct format. \"\"\" try : return datetime . strptime ( date_str , \"%Y-%m- %d \" ) except ValueError as exc : raise HTTPException ( status_code = 400 , detail = \"start_date must be in YYYY-MM-DD format\" ) from exc","title":"File Handling"},{"location":"llm_utils/aiweb_common/file_operations/file_handling.html#aiweb_common.file_operations.file_handling.create_base64_file_validator","text":"Creates a function that validates the MIME type of a base64-encoded file. Parameters: allowed_mime_types ( tuple , default: () ) \u2013 A tuple of strings representing the allowed MIME types. Returns: \u2013 Callable[[str, ValidationInfo], str]: A function to validate base64-encoded files. Source code in llm_utils/aiweb_common/file_operations/file_handling.py 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 def create_base64_file_validator ( * allowed_mime_types ): \"\"\" Creates a function that validates the MIME type of a base64-encoded file. Args: allowed_mime_types (tuple): A tuple of strings representing the allowed MIME types. Returns: Callable[[str, ValidationInfo], str]: A function to validate base64-encoded files. \"\"\" def validate_base64_encoded_file ( cls , v , info ): \"\"\" Validate the MIME type of a base64-encoded file. Raises ValueError if the MIME type is not what is expected. \"\"\" try : file_bytes = base64 . b64decode ( v , validate = True ) except ValueError : raise ValueError ( \"Invalid base64 encoding\" ) # Use python-magic to check MIME type mime = magic . Magic ( mime = True ) mime_type = mime . from_buffer ( file_bytes ) if mime_type not in allowed_mime_types : allowed_types_formatted = \", \" . join ( allowed_mime_types ) raise ValueError ( f \"Incorrect file type. Required types: { allowed_types_formatted } \" ) return v return validate_base64_encoded_file","title":"create_base64_file_validator"},{"location":"llm_utils/aiweb_common/file_operations/file_handling.html#aiweb_common.file_operations.file_handling.create_file_validator","text":"Creates a dependency function that validates the MIME type of an uploaded file. Parameters: allowed_mime_types ( tuple , default: () ) \u2013 A tuple of strings representing the allowed MIME types. Returns: \u2013 Callable[[UploadFile], UploadFile]: A function that checks if the uploaded file's MIME type \u2013 is in the allowed MIME types. Examples: validate_docx_file = create_file_validator(\"application/vnd.openxmlformats-officedocument.wordprocessingml.document\") validate_pdf_file = create_file_validator(\"application/pdf\", \"application/x-pdf\") Source code in llm_utils/aiweb_common/file_operations/file_handling.py 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 def create_file_validator ( * allowed_mime_types ): \"\"\" Creates a dependency function that validates the MIME type of an uploaded file. Args: allowed_mime_types (tuple): A tuple of strings representing the allowed MIME types. Returns: Callable[[UploadFile], UploadFile]: A function that checks if the uploaded file's MIME type is in the allowed MIME types. Examples: validate_docx_file = create_file_validator(\"application/vnd.openxmlformats-officedocument.wordprocessingml.document\") validate_pdf_file = create_file_validator(\"application/pdf\", \"application/x-pdf\") \"\"\" def validate_file ( file : UploadFile = File ( ... )): \"\"\" Validate the MIME type of the uploaded file. Raises HTTPException if the MIME type is not what is expected. \"\"\" if file . content_type not in allowed_mime_types : allowed_types_formatted = \", \" . join ( allowed_mime_types ) raise HTTPException ( status_code = 415 , detail = \"Incorrect file type. Required type: \" + allowed_types_formatted , ) return file return validate_file","title":"create_file_validator"},{"location":"llm_utils/aiweb_common/file_operations/file_handling.html#aiweb_common.file_operations.file_handling.file_to_base64","text":"Converts a file to a base64-encoded string. Source code in llm_utils/aiweb_common/file_operations/file_handling.py 12 13 14 15 def file_to_base64 ( filepath ): \"\"\"Converts a file to a base64-encoded string.\"\"\" with open ( filepath , \"rb\" ) as file : return base64 . b64encode ( file . read ()) . decode ( \"utf-8\" )","title":"file_to_base64"},{"location":"llm_utils/aiweb_common/file_operations/file_handling.html#aiweb_common.file_operations.file_handling.ingest_docx","text":"The function ingest_docx reads the content of a DOCX file as bytes, writes it to a temporary file, and then loads the document from the temporary file. :param file: The file parameter in the ingest_docx function seems to be a file-like object that supports asynchronous reading operations. When await file.read() is called, it reads the content of the file as bytes. This content is then written to a temporary file with a .docx :return: The function ingest_docx returns a tuple containing two values: 1. The name of the temporary file where the DOCX content was written. 2. An instance of the Document` class representing the loaded document from the temporary file. Source code in llm_utils/aiweb_common/file_operations/file_handling.py 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 async def ingest_docx ( file ): \"\"\" The function `ingest_docx` reads the content of a DOCX file as bytes, writes it to a temporary file, and then loads the document from the temporary file. :param file: The `file` parameter in the `ingest_docx` function seems to be a file-like object that supports asynchronous reading operations. When `await file.read()` is called, it reads the content of the file as bytes. This content is then written to a temporary file with a `.docx :return: The function `ingest_docx` returns a tuple containing two values: 1. The name of the temporary file where the DOCX content was written. 2. An instance of the `Document` class representing the loaded document from the temporary file. \"\"\" with tempfile . NamedTemporaryFile ( delete = False , suffix = \".docx\" ) as temp_doc : content = await file . read () # Read file content as bytes # have to do this because of async context, otherwise calling function moves on temp_doc . write ( content ) temp_doc . flush () # Ensure all content is written to disk # Load the document from the temporary file return temp_doc . name , Document ( temp_doc . name ) # Load the document here","title":"ingest_docx"},{"location":"llm_utils/aiweb_common/file_operations/file_handling.html#aiweb_common.file_operations.file_handling.ingest_docx_bytes","text":"The function ingest_docx_bytes reads the content of a DOCX file from bytes, saves it to a temporary file, and then loads the document using the Document class. :param content: The ingest_docx_bytes function you provided seems to be designed to ingest the content of a DOCX file as bytes and load it using the python-docx library. However, it looks like the content parameter is missing in your message. Could you please provide the content parameter so :return: The ingest_docx_bytes function returns a tuple containing two values: 1. The name of the temporary file where the content was written (temp_doc.name) 2. The Document object representing the content loaded from the temporary file Source code in llm_utils/aiweb_common/file_operations/file_handling.py 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 def ingest_docx_bytes ( content ): \"\"\" The function `ingest_docx_bytes` reads the content of a DOCX file from bytes, saves it to a temporary file, and then loads the document using the `Document` class. :param content: The `ingest_docx_bytes` function you provided seems to be designed to ingest the content of a DOCX file as bytes and load it using the `python-docx` library. However, it looks like the content parameter is missing in your message. Could you please provide the content parameter so :return: The `ingest_docx_bytes` function returns a tuple containing two values: 1. The name of the temporary file where the content was written (temp_doc.name) 2. The Document object representing the content loaded from the temporary file \"\"\" with tempfile . NamedTemporaryFile ( delete = False , suffix = \".docx\" ) as temp_doc : temp_doc . write ( content ) temp_doc . flush () # Ensure all content is written to disk # Load the document from the temporary file return temp_doc . name , Document ( temp_doc . name ) # Load the document here","title":"ingest_docx_bytes"},{"location":"llm_utils/aiweb_common/file_operations/file_handling.html#aiweb_common.file_operations.file_handling.markdown_to_docx_temporary_file","text":"The function prepare_docx_response converts Markdown content to a DOCX file and returns the temporary file path. :param content: The content parameter in the prepare_docx_response function is the text or data that you want to convert to a DOCX file. This content will be processed and converted into a DOCX file using the convert_markdown_docx function :param template_location: The prepare_docx_response function takes two parameters: :return: The function prepare_docx_response returns the file path of the temporary .docx file that is created after converting the provided content (in markdown format) to a .docx file using the specified template location. Source code in llm_utils/aiweb_common/file_operations/file_handling.py 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 def markdown_to_docx_temporary_file ( content , template_location = None ): \"\"\" The function `prepare_docx_response` converts Markdown content to a DOCX file and returns the temporary file path. :param content: The `content` parameter in the `prepare_docx_response` function is the text or data that you want to convert to a DOCX file. This content will be processed and converted into a DOCX file using the `convert_markdown_docx` function :param template_location: The `prepare_docx_response` function takes two parameters: :return: The function `prepare_docx_response` returns the file path of the temporary .docx file that is created after converting the provided content (in markdown format) to a .docx file using the specified template location. \"\"\" docx_data = convert_markdown_docx ( content , template_location ) # Using tempfile to save the output file temporarily with tempfile . NamedTemporaryFile ( delete = False , suffix = \".docx\" ) as temp_file : temp_file . write ( docx_data ) temp_file_path = temp_file . name return temp_file_path","title":"markdown_to_docx_temporary_file"},{"location":"llm_utils/aiweb_common/file_operations/file_handling.html#aiweb_common.file_operations.file_handling.validate_date","text":"Custom dependency that validates and parses a date string. Args: date_str (str): A date string in the YYYY-MM-DD format. Returns: datetime: The parsed datetime object. Raises: HTTPException: If the date string is not in the correct format. Source code in llm_utils/aiweb_common/file_operations/file_handling.py 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 def validate_date ( date_str : str = Query ( ... , description = \"The start date in YYYY-MM-DD format\" ) ) -> datetime : \"\"\" Custom dependency that validates and parses a date string. Args: date_str (str): A date string in the YYYY-MM-DD format. Returns: datetime: The parsed datetime object. Raises: HTTPException: If the date string is not in the correct format. \"\"\" try : return datetime . strptime ( date_str , \"%Y-%m- %d \" ) except ValueError as exc : raise HTTPException ( status_code = 400 , detail = \"start_date must be in YYYY-MM-DD format\" ) from exc","title":"validate_date"},{"location":"llm_utils/aiweb_common/file_operations/text_format.html","text":"","title":"Text Format"},{"location":"llm_utils/aiweb_common/file_operations/upload_manager.html","text":"BytesToDocx Bases: FastAPIUploadManager Source code in llm_utils/aiweb_common/file_operations/upload_manager.py 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 class BytesToDocx ( FastAPIUploadManager ): def __init__ ( self , background_tasks ): super () . __init__ ( background_tasks ) def process_file_bytes ( self , file : bytes , extension = \".docx\" ) -> Document : \"\"\" The function `process_file_bytes` processes a file in bytes format, specifically for a .docx extension, and returns a Document object while also handling background tasks. Args: file (bytes): The `file` parameter in the `process_file_bytes` method is expected to be a bytes object representing the content of a file. This method processes the file bytes, specifically for a `.docx` file. If the provided extension is not `.docx`, a `TypeError` is raised. extension: The `extension` parameter in the `process_file_bytes` method is used to specify the file extension that the method expects the input file to have. In this case, the default extension is set to \".docx\". If the provided extension does not match \".docx\", a TypeError is raised. Defaults to .docx Returns: The function `process_file_bytes` returns the content of a document (cv_in_docx) after processing a file given as bytes input. \"\"\" if extension != \".docx\" : raise TypeError cv_in_docx_filepath , cv_in_docx = ingest_docx_bytes ( file ) self . background_tasks . add_task ( os . unlink , cv_in_docx_filepath ) return cv_in_docx process_file_bytes ( file , extension = '.docx' ) The function process_file_bytes processes a file in bytes format, specifically for a .docx extension, and returns a Document object while also handling background tasks. Parameters: file ( bytes ) \u2013 The file parameter in the process_file_bytes method is expected to be a bytes object representing the content of a file. This method processes the file bytes, specifically for a .docx file. If the provided extension is not .docx , a TypeError is raised. extension: The extension parameter in the process_file_bytes method is used to specify the file extension that the method expects the input file to have. In this case, the default extension is set to \".docx\". If the provided extension does not match \".docx\", a TypeError is raised. Defaults to .docx Returns: Document \u2013 The function process_file_bytes returns the content of a document (cv_in_docx) after processing a file given as bytes input. Source code in llm_utils/aiweb_common/file_operations/upload_manager.py 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 def process_file_bytes ( self , file : bytes , extension = \".docx\" ) -> Document : \"\"\" The function `process_file_bytes` processes a file in bytes format, specifically for a .docx extension, and returns a Document object while also handling background tasks. Args: file (bytes): The `file` parameter in the `process_file_bytes` method is expected to be a bytes object representing the content of a file. This method processes the file bytes, specifically for a `.docx` file. If the provided extension is not `.docx`, a `TypeError` is raised. extension: The `extension` parameter in the `process_file_bytes` method is used to specify the file extension that the method expects the input file to have. In this case, the default extension is set to \".docx\". If the provided extension does not match \".docx\", a TypeError is raised. Defaults to .docx Returns: The function `process_file_bytes` returns the content of a document (cv_in_docx) after processing a file given as bytes input. \"\"\" if extension != \".docx\" : raise TypeError cv_in_docx_filepath , cv_in_docx = ingest_docx_bytes ( file ) self . background_tasks . add_task ( os . unlink , cv_in_docx_filepath ) return cv_in_docx FastAPIUploadManager Bases: UploadManager Source code in llm_utils/aiweb_common/file_operations/upload_manager.py 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 class FastAPIUploadManager ( UploadManager ): def __init__ ( self , background_tasks : BackgroundTasks ): self . background_tasks = background_tasks def process_file_bytes ( self , file : bytes , extension : str ) -> Union [ pd . DataFrame , str ]: \"\"\" Reads the file from byte string based on the file extension and returns either a DataFrame or a markdown string. Args: file_bytes (bytes): The byte-encoded content of the file. extension (str): The file extension indicating the file type. Returns: Union[pd.DataFrame, str]: Depending on the file extension, either returns a DataFrame for Excel files or a markdown string for DOCX and TXT files. \"\"\" print ( \"Processing file with extension - \" , extension ) if extension == \".xlsx\" : print ( \"Opening Excel file\" ) return pd . read_excel ( BytesIO ( file )) elif extension == \".csv\" : return pd . read_csv ( BytesIO ( file )) elif extension == \".txt\" : print ( \"Reading text file\" ) return file . decode ( \"utf-8\" ) elif extension == \".docx\" : doc = Document ( BytesIO ( file )) text = \"\" for paragraph in doc . paragraphs : text += paragraph . text + \" \\n \" return text elif extension == \".pdf\" : return self . read_pdf ( BytesIO ( file ), self . document_analysis_client ) else : print ( \"Converting file to Markdown\" ) with tempfile . NamedTemporaryFile ( delete = True , suffix = extension ) as tmpfile : tmpfile . write ( file ) tmpfile . seek ( 0 ) return pypandoc . convert_file ( tmpfile . name , \"markdown\" ) def read_and_validate_file ( self , encoded_file : str , extension : str ) -> Any : \"\"\" The function reads and validates a base64 encoded file, then processes it based on the specified file extension. Args: encoded_file (str): The `encoded_file` parameter is a string that represents the file content encoded in base64 format. This function reads and validates the file content by decoding it from base64 and then passing it to the `read_file` method for further processing based on the specified file extension. If any errors occur during extension (str): Extension refers to the file format or type of the file being processed. It is typically represented by a file extension such as \".txt\", \".pdf\", \".jpg\", etc. In the context of the provided code snippet, the extension parameter is used to specify the file format of the decoded file before further Returns: The `read_and_validate_file` method returns the output of the `read_file` method if it is not None. If the `read_file` method returns None, a HTTPException with status code 422 and detail \"Failed to process the file\" is raised. If any other exception occurs during the process, a HTTPException with status code 500 and the exception message is raised. \"\"\" try : file_bytes = base64 . b64decode ( encoded_file ) output = self . process_file_bytes ( file_bytes , extension ) if output is None : raise HTTPException ( status_code = 422 , detail = \"Failed to process the file\" ) return output except Exception as e : raise HTTPException ( status_code = 500 , detail = str ( e )) from e process_file_bytes ( file , extension ) Reads the file from byte string based on the file extension and returns either a DataFrame or a markdown string. Parameters: file_bytes ( bytes ) \u2013 The byte-encoded content of the file. extension ( str ) \u2013 The file extension indicating the file type. Returns: Union [ DataFrame , str ] \u2013 Union[pd.DataFrame, str]: Depending on the file extension, either returns a DataFrame for Excel files Union [ DataFrame , str ] \u2013 or a markdown string for DOCX and TXT files. Source code in llm_utils/aiweb_common/file_operations/upload_manager.py 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 def process_file_bytes ( self , file : bytes , extension : str ) -> Union [ pd . DataFrame , str ]: \"\"\" Reads the file from byte string based on the file extension and returns either a DataFrame or a markdown string. Args: file_bytes (bytes): The byte-encoded content of the file. extension (str): The file extension indicating the file type. Returns: Union[pd.DataFrame, str]: Depending on the file extension, either returns a DataFrame for Excel files or a markdown string for DOCX and TXT files. \"\"\" print ( \"Processing file with extension - \" , extension ) if extension == \".xlsx\" : print ( \"Opening Excel file\" ) return pd . read_excel ( BytesIO ( file )) elif extension == \".csv\" : return pd . read_csv ( BytesIO ( file )) elif extension == \".txt\" : print ( \"Reading text file\" ) return file . decode ( \"utf-8\" ) elif extension == \".docx\" : doc = Document ( BytesIO ( file )) text = \"\" for paragraph in doc . paragraphs : text += paragraph . text + \" \\n \" return text elif extension == \".pdf\" : return self . read_pdf ( BytesIO ( file ), self . document_analysis_client ) else : print ( \"Converting file to Markdown\" ) with tempfile . NamedTemporaryFile ( delete = True , suffix = extension ) as tmpfile : tmpfile . write ( file ) tmpfile . seek ( 0 ) return pypandoc . convert_file ( tmpfile . name , \"markdown\" ) read_and_validate_file ( encoded_file , extension ) The function reads and validates a base64 encoded file, then processes it based on the specified file extension. Parameters: encoded_file ( str ) \u2013 The encoded_file parameter is a string that represents the file content encoded in base64 format. This function reads and validates the file content by decoding it from base64 and then passing it to the read_file method for further processing based on the specified file extension. If any errors occur during extension (str): Extension refers to the file format or type of the file being processed. It is typically represented by a file extension such as \".txt\", \".pdf\", \".jpg\", etc. In the context of the provided code snippet, the extension parameter is used to specify the file format of the decoded file before further Returns: Any \u2013 The read_and_validate_file method returns the output of the read_file method if it is not None. If the read_file method returns None, a HTTPException with status code 422 and detail \"Failed to process the file\" is raised. If any other exception occurs during the process, a HTTPException with status code 500 and the exception message is raised. Source code in llm_utils/aiweb_common/file_operations/upload_manager.py 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 def read_and_validate_file ( self , encoded_file : str , extension : str ) -> Any : \"\"\" The function reads and validates a base64 encoded file, then processes it based on the specified file extension. Args: encoded_file (str): The `encoded_file` parameter is a string that represents the file content encoded in base64 format. This function reads and validates the file content by decoding it from base64 and then passing it to the `read_file` method for further processing based on the specified file extension. If any errors occur during extension (str): Extension refers to the file format or type of the file being processed. It is typically represented by a file extension such as \".txt\", \".pdf\", \".jpg\", etc. In the context of the provided code snippet, the extension parameter is used to specify the file format of the decoded file before further Returns: The `read_and_validate_file` method returns the output of the `read_file` method if it is not None. If the `read_file` method returns None, a HTTPException with status code 422 and detail \"Failed to process the file\" is raised. If any other exception occurs during the process, a HTTPException with status code 500 and the exception message is raised. \"\"\" try : file_bytes = base64 . b64decode ( encoded_file ) output = self . process_file_bytes ( file_bytes , extension ) if output is None : raise HTTPException ( status_code = 422 , detail = \"Failed to process the file\" ) return output except Exception as e : raise HTTPException ( status_code = 500 , detail = str ( e )) from e StreamlitUploadManager Bases: UploadManager Source code in llm_utils/aiweb_common/file_operations/upload_manager.py 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 class StreamlitUploadManager ( UploadManager ): def __init__ ( self , file = None , message : str = \"Please upload a file\" , file_types : list = None , accept_multiple_files : bool = False , document_analysis_client = None , ): \"\"\" Allows either an already-uploaded file (passed via `file`) or performs an interactive upload. Args: file: (Optional) an already-uploaded file object. message: The label for the uploader widget. file_types: List of allowed file extensions (default list if None). accept_multiple_files: Whether to allow multiple file uploads. document_analysis_client: (Optional) any additional client if needed. \"\"\" self . file = file self . message = message self . file_types = file_types if file_types is not None else [ \"csv\" , \"xlsx\" , \"docx\" , \"pdf\" , \"txt\" ] self . accept_multiple_files = accept_multiple_files self . document_analysis_client = document_analysis_client def process_upload ( self ): \"\"\" If no file has been provided during initialization, show the file uploader. Then, process the file based on its extension. Returns a tuple (processed_file, extension) or (None, None) if no file is provided. \"\"\" # If no file was provided externally then invoke the uploader. if self . file is None : self . file = st . file_uploader ( label = self . message , type = self . file_types , accept_multiple_files = self . accept_multiple_files ) if not self . file : st . write ( \"Please upload a file to continue...\" ) return None , None # Call read_file on the provided file. extension = Path ( self . file . name ) . suffix return self . read_file ( self . file , extension = extension ) def upload_file ( self ): \"\"\" Wraps process_upload for backward compatibility. You can choose your naming. \"\"\" return self . process_upload () def read_file ( self , file , extension ): # print(\"Extension - \", extension) if extension == \".xlsx\" : return pd . read_excel ( file ), extension elif extension == \".docx\" : doc = Document ( file ) text = \"\" for paragraph in doc . paragraphs : text += paragraph . text + \" \\n \" return text , extension elif extension == \".csv\" : return pd . read_csv ( file ), extension elif extension == \".pdf\" : return self . read_pdf ( file , self . document_analysis_client ), extension else : return None , None __init__ ( file = None , message = 'Please upload a file' , file_types = None , accept_multiple_files = False , document_analysis_client = None ) Allows either an already-uploaded file (passed via file ) or performs an interactive upload. Parameters: file \u2013 (Optional) an already-uploaded file object. message ( str , default: 'Please upload a file' ) \u2013 The label for the uploader widget. file_types ( list , default: None ) \u2013 List of allowed file extensions (default list if None). accept_multiple_files ( bool , default: False ) \u2013 Whether to allow multiple file uploads. document_analysis_client \u2013 (Optional) any additional client if needed. Source code in llm_utils/aiweb_common/file_operations/upload_manager.py 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 def __init__ ( self , file = None , message : str = \"Please upload a file\" , file_types : list = None , accept_multiple_files : bool = False , document_analysis_client = None , ): \"\"\" Allows either an already-uploaded file (passed via `file`) or performs an interactive upload. Args: file: (Optional) an already-uploaded file object. message: The label for the uploader widget. file_types: List of allowed file extensions (default list if None). accept_multiple_files: Whether to allow multiple file uploads. document_analysis_client: (Optional) any additional client if needed. \"\"\" self . file = file self . message = message self . file_types = file_types if file_types is not None else [ \"csv\" , \"xlsx\" , \"docx\" , \"pdf\" , \"txt\" ] self . accept_multiple_files = accept_multiple_files self . document_analysis_client = document_analysis_client process_upload () If no file has been provided during initialization, show the file uploader. Then, process the file based on its extension. Returns a tuple (processed_file, extension) or (None, None) if no file is provided. Source code in llm_utils/aiweb_common/file_operations/upload_manager.py 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 def process_upload ( self ): \"\"\" If no file has been provided during initialization, show the file uploader. Then, process the file based on its extension. Returns a tuple (processed_file, extension) or (None, None) if no file is provided. \"\"\" # If no file was provided externally then invoke the uploader. if self . file is None : self . file = st . file_uploader ( label = self . message , type = self . file_types , accept_multiple_files = self . accept_multiple_files ) if not self . file : st . write ( \"Please upload a file to continue...\" ) return None , None # Call read_file on the provided file. extension = Path ( self . file . name ) . suffix return self . read_file ( self . file , extension = extension ) upload_file () Wraps process_upload for backward compatibility. You can choose your naming. Source code in llm_utils/aiweb_common/file_operations/upload_manager.py 79 80 81 82 83 def upload_file ( self ): \"\"\" Wraps process_upload for backward compatibility. You can choose your naming. \"\"\" return self . process_upload ()","title":"Upload Manager"},{"location":"llm_utils/aiweb_common/file_operations/upload_manager.html#aiweb_common.file_operations.upload_manager.BytesToDocx","text":"Bases: FastAPIUploadManager Source code in llm_utils/aiweb_common/file_operations/upload_manager.py 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 class BytesToDocx ( FastAPIUploadManager ): def __init__ ( self , background_tasks ): super () . __init__ ( background_tasks ) def process_file_bytes ( self , file : bytes , extension = \".docx\" ) -> Document : \"\"\" The function `process_file_bytes` processes a file in bytes format, specifically for a .docx extension, and returns a Document object while also handling background tasks. Args: file (bytes): The `file` parameter in the `process_file_bytes` method is expected to be a bytes object representing the content of a file. This method processes the file bytes, specifically for a `.docx` file. If the provided extension is not `.docx`, a `TypeError` is raised. extension: The `extension` parameter in the `process_file_bytes` method is used to specify the file extension that the method expects the input file to have. In this case, the default extension is set to \".docx\". If the provided extension does not match \".docx\", a TypeError is raised. Defaults to .docx Returns: The function `process_file_bytes` returns the content of a document (cv_in_docx) after processing a file given as bytes input. \"\"\" if extension != \".docx\" : raise TypeError cv_in_docx_filepath , cv_in_docx = ingest_docx_bytes ( file ) self . background_tasks . add_task ( os . unlink , cv_in_docx_filepath ) return cv_in_docx","title":"BytesToDocx"},{"location":"llm_utils/aiweb_common/file_operations/upload_manager.html#aiweb_common.file_operations.upload_manager.BytesToDocx.process_file_bytes","text":"The function process_file_bytes processes a file in bytes format, specifically for a .docx extension, and returns a Document object while also handling background tasks. Parameters: file ( bytes ) \u2013 The file parameter in the process_file_bytes method is expected to be a bytes object representing the content of a file. This method processes the file bytes, specifically for a .docx file. If the provided extension is not .docx , a TypeError is raised. extension: The extension parameter in the process_file_bytes method is used to specify the file extension that the method expects the input file to have. In this case, the default extension is set to \".docx\". If the provided extension does not match \".docx\", a TypeError is raised. Defaults to .docx Returns: Document \u2013 The function process_file_bytes returns the content of a document (cv_in_docx) after processing a file given as bytes input. Source code in llm_utils/aiweb_common/file_operations/upload_manager.py 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 def process_file_bytes ( self , file : bytes , extension = \".docx\" ) -> Document : \"\"\" The function `process_file_bytes` processes a file in bytes format, specifically for a .docx extension, and returns a Document object while also handling background tasks. Args: file (bytes): The `file` parameter in the `process_file_bytes` method is expected to be a bytes object representing the content of a file. This method processes the file bytes, specifically for a `.docx` file. If the provided extension is not `.docx`, a `TypeError` is raised. extension: The `extension` parameter in the `process_file_bytes` method is used to specify the file extension that the method expects the input file to have. In this case, the default extension is set to \".docx\". If the provided extension does not match \".docx\", a TypeError is raised. Defaults to .docx Returns: The function `process_file_bytes` returns the content of a document (cv_in_docx) after processing a file given as bytes input. \"\"\" if extension != \".docx\" : raise TypeError cv_in_docx_filepath , cv_in_docx = ingest_docx_bytes ( file ) self . background_tasks . add_task ( os . unlink , cv_in_docx_filepath ) return cv_in_docx","title":"process_file_bytes"},{"location":"llm_utils/aiweb_common/file_operations/upload_manager.html#aiweb_common.file_operations.upload_manager.FastAPIUploadManager","text":"Bases: UploadManager Source code in llm_utils/aiweb_common/file_operations/upload_manager.py 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 class FastAPIUploadManager ( UploadManager ): def __init__ ( self , background_tasks : BackgroundTasks ): self . background_tasks = background_tasks def process_file_bytes ( self , file : bytes , extension : str ) -> Union [ pd . DataFrame , str ]: \"\"\" Reads the file from byte string based on the file extension and returns either a DataFrame or a markdown string. Args: file_bytes (bytes): The byte-encoded content of the file. extension (str): The file extension indicating the file type. Returns: Union[pd.DataFrame, str]: Depending on the file extension, either returns a DataFrame for Excel files or a markdown string for DOCX and TXT files. \"\"\" print ( \"Processing file with extension - \" , extension ) if extension == \".xlsx\" : print ( \"Opening Excel file\" ) return pd . read_excel ( BytesIO ( file )) elif extension == \".csv\" : return pd . read_csv ( BytesIO ( file )) elif extension == \".txt\" : print ( \"Reading text file\" ) return file . decode ( \"utf-8\" ) elif extension == \".docx\" : doc = Document ( BytesIO ( file )) text = \"\" for paragraph in doc . paragraphs : text += paragraph . text + \" \\n \" return text elif extension == \".pdf\" : return self . read_pdf ( BytesIO ( file ), self . document_analysis_client ) else : print ( \"Converting file to Markdown\" ) with tempfile . NamedTemporaryFile ( delete = True , suffix = extension ) as tmpfile : tmpfile . write ( file ) tmpfile . seek ( 0 ) return pypandoc . convert_file ( tmpfile . name , \"markdown\" ) def read_and_validate_file ( self , encoded_file : str , extension : str ) -> Any : \"\"\" The function reads and validates a base64 encoded file, then processes it based on the specified file extension. Args: encoded_file (str): The `encoded_file` parameter is a string that represents the file content encoded in base64 format. This function reads and validates the file content by decoding it from base64 and then passing it to the `read_file` method for further processing based on the specified file extension. If any errors occur during extension (str): Extension refers to the file format or type of the file being processed. It is typically represented by a file extension such as \".txt\", \".pdf\", \".jpg\", etc. In the context of the provided code snippet, the extension parameter is used to specify the file format of the decoded file before further Returns: The `read_and_validate_file` method returns the output of the `read_file` method if it is not None. If the `read_file` method returns None, a HTTPException with status code 422 and detail \"Failed to process the file\" is raised. If any other exception occurs during the process, a HTTPException with status code 500 and the exception message is raised. \"\"\" try : file_bytes = base64 . b64decode ( encoded_file ) output = self . process_file_bytes ( file_bytes , extension ) if output is None : raise HTTPException ( status_code = 422 , detail = \"Failed to process the file\" ) return output except Exception as e : raise HTTPException ( status_code = 500 , detail = str ( e )) from e","title":"FastAPIUploadManager"},{"location":"llm_utils/aiweb_common/file_operations/upload_manager.html#aiweb_common.file_operations.upload_manager.FastAPIUploadManager.process_file_bytes","text":"Reads the file from byte string based on the file extension and returns either a DataFrame or a markdown string. Parameters: file_bytes ( bytes ) \u2013 The byte-encoded content of the file. extension ( str ) \u2013 The file extension indicating the file type. Returns: Union [ DataFrame , str ] \u2013 Union[pd.DataFrame, str]: Depending on the file extension, either returns a DataFrame for Excel files Union [ DataFrame , str ] \u2013 or a markdown string for DOCX and TXT files. Source code in llm_utils/aiweb_common/file_operations/upload_manager.py 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 def process_file_bytes ( self , file : bytes , extension : str ) -> Union [ pd . DataFrame , str ]: \"\"\" Reads the file from byte string based on the file extension and returns either a DataFrame or a markdown string. Args: file_bytes (bytes): The byte-encoded content of the file. extension (str): The file extension indicating the file type. Returns: Union[pd.DataFrame, str]: Depending on the file extension, either returns a DataFrame for Excel files or a markdown string for DOCX and TXT files. \"\"\" print ( \"Processing file with extension - \" , extension ) if extension == \".xlsx\" : print ( \"Opening Excel file\" ) return pd . read_excel ( BytesIO ( file )) elif extension == \".csv\" : return pd . read_csv ( BytesIO ( file )) elif extension == \".txt\" : print ( \"Reading text file\" ) return file . decode ( \"utf-8\" ) elif extension == \".docx\" : doc = Document ( BytesIO ( file )) text = \"\" for paragraph in doc . paragraphs : text += paragraph . text + \" \\n \" return text elif extension == \".pdf\" : return self . read_pdf ( BytesIO ( file ), self . document_analysis_client ) else : print ( \"Converting file to Markdown\" ) with tempfile . NamedTemporaryFile ( delete = True , suffix = extension ) as tmpfile : tmpfile . write ( file ) tmpfile . seek ( 0 ) return pypandoc . convert_file ( tmpfile . name , \"markdown\" )","title":"process_file_bytes"},{"location":"llm_utils/aiweb_common/file_operations/upload_manager.html#aiweb_common.file_operations.upload_manager.FastAPIUploadManager.read_and_validate_file","text":"The function reads and validates a base64 encoded file, then processes it based on the specified file extension. Parameters: encoded_file ( str ) \u2013 The encoded_file parameter is a string that represents the file content encoded in base64 format. This function reads and validates the file content by decoding it from base64 and then passing it to the read_file method for further processing based on the specified file extension. If any errors occur during extension (str): Extension refers to the file format or type of the file being processed. It is typically represented by a file extension such as \".txt\", \".pdf\", \".jpg\", etc. In the context of the provided code snippet, the extension parameter is used to specify the file format of the decoded file before further Returns: Any \u2013 The read_and_validate_file method returns the output of the read_file method if it is not None. If the read_file method returns None, a HTTPException with status code 422 and detail \"Failed to process the file\" is raised. If any other exception occurs during the process, a HTTPException with status code 500 and the exception message is raised. Source code in llm_utils/aiweb_common/file_operations/upload_manager.py 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 def read_and_validate_file ( self , encoded_file : str , extension : str ) -> Any : \"\"\" The function reads and validates a base64 encoded file, then processes it based on the specified file extension. Args: encoded_file (str): The `encoded_file` parameter is a string that represents the file content encoded in base64 format. This function reads and validates the file content by decoding it from base64 and then passing it to the `read_file` method for further processing based on the specified file extension. If any errors occur during extension (str): Extension refers to the file format or type of the file being processed. It is typically represented by a file extension such as \".txt\", \".pdf\", \".jpg\", etc. In the context of the provided code snippet, the extension parameter is used to specify the file format of the decoded file before further Returns: The `read_and_validate_file` method returns the output of the `read_file` method if it is not None. If the `read_file` method returns None, a HTTPException with status code 422 and detail \"Failed to process the file\" is raised. If any other exception occurs during the process, a HTTPException with status code 500 and the exception message is raised. \"\"\" try : file_bytes = base64 . b64decode ( encoded_file ) output = self . process_file_bytes ( file_bytes , extension ) if output is None : raise HTTPException ( status_code = 422 , detail = \"Failed to process the file\" ) return output except Exception as e : raise HTTPException ( status_code = 500 , detail = str ( e )) from e","title":"read_and_validate_file"},{"location":"llm_utils/aiweb_common/file_operations/upload_manager.html#aiweb_common.file_operations.upload_manager.StreamlitUploadManager","text":"Bases: UploadManager Source code in llm_utils/aiweb_common/file_operations/upload_manager.py 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 class StreamlitUploadManager ( UploadManager ): def __init__ ( self , file = None , message : str = \"Please upload a file\" , file_types : list = None , accept_multiple_files : bool = False , document_analysis_client = None , ): \"\"\" Allows either an already-uploaded file (passed via `file`) or performs an interactive upload. Args: file: (Optional) an already-uploaded file object. message: The label for the uploader widget. file_types: List of allowed file extensions (default list if None). accept_multiple_files: Whether to allow multiple file uploads. document_analysis_client: (Optional) any additional client if needed. \"\"\" self . file = file self . message = message self . file_types = file_types if file_types is not None else [ \"csv\" , \"xlsx\" , \"docx\" , \"pdf\" , \"txt\" ] self . accept_multiple_files = accept_multiple_files self . document_analysis_client = document_analysis_client def process_upload ( self ): \"\"\" If no file has been provided during initialization, show the file uploader. Then, process the file based on its extension. Returns a tuple (processed_file, extension) or (None, None) if no file is provided. \"\"\" # If no file was provided externally then invoke the uploader. if self . file is None : self . file = st . file_uploader ( label = self . message , type = self . file_types , accept_multiple_files = self . accept_multiple_files ) if not self . file : st . write ( \"Please upload a file to continue...\" ) return None , None # Call read_file on the provided file. extension = Path ( self . file . name ) . suffix return self . read_file ( self . file , extension = extension ) def upload_file ( self ): \"\"\" Wraps process_upload for backward compatibility. You can choose your naming. \"\"\" return self . process_upload () def read_file ( self , file , extension ): # print(\"Extension - \", extension) if extension == \".xlsx\" : return pd . read_excel ( file ), extension elif extension == \".docx\" : doc = Document ( file ) text = \"\" for paragraph in doc . paragraphs : text += paragraph . text + \" \\n \" return text , extension elif extension == \".csv\" : return pd . read_csv ( file ), extension elif extension == \".pdf\" : return self . read_pdf ( file , self . document_analysis_client ), extension else : return None , None","title":"StreamlitUploadManager"},{"location":"llm_utils/aiweb_common/file_operations/upload_manager.html#aiweb_common.file_operations.upload_manager.StreamlitUploadManager.__init__","text":"Allows either an already-uploaded file (passed via file ) or performs an interactive upload. Parameters: file \u2013 (Optional) an already-uploaded file object. message ( str , default: 'Please upload a file' ) \u2013 The label for the uploader widget. file_types ( list , default: None ) \u2013 List of allowed file extensions (default list if None). accept_multiple_files ( bool , default: False ) \u2013 Whether to allow multiple file uploads. document_analysis_client \u2013 (Optional) any additional client if needed. Source code in llm_utils/aiweb_common/file_operations/upload_manager.py 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 def __init__ ( self , file = None , message : str = \"Please upload a file\" , file_types : list = None , accept_multiple_files : bool = False , document_analysis_client = None , ): \"\"\" Allows either an already-uploaded file (passed via `file`) or performs an interactive upload. Args: file: (Optional) an already-uploaded file object. message: The label for the uploader widget. file_types: List of allowed file extensions (default list if None). accept_multiple_files: Whether to allow multiple file uploads. document_analysis_client: (Optional) any additional client if needed. \"\"\" self . file = file self . message = message self . file_types = file_types if file_types is not None else [ \"csv\" , \"xlsx\" , \"docx\" , \"pdf\" , \"txt\" ] self . accept_multiple_files = accept_multiple_files self . document_analysis_client = document_analysis_client","title":"__init__"},{"location":"llm_utils/aiweb_common/file_operations/upload_manager.html#aiweb_common.file_operations.upload_manager.StreamlitUploadManager.process_upload","text":"If no file has been provided during initialization, show the file uploader. Then, process the file based on its extension. Returns a tuple (processed_file, extension) or (None, None) if no file is provided. Source code in llm_utils/aiweb_common/file_operations/upload_manager.py 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 def process_upload ( self ): \"\"\" If no file has been provided during initialization, show the file uploader. Then, process the file based on its extension. Returns a tuple (processed_file, extension) or (None, None) if no file is provided. \"\"\" # If no file was provided externally then invoke the uploader. if self . file is None : self . file = st . file_uploader ( label = self . message , type = self . file_types , accept_multiple_files = self . accept_multiple_files ) if not self . file : st . write ( \"Please upload a file to continue...\" ) return None , None # Call read_file on the provided file. extension = Path ( self . file . name ) . suffix return self . read_file ( self . file , extension = extension )","title":"process_upload"},{"location":"llm_utils/aiweb_common/file_operations/upload_manager.html#aiweb_common.file_operations.upload_manager.StreamlitUploadManager.upload_file","text":"Wraps process_upload for backward compatibility. You can choose your naming. Source code in llm_utils/aiweb_common/file_operations/upload_manager.py 79 80 81 82 83 def upload_file ( self ): \"\"\" Wraps process_upload for backward compatibility. You can choose your naming. \"\"\" return self . process_upload ()","title":"upload_file"},{"location":"llm_utils/aiweb_common/generate/AugmentedResponse.html","text":"","title":"AugmentedResponse"},{"location":"llm_utils/aiweb_common/generate/AugmentedServicer.html","text":"","title":"AugmentedServicer"},{"location":"llm_utils/aiweb_common/generate/ChatResponse.html","text":"","title":"ChatResponse"},{"location":"llm_utils/aiweb_common/generate/ChatSchemas.html","text":"","title":"ChatSchemas"},{"location":"llm_utils/aiweb_common/generate/ChatServicer.html","text":"","title":"ChatServicer"},{"location":"llm_utils/aiweb_common/generate/PromptAssembler.html","text":"","title":"PromptAssembler"},{"location":"llm_utils/aiweb_common/generate/PromptyResponseHandler.html","text":"","title":"PromptyResponse"},{"location":"llm_utils/aiweb_common/generate/PromptyServicer.html","text":"","title":"PromptyServicer"},{"location":"llm_utils/aiweb_common/generate/QueryInterface.html","text":"","title":"QueryInterface"},{"location":"llm_utils/aiweb_common/generate/Response.html","text":"","title":"Response"},{"location":"llm_utils/aiweb_common/generate/SingleResponse.html","text":"","title":"SingleResponse"},{"location":"llm_utils/aiweb_common/generate/SingleResponseServicer.html","text":"","title":"SingleResponseServicer"},{"location":"llm_utils/aiweb_common/resource/NIHRePORTERInterface.html","text":"","title":"NIH RePORTER Interface"},{"location":"llm_utils/aiweb_common/resource/PubMedInterface.html","text":"PubMedInterface Source code in llm_utils/aiweb_common/resource/PubMedInterface.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 class PubMedInterface : def __init__ ( self , email = \"rmelvin@uabmc.edu\" , max_results = 50 , streamlit_context = False , max_retries = 3 , delay_seconds = 5 , ): self . email = email self . max_results = max_results self . streamlit_context = streamlit_context self . max_retries = max_retries self . delay_seconds = delay_seconds Entrez . email = email def _format_authors ( self ): \"\"\" The function `format_authors` takes a list of strings representing authors and returns a formatted string of their last names followed by initials, following APA rules. Args: author_list: A list of strings, where each string represents an author in the format \"Last Name Initials\". Returns: a formatted string of authors' names in the format \"Last Name, Initials,\" following APA rules. \"\"\" formatted_authors = [] num_authors = len ( self . _authors ) if num_authors <= 20 : # Normal case, just list all authors for author in self . _authors : * last_name , initials = author . rsplit ( \" \" , 1 ) last_name = \" \" . join ( last_name ) formatted_authors . append ( f \" { last_name } , { initials } .\" ) return \", \" . join ( formatted_authors ) else : # APA rule for > 20 authors: first 19, ellipsis, last author for author in self . _authors [: 19 ]: * last_name , initials = author . rsplit ( \" \" , 1 ) last_name = \" \" . join ( last_name ) formatted_authors . append ( f \" { last_name } , { initials } .\" ) last_author = self . _authors [ - 1 ] last_author_name , last_author_initials = last_author . rsplit ( \" \" , 1 ) formatted_authors . append ( \"\u2026\" ) formatted_authors . append ( f \" { last_author_name } , { last_author_initials } .\" ) return \", \" . join ( formatted_authors ) def _format_apa_citation ( self ): \"\"\" The function `format_apa_citation` takes in an article and its ID and returns a formatted APA citation string. Args: article: The `article` parameter is a dictionary that contains information about a specific article. It should have the following structure: article_id: The article_id parameter is the unique identifier for the article. It is used to include the PMID (PubMed ID) in the APA citation format. Returns: a formatted APA citation for an article, including the authors, publication year, title, journal, volume, pages, and PMID (PubMed ID). \"\"\" try : authors = self . _format_authors () except KeyError : authors = \"\" return f \" { authors } ( { self . _pub_month } ). { self . _title } { self . _journal } , { self . _volume } , { self . _pages } . PMID: { self . _pmid } \" def _extract_record_data ( self , record ): # Extract the desired information self . _title = record . get ( \"TI\" , \"No title available\" ) self . _keywords = record . get ( \"OT\" , []) # OT might not be present in all records # try to use mesh headers if keywords not present if not self . _keywords : self . _keywords = record . get ( \"MH\" , []) self . _abstract = record . get ( \"AB\" , \"No abstract available\" ) self . _pmid = record . get ( \"PMID\" , \"No PMID available\" ) self . _pub_month = record . get ( \"DP\" , \"No date available\" ) self . _authors = record . get ( \"AU\" , []) self . _journal = record . get ( \"JT\" , \"No jounral name available\" ) self . _volume = record . get ( \"VI\" , \"No volume available\" ) self . _pages = record . get ( \"PG\" , \"No pages available\" ) def search_pubmed_articles ( self , query ): \"\"\" The function `search_pubmed_articles` takes a PubMed search string, an email address, and an optional maximum number of results, and returns a list of PubMed article IDs that match the search criteria. Args: query: The search query string for PubMed. email: The email address associated with your NCBI account. max_results: Optional; maximum number of results to retrieve (default 10). streamlit_context: Optional; a boolean flag indicating whether the code is running within a Streamlit app (default False). max_retries: Optional; the maximum number of retry attempts if an HTTP error occurs (default 3). delay_seconds: Optional; the number of seconds to wait between retry attempts (default 5). Returns: A list of PubMed article IDs that match the search criteria. \"\"\" for attempt in range ( self . max_retries ): try : handle = Entrez . esearch ( db = \"pubmed\" , term = query , sort = \"relevance\" , retmax = self . max_results ) record = Entrez . read ( handle ) handle . close () return record [ \"IdList\" ] except HTTPError as e : error_message = f \"PubMed didn't respond (attempt { attempt + 1 } / { self . max_retries } ): { e } \" if attempt < self . max_retries : wait_message = f \"Waiting { self . delay_seconds } seconds before trying PubMed again...\" print ( error_message ) print ( wait_message ) if self . streamlit_context : st . warning ( error_message ) st . warning ( wait_message ) time . sleep ( self . delay_seconds ) else : final_message = \"Giving up on PubMed. It was an issue on their end. You may want to try again later.\" print ( error_message ) print ( final_message ) if self . streamlit_context : st . warning ( error_message ) st . warning ( final_message ) return [] def fetch_article_details ( self , pubmed_ids ): \"\"\" The function fetches article details from PubMed using the provided PubMed IDs. Args: pubmed_ids: A list of strings where each string represents the PubMed ID (PMID) of the article you want to fetch details for. max_retries: The maximum number of retry attempts if an HTTP error occurs. Default is 3. delay_seconds: The number of seconds to wait between retry attempts. Default is 5. streamlit_context: A boolean flag indicating whether the code is running within a Streamlit app. Default is False. Returns: A list of dictionaries, where each dictionary contains the details of an article with the given PubMed ID. \"\"\" pubmed_ids = [ str ( id ) for id in pubmed_ids ] ids_string = \",\" . join ( pubmed_ids ) for attempt in range ( self . max_retries + 1 ): try : handle = Entrez . efetch ( db = \"pubmed\" , id = ids_string , rettype = \"medline\" , retmode = \"text\" ) records = Medline . parse ( handle ) records = list ( records ) parsed_data = [] for record in records : self . _extract_record_data ( record ) citation = self . _format_apa_citation () parsed_data . append ( { \"date_published\" : self . _pub_month , \"title\" : self . _title , \"keywords\" : self . _keywords , \"abstract\" : self . _abstract , \"pmid\" : self . _pmid , \"authors\" : self . _authors , \"journal\" : self . _journal , \"citation\" : citation , } ) parsed_df = pd . DataFrame ( parsed_data ) return parsed_df except HTTPError as e : error_message = f \"PubMed didn't respond (attempt { attempt + 1 } / { self . max_retries } ): { e } \" if attempt < self . max_retries : wait_message = f \"Waiting { self . delay_seconds } seconds before trying PubMed again...\" print ( error_message ) print ( wait_message ) else : final_message = \"Giving up on PubMed. It was an issue on their end. You may want to try again later.\" print ( error_message ) print ( final_message ) return None def fetch_article_details_xml ( self , pubmed_ids ): \"\"\" The function fetches article details from PubMed using the provided PubMed IDs. Args: pubmed_ids: A list of strings where each string represents the PubMed ID (PMID) of the article you want to fetch details for. max_retries: The maximum number of retry attempts if an HTTP error occurs. Default is 3. delay_seconds: The number of seconds to wait between retry attempts. Default is 5. streamlit_context: A boolean flag indicating whether the code is running within a Streamlit app. Default is False. Returns: A list of dictionaries, where each dictionary contains the details of an article with the given PubMed ID. \"\"\" ids_string = \",\" . join ( pubmed_ids ) for attempt in range ( self . max_retries + 1 ): try : handle = Entrez . efetch ( db = \"pubmed\" , id = ids_string , retmode = \"xml\" ) articles = Entrez . read ( handle )[ \"PubmedArticle\" ] handle . close () return articles except HTTPError as e : error_message = f \"PubMed didn't respond (attempt { attempt + 1 } / { self . max_retries } ): { e } \" if attempt < self . max_retries : wait_message = f \"Waiting { self . delay_seconds } seconds before trying PubMed again...\" print ( error_message ) print ( wait_message ) if self . streamlit_context : # TODO Ask about these imports... st . warning ( error_message ) st . warning ( wait_message ) time . sleep ( self . delay_seconds ) else : final_message = \"Giving up on PubMed. It was an issue on their end. You may want to try again later.\" print ( error_message ) print ( final_message ) if self . streamlit_context : st . warning ( error_message ) st . error ( final_message ) return [] fetch_article_details ( pubmed_ids ) The function fetches article details from PubMed using the provided PubMed IDs. pubmed_ids: A list of strings where each string represents the PubMed ID (PMID) of the article you want to fetch details for. max_retries: The maximum number of retry attempts if an HTTP error occurs. Default is 3. delay_seconds: The number of seconds to wait between retry attempts. Default is 5. streamlit_context: A boolean flag indicating whether the code is running within a Streamlit app. Default is False. Returns: A list of dictionaries, where each dictionary contains the details of an article with the given PubMed ID. Source code in llm_utils/aiweb_common/resource/PubMedInterface.py 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 def fetch_article_details ( self , pubmed_ids ): \"\"\" The function fetches article details from PubMed using the provided PubMed IDs. Args: pubmed_ids: A list of strings where each string represents the PubMed ID (PMID) of the article you want to fetch details for. max_retries: The maximum number of retry attempts if an HTTP error occurs. Default is 3. delay_seconds: The number of seconds to wait between retry attempts. Default is 5. streamlit_context: A boolean flag indicating whether the code is running within a Streamlit app. Default is False. Returns: A list of dictionaries, where each dictionary contains the details of an article with the given PubMed ID. \"\"\" pubmed_ids = [ str ( id ) for id in pubmed_ids ] ids_string = \",\" . join ( pubmed_ids ) for attempt in range ( self . max_retries + 1 ): try : handle = Entrez . efetch ( db = \"pubmed\" , id = ids_string , rettype = \"medline\" , retmode = \"text\" ) records = Medline . parse ( handle ) records = list ( records ) parsed_data = [] for record in records : self . _extract_record_data ( record ) citation = self . _format_apa_citation () parsed_data . append ( { \"date_published\" : self . _pub_month , \"title\" : self . _title , \"keywords\" : self . _keywords , \"abstract\" : self . _abstract , \"pmid\" : self . _pmid , \"authors\" : self . _authors , \"journal\" : self . _journal , \"citation\" : citation , } ) parsed_df = pd . DataFrame ( parsed_data ) return parsed_df except HTTPError as e : error_message = f \"PubMed didn't respond (attempt { attempt + 1 } / { self . max_retries } ): { e } \" if attempt < self . max_retries : wait_message = f \"Waiting { self . delay_seconds } seconds before trying PubMed again...\" print ( error_message ) print ( wait_message ) else : final_message = \"Giving up on PubMed. It was an issue on their end. You may want to try again later.\" print ( error_message ) print ( final_message ) return None fetch_article_details_xml ( pubmed_ids ) The function fetches article details from PubMed using the provided PubMed IDs. pubmed_ids: A list of strings where each string represents the PubMed ID (PMID) of the article you want to fetch details for. max_retries: The maximum number of retry attempts if an HTTP error occurs. Default is 3. delay_seconds: The number of seconds to wait between retry attempts. Default is 5. streamlit_context: A boolean flag indicating whether the code is running within a Streamlit app. Default is False. Returns: A list of dictionaries, where each dictionary contains the details of an article with the given PubMed ID. Source code in llm_utils/aiweb_common/resource/PubMedInterface.py 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 def fetch_article_details_xml ( self , pubmed_ids ): \"\"\" The function fetches article details from PubMed using the provided PubMed IDs. Args: pubmed_ids: A list of strings where each string represents the PubMed ID (PMID) of the article you want to fetch details for. max_retries: The maximum number of retry attempts if an HTTP error occurs. Default is 3. delay_seconds: The number of seconds to wait between retry attempts. Default is 5. streamlit_context: A boolean flag indicating whether the code is running within a Streamlit app. Default is False. Returns: A list of dictionaries, where each dictionary contains the details of an article with the given PubMed ID. \"\"\" ids_string = \",\" . join ( pubmed_ids ) for attempt in range ( self . max_retries + 1 ): try : handle = Entrez . efetch ( db = \"pubmed\" , id = ids_string , retmode = \"xml\" ) articles = Entrez . read ( handle )[ \"PubmedArticle\" ] handle . close () return articles except HTTPError as e : error_message = f \"PubMed didn't respond (attempt { attempt + 1 } / { self . max_retries } ): { e } \" if attempt < self . max_retries : wait_message = f \"Waiting { self . delay_seconds } seconds before trying PubMed again...\" print ( error_message ) print ( wait_message ) if self . streamlit_context : # TODO Ask about these imports... st . warning ( error_message ) st . warning ( wait_message ) time . sleep ( self . delay_seconds ) else : final_message = \"Giving up on PubMed. It was an issue on their end. You may want to try again later.\" print ( error_message ) print ( final_message ) if self . streamlit_context : st . warning ( error_message ) st . error ( final_message ) return [] search_pubmed_articles ( query ) The function search_pubmed_articles takes a PubMed search string, an email address, and an optional maximum number of results, and returns a list of PubMed article IDs that match the search criteria. Args: query: The search query string for PubMed. email: The email address associated with your NCBI account. max_results: Optional; maximum number of results to retrieve (default 10). streamlit_context: Optional; a boolean flag indicating whether the code is running within a Streamlit app (default False). max_retries: Optional; the maximum number of retry attempts if an HTTP error occurs (default 3). delay_seconds: Optional; the number of seconds to wait between retry attempts (default 5). Returns: A list of PubMed article IDs that match the search criteria. Source code in llm_utils/aiweb_common/resource/PubMedInterface.py 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 def search_pubmed_articles ( self , query ): \"\"\" The function `search_pubmed_articles` takes a PubMed search string, an email address, and an optional maximum number of results, and returns a list of PubMed article IDs that match the search criteria. Args: query: The search query string for PubMed. email: The email address associated with your NCBI account. max_results: Optional; maximum number of results to retrieve (default 10). streamlit_context: Optional; a boolean flag indicating whether the code is running within a Streamlit app (default False). max_retries: Optional; the maximum number of retry attempts if an HTTP error occurs (default 3). delay_seconds: Optional; the number of seconds to wait between retry attempts (default 5). Returns: A list of PubMed article IDs that match the search criteria. \"\"\" for attempt in range ( self . max_retries ): try : handle = Entrez . esearch ( db = \"pubmed\" , term = query , sort = \"relevance\" , retmax = self . max_results ) record = Entrez . read ( handle ) handle . close () return record [ \"IdList\" ] except HTTPError as e : error_message = f \"PubMed didn't respond (attempt { attempt + 1 } / { self . max_retries } ): { e } \" if attempt < self . max_retries : wait_message = f \"Waiting { self . delay_seconds } seconds before trying PubMed again...\" print ( error_message ) print ( wait_message ) if self . streamlit_context : st . warning ( error_message ) st . warning ( wait_message ) time . sleep ( self . delay_seconds ) else : final_message = \"Giving up on PubMed. It was an issue on their end. You may want to try again later.\" print ( error_message ) print ( final_message ) if self . streamlit_context : st . warning ( error_message ) st . warning ( final_message ) return []","title":"PubMed Interface"},{"location":"llm_utils/aiweb_common/resource/PubMedInterface.html#aiweb_common.resource.PubMedInterface.PubMedInterface","text":"Source code in llm_utils/aiweb_common/resource/PubMedInterface.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 class PubMedInterface : def __init__ ( self , email = \"rmelvin@uabmc.edu\" , max_results = 50 , streamlit_context = False , max_retries = 3 , delay_seconds = 5 , ): self . email = email self . max_results = max_results self . streamlit_context = streamlit_context self . max_retries = max_retries self . delay_seconds = delay_seconds Entrez . email = email def _format_authors ( self ): \"\"\" The function `format_authors` takes a list of strings representing authors and returns a formatted string of their last names followed by initials, following APA rules. Args: author_list: A list of strings, where each string represents an author in the format \"Last Name Initials\". Returns: a formatted string of authors' names in the format \"Last Name, Initials,\" following APA rules. \"\"\" formatted_authors = [] num_authors = len ( self . _authors ) if num_authors <= 20 : # Normal case, just list all authors for author in self . _authors : * last_name , initials = author . rsplit ( \" \" , 1 ) last_name = \" \" . join ( last_name ) formatted_authors . append ( f \" { last_name } , { initials } .\" ) return \", \" . join ( formatted_authors ) else : # APA rule for > 20 authors: first 19, ellipsis, last author for author in self . _authors [: 19 ]: * last_name , initials = author . rsplit ( \" \" , 1 ) last_name = \" \" . join ( last_name ) formatted_authors . append ( f \" { last_name } , { initials } .\" ) last_author = self . _authors [ - 1 ] last_author_name , last_author_initials = last_author . rsplit ( \" \" , 1 ) formatted_authors . append ( \"\u2026\" ) formatted_authors . append ( f \" { last_author_name } , { last_author_initials } .\" ) return \", \" . join ( formatted_authors ) def _format_apa_citation ( self ): \"\"\" The function `format_apa_citation` takes in an article and its ID and returns a formatted APA citation string. Args: article: The `article` parameter is a dictionary that contains information about a specific article. It should have the following structure: article_id: The article_id parameter is the unique identifier for the article. It is used to include the PMID (PubMed ID) in the APA citation format. Returns: a formatted APA citation for an article, including the authors, publication year, title, journal, volume, pages, and PMID (PubMed ID). \"\"\" try : authors = self . _format_authors () except KeyError : authors = \"\" return f \" { authors } ( { self . _pub_month } ). { self . _title } { self . _journal } , { self . _volume } , { self . _pages } . PMID: { self . _pmid } \" def _extract_record_data ( self , record ): # Extract the desired information self . _title = record . get ( \"TI\" , \"No title available\" ) self . _keywords = record . get ( \"OT\" , []) # OT might not be present in all records # try to use mesh headers if keywords not present if not self . _keywords : self . _keywords = record . get ( \"MH\" , []) self . _abstract = record . get ( \"AB\" , \"No abstract available\" ) self . _pmid = record . get ( \"PMID\" , \"No PMID available\" ) self . _pub_month = record . get ( \"DP\" , \"No date available\" ) self . _authors = record . get ( \"AU\" , []) self . _journal = record . get ( \"JT\" , \"No jounral name available\" ) self . _volume = record . get ( \"VI\" , \"No volume available\" ) self . _pages = record . get ( \"PG\" , \"No pages available\" ) def search_pubmed_articles ( self , query ): \"\"\" The function `search_pubmed_articles` takes a PubMed search string, an email address, and an optional maximum number of results, and returns a list of PubMed article IDs that match the search criteria. Args: query: The search query string for PubMed. email: The email address associated with your NCBI account. max_results: Optional; maximum number of results to retrieve (default 10). streamlit_context: Optional; a boolean flag indicating whether the code is running within a Streamlit app (default False). max_retries: Optional; the maximum number of retry attempts if an HTTP error occurs (default 3). delay_seconds: Optional; the number of seconds to wait between retry attempts (default 5). Returns: A list of PubMed article IDs that match the search criteria. \"\"\" for attempt in range ( self . max_retries ): try : handle = Entrez . esearch ( db = \"pubmed\" , term = query , sort = \"relevance\" , retmax = self . max_results ) record = Entrez . read ( handle ) handle . close () return record [ \"IdList\" ] except HTTPError as e : error_message = f \"PubMed didn't respond (attempt { attempt + 1 } / { self . max_retries } ): { e } \" if attempt < self . max_retries : wait_message = f \"Waiting { self . delay_seconds } seconds before trying PubMed again...\" print ( error_message ) print ( wait_message ) if self . streamlit_context : st . warning ( error_message ) st . warning ( wait_message ) time . sleep ( self . delay_seconds ) else : final_message = \"Giving up on PubMed. It was an issue on their end. You may want to try again later.\" print ( error_message ) print ( final_message ) if self . streamlit_context : st . warning ( error_message ) st . warning ( final_message ) return [] def fetch_article_details ( self , pubmed_ids ): \"\"\" The function fetches article details from PubMed using the provided PubMed IDs. Args: pubmed_ids: A list of strings where each string represents the PubMed ID (PMID) of the article you want to fetch details for. max_retries: The maximum number of retry attempts if an HTTP error occurs. Default is 3. delay_seconds: The number of seconds to wait between retry attempts. Default is 5. streamlit_context: A boolean flag indicating whether the code is running within a Streamlit app. Default is False. Returns: A list of dictionaries, where each dictionary contains the details of an article with the given PubMed ID. \"\"\" pubmed_ids = [ str ( id ) for id in pubmed_ids ] ids_string = \",\" . join ( pubmed_ids ) for attempt in range ( self . max_retries + 1 ): try : handle = Entrez . efetch ( db = \"pubmed\" , id = ids_string , rettype = \"medline\" , retmode = \"text\" ) records = Medline . parse ( handle ) records = list ( records ) parsed_data = [] for record in records : self . _extract_record_data ( record ) citation = self . _format_apa_citation () parsed_data . append ( { \"date_published\" : self . _pub_month , \"title\" : self . _title , \"keywords\" : self . _keywords , \"abstract\" : self . _abstract , \"pmid\" : self . _pmid , \"authors\" : self . _authors , \"journal\" : self . _journal , \"citation\" : citation , } ) parsed_df = pd . DataFrame ( parsed_data ) return parsed_df except HTTPError as e : error_message = f \"PubMed didn't respond (attempt { attempt + 1 } / { self . max_retries } ): { e } \" if attempt < self . max_retries : wait_message = f \"Waiting { self . delay_seconds } seconds before trying PubMed again...\" print ( error_message ) print ( wait_message ) else : final_message = \"Giving up on PubMed. It was an issue on their end. You may want to try again later.\" print ( error_message ) print ( final_message ) return None def fetch_article_details_xml ( self , pubmed_ids ): \"\"\" The function fetches article details from PubMed using the provided PubMed IDs. Args: pubmed_ids: A list of strings where each string represents the PubMed ID (PMID) of the article you want to fetch details for. max_retries: The maximum number of retry attempts if an HTTP error occurs. Default is 3. delay_seconds: The number of seconds to wait between retry attempts. Default is 5. streamlit_context: A boolean flag indicating whether the code is running within a Streamlit app. Default is False. Returns: A list of dictionaries, where each dictionary contains the details of an article with the given PubMed ID. \"\"\" ids_string = \",\" . join ( pubmed_ids ) for attempt in range ( self . max_retries + 1 ): try : handle = Entrez . efetch ( db = \"pubmed\" , id = ids_string , retmode = \"xml\" ) articles = Entrez . read ( handle )[ \"PubmedArticle\" ] handle . close () return articles except HTTPError as e : error_message = f \"PubMed didn't respond (attempt { attempt + 1 } / { self . max_retries } ): { e } \" if attempt < self . max_retries : wait_message = f \"Waiting { self . delay_seconds } seconds before trying PubMed again...\" print ( error_message ) print ( wait_message ) if self . streamlit_context : # TODO Ask about these imports... st . warning ( error_message ) st . warning ( wait_message ) time . sleep ( self . delay_seconds ) else : final_message = \"Giving up on PubMed. It was an issue on their end. You may want to try again later.\" print ( error_message ) print ( final_message ) if self . streamlit_context : st . warning ( error_message ) st . error ( final_message ) return []","title":"PubMedInterface"},{"location":"llm_utils/aiweb_common/resource/PubMedInterface.html#aiweb_common.resource.PubMedInterface.PubMedInterface.fetch_article_details","text":"The function fetches article details from PubMed using the provided PubMed IDs. pubmed_ids: A list of strings where each string represents the PubMed ID (PMID) of the article you want to fetch details for. max_retries: The maximum number of retry attempts if an HTTP error occurs. Default is 3. delay_seconds: The number of seconds to wait between retry attempts. Default is 5. streamlit_context: A boolean flag indicating whether the code is running within a Streamlit app. Default is False. Returns: A list of dictionaries, where each dictionary contains the details of an article with the given PubMed ID. Source code in llm_utils/aiweb_common/resource/PubMedInterface.py 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 def fetch_article_details ( self , pubmed_ids ): \"\"\" The function fetches article details from PubMed using the provided PubMed IDs. Args: pubmed_ids: A list of strings where each string represents the PubMed ID (PMID) of the article you want to fetch details for. max_retries: The maximum number of retry attempts if an HTTP error occurs. Default is 3. delay_seconds: The number of seconds to wait between retry attempts. Default is 5. streamlit_context: A boolean flag indicating whether the code is running within a Streamlit app. Default is False. Returns: A list of dictionaries, where each dictionary contains the details of an article with the given PubMed ID. \"\"\" pubmed_ids = [ str ( id ) for id in pubmed_ids ] ids_string = \",\" . join ( pubmed_ids ) for attempt in range ( self . max_retries + 1 ): try : handle = Entrez . efetch ( db = \"pubmed\" , id = ids_string , rettype = \"medline\" , retmode = \"text\" ) records = Medline . parse ( handle ) records = list ( records ) parsed_data = [] for record in records : self . _extract_record_data ( record ) citation = self . _format_apa_citation () parsed_data . append ( { \"date_published\" : self . _pub_month , \"title\" : self . _title , \"keywords\" : self . _keywords , \"abstract\" : self . _abstract , \"pmid\" : self . _pmid , \"authors\" : self . _authors , \"journal\" : self . _journal , \"citation\" : citation , } ) parsed_df = pd . DataFrame ( parsed_data ) return parsed_df except HTTPError as e : error_message = f \"PubMed didn't respond (attempt { attempt + 1 } / { self . max_retries } ): { e } \" if attempt < self . max_retries : wait_message = f \"Waiting { self . delay_seconds } seconds before trying PubMed again...\" print ( error_message ) print ( wait_message ) else : final_message = \"Giving up on PubMed. It was an issue on their end. You may want to try again later.\" print ( error_message ) print ( final_message ) return None","title":"fetch_article_details"},{"location":"llm_utils/aiweb_common/resource/PubMedInterface.html#aiweb_common.resource.PubMedInterface.PubMedInterface.fetch_article_details_xml","text":"The function fetches article details from PubMed using the provided PubMed IDs. pubmed_ids: A list of strings where each string represents the PubMed ID (PMID) of the article you want to fetch details for. max_retries: The maximum number of retry attempts if an HTTP error occurs. Default is 3. delay_seconds: The number of seconds to wait between retry attempts. Default is 5. streamlit_context: A boolean flag indicating whether the code is running within a Streamlit app. Default is False. Returns: A list of dictionaries, where each dictionary contains the details of an article with the given PubMed ID. Source code in llm_utils/aiweb_common/resource/PubMedInterface.py 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 def fetch_article_details_xml ( self , pubmed_ids ): \"\"\" The function fetches article details from PubMed using the provided PubMed IDs. Args: pubmed_ids: A list of strings where each string represents the PubMed ID (PMID) of the article you want to fetch details for. max_retries: The maximum number of retry attempts if an HTTP error occurs. Default is 3. delay_seconds: The number of seconds to wait between retry attempts. Default is 5. streamlit_context: A boolean flag indicating whether the code is running within a Streamlit app. Default is False. Returns: A list of dictionaries, where each dictionary contains the details of an article with the given PubMed ID. \"\"\" ids_string = \",\" . join ( pubmed_ids ) for attempt in range ( self . max_retries + 1 ): try : handle = Entrez . efetch ( db = \"pubmed\" , id = ids_string , retmode = \"xml\" ) articles = Entrez . read ( handle )[ \"PubmedArticle\" ] handle . close () return articles except HTTPError as e : error_message = f \"PubMed didn't respond (attempt { attempt + 1 } / { self . max_retries } ): { e } \" if attempt < self . max_retries : wait_message = f \"Waiting { self . delay_seconds } seconds before trying PubMed again...\" print ( error_message ) print ( wait_message ) if self . streamlit_context : # TODO Ask about these imports... st . warning ( error_message ) st . warning ( wait_message ) time . sleep ( self . delay_seconds ) else : final_message = \"Giving up on PubMed. It was an issue on their end. You may want to try again later.\" print ( error_message ) print ( final_message ) if self . streamlit_context : st . warning ( error_message ) st . error ( final_message ) return []","title":"fetch_article_details_xml"},{"location":"llm_utils/aiweb_common/resource/PubMedInterface.html#aiweb_common.resource.PubMedInterface.PubMedInterface.search_pubmed_articles","text":"The function search_pubmed_articles takes a PubMed search string, an email address, and an optional maximum number of results, and returns a list of PubMed article IDs that match the search criteria. Args: query: The search query string for PubMed. email: The email address associated with your NCBI account. max_results: Optional; maximum number of results to retrieve (default 10). streamlit_context: Optional; a boolean flag indicating whether the code is running within a Streamlit app (default False). max_retries: Optional; the maximum number of retry attempts if an HTTP error occurs (default 3). delay_seconds: Optional; the number of seconds to wait between retry attempts (default 5). Returns: A list of PubMed article IDs that match the search criteria. Source code in llm_utils/aiweb_common/resource/PubMedInterface.py 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 def search_pubmed_articles ( self , query ): \"\"\" The function `search_pubmed_articles` takes a PubMed search string, an email address, and an optional maximum number of results, and returns a list of PubMed article IDs that match the search criteria. Args: query: The search query string for PubMed. email: The email address associated with your NCBI account. max_results: Optional; maximum number of results to retrieve (default 10). streamlit_context: Optional; a boolean flag indicating whether the code is running within a Streamlit app (default False). max_retries: Optional; the maximum number of retry attempts if an HTTP error occurs (default 3). delay_seconds: Optional; the number of seconds to wait between retry attempts (default 5). Returns: A list of PubMed article IDs that match the search criteria. \"\"\" for attempt in range ( self . max_retries ): try : handle = Entrez . esearch ( db = \"pubmed\" , term = query , sort = \"relevance\" , retmax = self . max_results ) record = Entrez . read ( handle ) handle . close () return record [ \"IdList\" ] except HTTPError as e : error_message = f \"PubMed didn't respond (attempt { attempt + 1 } / { self . max_retries } ): { e } \" if attempt < self . max_retries : wait_message = f \"Waiting { self . delay_seconds } seconds before trying PubMed again...\" print ( error_message ) print ( wait_message ) if self . streamlit_context : st . warning ( error_message ) st . warning ( wait_message ) time . sleep ( self . delay_seconds ) else : final_message = \"Giving up on PubMed. It was an issue on their end. You may want to try again later.\" print ( error_message ) print ( final_message ) if self . streamlit_context : st . warning ( error_message ) st . warning ( final_message ) return []","title":"search_pubmed_articles"},{"location":"llm_utils/aiweb_common/resource/PubMedQuery.html","text":"PubMedQueryGenerator Bases: WorkflowHandler Source code in llm_utils/aiweb_common/resource/PubMedQuery.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 class PubMedQueryGenerator ( WorkflowHandler ): def __init__ ( self , LLM_INTERFACE , input_research_q , ): super () . __init__ () self . input_research_q = input_research_q self . single_response = SingleResponseHandler ( LLM_INTERFACE ) def generate_search_string ( self , loop_n = 0 , last_query = \"\" ): \"\"\" The function generates a search string for a research query using prompts and responses. Args: loop_n: The `loop_n` parameter in the `generate_search_string` method is used to track the number of times the method has been called recursively. It is used to determine if additional prompts or information should be included in the search string based on previous iterations. Defaults to 0 last_query: The `last_query` parameter in the `generate_search_string` method is used to store the last query that was executed. It is then appended to the prompt if the `loop_n` parameter is greater than 0. This allows the method to provide additional context or information based on the previous query Returns: The function `generate_search_string` returns the response generated based on the assembled prompt, after updating the total cost. \"\"\" prompt = default_resource_config . PUBMED_QUERY_PROMPT . format ( self . input_research_q ) if loop_n > 0 : prompt = ( prompt + default_resource_config . PUBMED_FEW_RESULTS_PROMPT + last_query ) assembled_prompt = ( self . single_response . single_response_service . preparer . assemble_prompt ( system_prompt = default_resource_config . PUBMED_SYSTEM_PROMPT , user_prompt = prompt , ) ) response , response_meta = self . single_response . generate_response ( assembled_prompt ) self . _update_total_cost ( response_meta ) return response . content generate_search_string ( loop_n = 0 , last_query = '' ) The function generates a search string for a research query using prompts and responses. Parameters: loop_n \u2013 The loop_n parameter in the generate_search_string method is used to track the number of times the method has been called recursively. It is used to determine if additional prompts or information should be included in the search string based on previous iterations. Defaults to 0 last_query: The last_query parameter in the generate_search_string method is used to store the last query that was executed. It is then appended to the prompt if the loop_n parameter is greater than 0. This allows the method to provide additional context or information based on the previous query Returns: \u2013 The function generate_search_string returns the response generated based on the assembled prompt, after updating the total cost. Source code in llm_utils/aiweb_common/resource/PubMedQuery.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 def generate_search_string ( self , loop_n = 0 , last_query = \"\" ): \"\"\" The function generates a search string for a research query using prompts and responses. Args: loop_n: The `loop_n` parameter in the `generate_search_string` method is used to track the number of times the method has been called recursively. It is used to determine if additional prompts or information should be included in the search string based on previous iterations. Defaults to 0 last_query: The `last_query` parameter in the `generate_search_string` method is used to store the last query that was executed. It is then appended to the prompt if the `loop_n` parameter is greater than 0. This allows the method to provide additional context or information based on the previous query Returns: The function `generate_search_string` returns the response generated based on the assembled prompt, after updating the total cost. \"\"\" prompt = default_resource_config . PUBMED_QUERY_PROMPT . format ( self . input_research_q ) if loop_n > 0 : prompt = ( prompt + default_resource_config . PUBMED_FEW_RESULTS_PROMPT + last_query ) assembled_prompt = ( self . single_response . single_response_service . preparer . assemble_prompt ( system_prompt = default_resource_config . PUBMED_SYSTEM_PROMPT , user_prompt = prompt , ) ) response , response_meta = self . single_response . generate_response ( assembled_prompt ) self . _update_total_cost ( response_meta ) return response . content","title":"PubMed Query"},{"location":"llm_utils/aiweb_common/resource/PubMedQuery.html#aiweb_common.resource.PubMedQuery.PubMedQueryGenerator","text":"Bases: WorkflowHandler Source code in llm_utils/aiweb_common/resource/PubMedQuery.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 class PubMedQueryGenerator ( WorkflowHandler ): def __init__ ( self , LLM_INTERFACE , input_research_q , ): super () . __init__ () self . input_research_q = input_research_q self . single_response = SingleResponseHandler ( LLM_INTERFACE ) def generate_search_string ( self , loop_n = 0 , last_query = \"\" ): \"\"\" The function generates a search string for a research query using prompts and responses. Args: loop_n: The `loop_n` parameter in the `generate_search_string` method is used to track the number of times the method has been called recursively. It is used to determine if additional prompts or information should be included in the search string based on previous iterations. Defaults to 0 last_query: The `last_query` parameter in the `generate_search_string` method is used to store the last query that was executed. It is then appended to the prompt if the `loop_n` parameter is greater than 0. This allows the method to provide additional context or information based on the previous query Returns: The function `generate_search_string` returns the response generated based on the assembled prompt, after updating the total cost. \"\"\" prompt = default_resource_config . PUBMED_QUERY_PROMPT . format ( self . input_research_q ) if loop_n > 0 : prompt = ( prompt + default_resource_config . PUBMED_FEW_RESULTS_PROMPT + last_query ) assembled_prompt = ( self . single_response . single_response_service . preparer . assemble_prompt ( system_prompt = default_resource_config . PUBMED_SYSTEM_PROMPT , user_prompt = prompt , ) ) response , response_meta = self . single_response . generate_response ( assembled_prompt ) self . _update_total_cost ( response_meta ) return response . content","title":"PubMedQueryGenerator"},{"location":"llm_utils/aiweb_common/resource/PubMedQuery.html#aiweb_common.resource.PubMedQuery.PubMedQueryGenerator.generate_search_string","text":"The function generates a search string for a research query using prompts and responses. Parameters: loop_n \u2013 The loop_n parameter in the generate_search_string method is used to track the number of times the method has been called recursively. It is used to determine if additional prompts or information should be included in the search string based on previous iterations. Defaults to 0 last_query: The last_query parameter in the generate_search_string method is used to store the last query that was executed. It is then appended to the prompt if the loop_n parameter is greater than 0. This allows the method to provide additional context or information based on the previous query Returns: \u2013 The function generate_search_string returns the response generated based on the assembled prompt, after updating the total cost. Source code in llm_utils/aiweb_common/resource/PubMedQuery.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 def generate_search_string ( self , loop_n = 0 , last_query = \"\" ): \"\"\" The function generates a search string for a research query using prompts and responses. Args: loop_n: The `loop_n` parameter in the `generate_search_string` method is used to track the number of times the method has been called recursively. It is used to determine if additional prompts or information should be included in the search string based on previous iterations. Defaults to 0 last_query: The `last_query` parameter in the `generate_search_string` method is used to store the last query that was executed. It is then appended to the prompt if the `loop_n` parameter is greater than 0. This allows the method to provide additional context or information based on the previous query Returns: The function `generate_search_string` returns the response generated based on the assembled prompt, after updating the total cost. \"\"\" prompt = default_resource_config . PUBMED_QUERY_PROMPT . format ( self . input_research_q ) if loop_n > 0 : prompt = ( prompt + default_resource_config . PUBMED_FEW_RESULTS_PROMPT + last_query ) assembled_prompt = ( self . single_response . single_response_service . preparer . assemble_prompt ( system_prompt = default_resource_config . PUBMED_SYSTEM_PROMPT , user_prompt = prompt , ) ) response , response_meta = self . single_response . generate_response ( assembled_prompt ) self . _update_total_cost ( response_meta ) return response . content","title":"generate_search_string"},{"location":"llm_utils/aiweb_common/resource/default_resource_config.html","text":"","title":"Default Resource Config"},{"location":"llm_utils/aiweb_common/streamlit/BYOKLogin.html","text":"","title":"Bring Your Own Key (BYOK)"},{"location":"llm_utils/aiweb_common/streamlit/streamlit_common.html","text":"","title":"Streamlit Common"}]}